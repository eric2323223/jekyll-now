# Transformer-è®¾è®¡å’Œæ„å»ºé«˜æ•ˆçš„æ—¶åºæ¨¡å‹
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸï¼ŒRNNä¸€ç›´æ˜¯è¢«æœ€å¹¿æ³›ä½¿ç”¨çš„æ·±åº¦æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿‘å¹´æ¥CNNä¹Ÿé€æ¸è¢«ç”¨äºè¿›è¡Œã€‚ã€‚ã€‚ç„¶è€Œè¿™ä¸¤ç±»æ¨¡å‹éƒ½æœ‰ä¸€äº›éš¾ä»¥å…‹æœçš„é—®é¢˜ï¼ŒTransformerå°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜çš„æ–°å‹æ¨¡å‹ï¼Œå¹¶å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœï¼Œå¤§æœ‰å–ä»£RNNåœ¨NLPé¢†åŸŸçš„ç»Ÿæ²»åœ°ä½çš„è¶‹åŠ¿ï¼Œæœ¬æ–‡æˆ‘ä»¬å°±æ¥ä¸€æ­¥æ­¥çš„åˆ†æå’Œç†è§£è¿™ä¸ªä¼˜ç§€çš„seq2seqæ¨¡å‹ã€‚

## åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼ˆseq2seqï¼‰
seq2seqé—®é¢˜æ˜¯ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ï¼‰è§£å†³çš„ä¸€ç±»å¸¸è§é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œè¯­æ€åˆ†æï¼Œæ‘˜è¦ç”Ÿæˆç­‰è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ï¼ˆNLPï¼‰ï¼Œè¿˜åŒ…æ‹¬_______ã€‚ è¿™ç±»é—®é¢˜çš„æœ€å¤§ç‰¹ç‚¹æ˜¯è¾“å…¥ï¼ˆæˆ–è¾“å‡ºï¼‰ä»¥åºåˆ—çš„å½¢å¼å‡ºç°ï¼Œåºåˆ—çš„é•¿åº¦å¯å˜ï¼Œä»»åŠ¡é€šå¸¸è¦æ±‚åˆ†ææ•´ä¸ªåºåˆ—æ‰èƒ½äº§ç”Ÿè¾“å‡ºâ€”â€”â€”â€”â€”â€”â€”â€”ã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰å¤„ç†seq2seqä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆencoder-decoderï¼‰æ¶æ„ï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºæ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ï¼Œè§£ç å™¨åˆ™å¯¹è¿™ä¸ªå†…éƒ¨è¡¨ç¤ºè¿›è¡Œè§£é‡Šã€‚
![enter image description here](https://img-blog.csdn.net/20180627114128329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hwdWxmYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)ä¼ ç»Ÿä¸Šæœ‰ä¸¤ç±»æ¨¡å‹ï¼š
- RNN
å¤„ç†seq2seqé—®é¢˜çš„ä¼ ç»Ÿæ–¹æ³•æ˜¯ä½¿ç”¨RNNæ¨¡å‹ï¼ŒRNNèƒ½å¤Ÿä¿å­˜çŠ¶æ€ï¼Œå®ƒå°†è¾“å…¥åˆ†ä¸ºå¤šæ­¥ï¼Œä¾é æ¯æ­¥è¾“å…¥å’Œä¸Šä¸€æ­¥çš„çŠ¶æ€æ›´æ–°å½“å‰çš„çŠ¶æ€ï¼ˆå’Œè¾“å‡ºï¼‰ï¼Œé€šè¿‡é‡å¤è¿™ç§æ­¥éª¤åœ¨è¯»å…¥æ‰€æœ‰åºåˆ—å…ƒç´ åå¾—åˆ°æ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ã€‚![enter image description here](https://miro.medium.com/max/2658/1*Ismhi-muID5ooWf3ZIQFFg.png)
ä»æ¨¡å‹ç»“æ„ä¸Šæ¥è¯´ç‰¹åˆ«é€‚åˆåºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚é—®é¢˜æœ‰ä¸‰ç‚¹
1. é•¿åºåˆ—çš„è®­ç»ƒå¾ˆå›°éš¾ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨é•¿åºåˆ—çš„è®­ç»ƒä¸­å®¹æ˜“å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ï¼Œè™½ç„¶LSTMå¯ä»¥æ”¹å–„è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åœ¨è¾ƒé•¿åºåˆ—çš„è®­ç»ƒä¸­ä»ç„¶æ— æ³•å®Œå…¨é¿å…ã€‚
2. åªèƒ½é¡ºåºæ‰§è¡Œï¼Œè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢
3. å›ºå®šçš„å­˜å‚¨ä¸é€‚åˆé•¿åºåˆ—
- CNN
CNNå¯ä»¥åŒæ—¶å¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œä½†æ˜¯ç”±äºå·ç§¯è¿ç®—çš„è§†åŸŸæœ‰é™ï¼Œä¸€æ¬¡å·ç§¯æ“ä½œåªèƒ½å¤„ç†æœ‰é™çš„å…ƒç´ ï¼Œå¯¹äºè¾ƒé•¿çš„åºåˆ—æ— æ³•å¤„ç†ã€‚è§£å†³åŠæ³•æ˜¯é€šè¿‡å åŠ å¤šå±‚å·ç§¯æ“ä½œæ¥é€æ¸å¢åŠ è§†åŸŸï¼Œä½†è¿™æ ·ä¼šä¸å¯é¿å…çš„å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ä¸”ä»æ²¡æœ‰å®Œå…¨è§£å†³é•¿åºåˆ—è¾“å…¥çš„å¤„ç†é—®é¢˜ï¼Œâ€”â€”â€”â€”â€”â€”â€”â€”è€Œä¸”å¢åŠ äº†æ¨¡å‹çš„å¤æ‚åº¦ï¼Œä½¿è¿ç®—å˜æ…¢ï¼Œè¿™å’Œåˆè¡·ä¸ç¬¦ã€‚

æ€»ç»“ä¸€ä¸‹ï¼Œä¸Šè¿°ä¸¤ç§æ¨¡å‹å¯¹äºé•¿åºåˆ—çš„å¤„ç†éƒ½æœ‰ç¼ºé™·ã€‚RNNéœ€è¦ä¸€æ­¥ä¸€æ­¥çš„å¤„ç†è¾“å…¥åºåˆ—ï¼ŒCNNåšå‡ºäº†ä¸€äº›æ”¹è¿›ä½†å¹¶ä¸å½»åº•ã€‚ä»æ ¹æœ¬ä¸Šçš„è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦èƒ½ä¸€æ¬¡æ€§çš„å¤„ç†å…¨éƒ¨è¾“å…¥ï¼ˆæ— è®ºåºåˆ—æœ‰å¤šé•¿ï¼‰ï¼Œå¹¶ä¸”èƒ½æ ¹æ®è¿™äº›è¾“å…¥ä¿¡æ¯åˆ†æåºåˆ—å…ƒç´ ä¹‹é—´çš„å…³è”å…³ç³»ã€‚äººä»¬ä»è‡ªå·±å¿«é€Ÿæµè§ˆçš„æ–¹å¼è·å¾—äº†å¯å‘ï¼Œå½“äººä»¬éœ€è¦å¿«é€Ÿæµè§ˆçš„æ—¶å€™ä¸ä¼šæŒ‰è¾“å…¥çš„é¡ºåºé€æ­¥é˜…è¯»ï¼Œè€Œä¼šç›´æ¥è·³åˆ°éœ€è¦å…³æ³¨çš„çš„éƒ¨åˆ†ï¼Œè¿™ç§æ ¹æ®éœ€è¦åœ¨ä¸åŒä½ç½®è·³è·ƒçš„é˜…è¯»æ–¹å¼å’Œæ³¨æ„åŠ›ç›¸å…³ï¼Œå› æ­¤è¿™ç§æ–°çš„åºåˆ—å¤„ç†æ–¹å¼è¢«å‘½åä¸ºæ³¨æ„åŠ›æœºåˆ¶
![enter image description here](https://www.visionears.nl/images/babyproduct.jpg)
Attentionæœºåˆ¶æ¥è‡ªäºäººç±»è§†è§‰æ³¨æ„åŠ›æœºåˆ¶ã€‚äººä»¬è§†è§‰åœ¨æ„ŸçŸ¥ä¸œè¥¿çš„æ—¶å€™ä¸€èˆ¬ä¸ä¼šæ˜¯ä¸€ä¸ªåœºæ™¯ä»åˆ°å¤´çœ‹åˆ°å°¾æ¯æ¬¡å…¨éƒ¨éƒ½çœ‹ï¼Œè€Œå¾€å¾€æ˜¯æ ¹æ®éœ€æ±‚è§‚å¯Ÿæ³¨æ„ç‰¹å®šçš„ä¸€éƒ¨åˆ†ã€‚è€Œä¸”å½“äººä»¬å‘ç°ä¸€ä¸ªåœºæ™¯ç»å¸¸åœ¨æŸéƒ¨åˆ†å‡ºç°è‡ªå·±æƒ³è§‚å¯Ÿçš„ä¸œè¥¿æ—¶ï¼Œäººä»¬ä¼šè¿›è¡Œå­¦ä¹ åœ¨å°†æ¥å†å‡ºç°ç±»ä¼¼åœºæ™¯æ—¶æŠŠæ³¨æ„åŠ›æ”¾åˆ°è¯¥éƒ¨åˆ†ä¸Šã€‚
> Attention is a method for aggregating a set of vectors  vivi  into just one vector, often via a lookup vector  uu. Usually,  vivi  is either the inputs to the model or the hidden states of previous time-steps, or the hidden states one level down (in the case of stacked LSTMs).
> 
> The result is often called the context vector  cc, since it contains
> the  _context_  relevant to the current time-step.
> 
> This additional context vector  cc  is then fed into the RNN/LSTM as
> well (it can be simply concatenated with the original input).
> Therefore, the context can be used to help with prediction.
> 
> The simplest way to do this is to compute probability vector 
> p=softmax(VTu)p=softmax(VTu)  and  c=âˆ‘ipivic=âˆ‘ipiviwhere  VV  is the
> concatenation of all previous  vivi. A common lookup vector  uu  is
> the current hidden state  htht.
> 
> There are many variations on this, and you can make things as
> complicated as you want. For example, instead using  vTiuviTu  as the
> logits, one may choose  f(vi,u)f(vi,u)  instead, where  ff  is an
> arbitrary neural network.
> 
> A common attention mechanism for sequence-to-sequence models uses 
> p=softmax(qTtanh(W1vi+W2ht))p=softmax(qTtanhâ¡(W1vi+W2ht)), where  vv 
> are the hidden states of the encoder, and  htht  is the current hidden
> state of the decoder.  qq  and both  WWs are parameters.
> 
> Some papers which show off different variations on the attention idea:
> 
> [Pointer Networks](https://arxiv.org/abs/1506.03134)  use attention to
> reference inputs in order to solve combinatorial optimization
> problems.
> 
> [Recurrent Entity Networks](https://arxiv.org/abs/1612.03969) 
> maintain separate memory states for different entities
> (people/objects) while reading text, and update the correct memory
> state usingf ttention attention.
> 
> [Transformer](https://arxiv.org/pdf/1706.03762.pdf)  models also make
> extensive use of attention. Their formulation of attention is slightly
> more general and also involves key vectors  kiki: the attention
> weights  pp  are actually computed between the keys and the lookup,
> and the context is then constructed with the  vivi.

## æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰
åŸºäºç»„æˆæ•´ä½“çš„å„ä¸ªå…ƒç´ åœ¨æ•´ä½“ä¸­å‘æŒ¥çš„ä½œç”¨ä¸ç›¸åŒè¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ€æƒ³æ˜¯é€šå¯¹ä½¿ç”¨ä¸åŒçš„æƒé‡ç»„åˆå„ä¸ªåºåˆ—å…ƒç´ æ¥æè¿°æ•´ä½“ï¼Œ~~è¿™å°±å¥½åƒæˆ‘ä»¬åœ¨å¿«é€Ÿè§‚å¯Ÿäººç‰©çš„ç…§ç‰‡æ—¶ä¼šæŠŠæ³¨æ„åŠ›æ›´å¤šçš„æ”¾åœ¨äººç‰©çš„é¢éƒ¨è€Œå‡ ä¹ä¸ä¼šç•™æ„èƒŒæ™¯ä¸­çš„æŸä¸€æ£µå°è‰~~ã€‚ä»æ•°å­¦è¿ç®—æ¥è®²ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯å¯¹ç»„æˆæ•´ä½“çš„å…ƒç´ åŠ æƒæ±‚å’Œçš„è¿‡ç¨‹ã€‚æƒå€¼çš„è®¡ç®—æ–¹æ³•ç”±ä»»åŠ¡ç›®æ ‡æ¥ç¡®å®šï¼Œè¿™å°±å¥½åƒã€‚ã€‚ã€‚å¯¹ã€‚ã€‚ã€‚çš„å…³æ³¨ç¨‹åº¦ä¸ä¸€è‡´æ˜¯ä¸€ä¸ªé“ç†ã€‚åœ¨æœºå™¨ç¿»è¯‘ï¼ˆä¸€ç§å¸¸è§çš„seq2seqä»»åŠ¡ï¼‰ä¸­ä¸€ç§å¸¸è§çš„æƒå€¼è¡¡é‡æ–¹æ³•æ˜¯è®¡ç®—åºåˆ—å…ƒç´ ï¼ˆå•è¯ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
æ³¨æ„åŠ›æœºåˆ¶æœ€æ—©ä½¿ç”¨åœ¨åŸºäº[RNNçš„æœºå™¨ç¿»è¯‘æ¨¡å‹](https://arxiv.org/pdf/1409.0473.pdf)ä¸­ï¼Œä¸åŒäºä»¥å¾€ä½¿ç”¨å›ºå®šçš„context vectorï¼Œ attentionèƒ½å¤Ÿè®©è§£ç å™¨æ¯æ¬¡è§£ç çš„æ—¶å€™å…³æ³¨æ›´ç›¸å…³çš„è¾“å…¥å…ƒç´ ï¼ˆç”ŸæˆåŠ¨æ€çš„context vectorï¼‰ä»è€Œæé«˜ç¿»è¯‘çš„å‡†ç¡®åº¦ã€‚

$$c_i=\sum_{j=1}\alpha_{ij}h_j$$
$$\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}exp(e_{ik})}$$
$$e_{ij}=alignment(h_i,x_j)$$
Instead of encoding the input sequence into a  **single fixed context vector**, we let the model  learn **how to generate a context vector** for each output time step. That is we let the model  **learn**  what to attend based on the input sentence and what it has produced so far.
![enter image description here](https://oscimg.oschina.net/oscnet/5bdc25e12070e665409112ee13ac9e76603.jpg)

æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦ç”¨äºseq2seqä»»åŠ¡ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³å°±æ˜¯å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ä»¥ä¸€å®šçš„è§„åˆ™åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸åŒäºRNNä¸­å…ˆé€šè¿‡ä¾æ¬¡åˆ†æè¾“å…¥å…ƒç´ æ¥é€æ­¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorçš„æ–¹å¼ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯¹è¿™äº›è¾“å…¥å…ƒç´ è¿›è¡ŒåŠ æƒå¹³å‡çš„æ–¹å¼æ¥ä¸€æ­¥åŠ å…¥æ‰€æœ‰å…ƒç´ ä¿¡æ¯æ¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯èƒ½å¤Ÿä¸€æ­¥åˆ°ä½æ•æ‰åˆ°å…¨å±€çš„è”ç³»(åºåˆ—å…ƒç´ ç›´æ¥è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ),ä¸ä»…å¤§å¤§åŠ é€Ÿï¼ˆå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼‰äº†context vectorçš„ç”Ÿæˆï¼Œè€Œä¸”é¿å…äº†RNNçš„é•¿åºåˆ—è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQZ5I4YZtpZOU8xnxqqJ2WVd7o9eeo0sHQa119cWm4qR85KanMs7-Z1DV1EfKxJLQrZaVglHLUJGPF2/pub?w=856&h=225)
-   **é¦–å…ˆ**ï¼Œä»æ•°å­¦å…¬å¼ä¸Šå’Œä»£ç å®ç°ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**åŠ æƒæ±‚å’Œ**ã€‚
-  **æœ¬è´¨**ï¼Œ***å¯¹å…ƒç´ åœ¨åºåˆ—çš„ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­é‡å®šä¹‰*** 
-   **å…¶æ¬¡**ï¼Œä»å½¢å¼ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**é”®å€¼æŸ¥è¯¢**ã€‚
-   **æœ€å**ï¼Œä»ç‰©ç†æ„ä¹‰ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**ç›¸ä¼¼æ€§åº¦é‡**ã€‚
~~ *self-attentionå±‚çš„å¥½å¤„ï¼Œè§£å†³äº†é•¿è·ç¦»ä¾èµ–ï¼Œå› ä¸ºå®ƒç›´æ¥æŠŠåºåˆ—ä¸¤ä¸¤æ¯”è¾ƒï¼ˆä»£ä»·æ˜¯è®¡ç®—é‡å˜ä¸º O(n2)ï¼Œå½“ç„¶ç”±äºæ˜¯çº¯çŸ©é˜µè¿ç®—ï¼Œè¿™ä¸ªè®¡ç®—é‡ç›¸å½“ä¹Ÿä¸æ˜¯å¾ˆä¸¥é‡ï¼‰ï¼Œè€Œä¸”æœ€é‡è¦çš„æ˜¯å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRNN
éœ€è¦ä¸€æ­¥æ­¥é€’æ¨æ‰èƒ½æ•æ‰åˆ°ï¼Œå¹¶ä¸”å¯¹äºé•¿è·ç¦»ä¾èµ–å¾ˆéš¾æ•æ‰ã€‚è€Œ CNN åˆ™éœ€è¦é€šè¿‡å±‚å æ¥æ‰©å¤§æ„Ÿå—é‡ï¼Œè¿™æ˜¯ Attention å±‚çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚*~~

Attention is cheap, ç‰¹åˆ«é€‚åˆæœºå™¨ç¿»è¯‘çš„åœºæ™¯ï¼ˆdim>lengthï¼‰
||FLOPs|
|--|--|
| attention | $O(length^2 \cdot dim)$ |
| RNN | $O(length \cdot dim^2)$ |
| CNN | $O(length \cdot dim^2 \cdot kernelwidth)$ |
ç”±äºé€šå¸¸dimè¦å¤§äºlengthï¼Œæ‰€ä»¥self-attentionçš„è¿ç®—é‡ä¼šå°‘äºRNNå’ŒCNNï¼Œ



å›¾attention mech^2anism


![å…¬å¼](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicMiaqpI5cdFEvj2sOZVykZic5SwVXksjias1lA5ukFcJ4ficRgmwIyBLK8PcibmvT8Tq4iaIqMl0IaQfVw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 
## Transformeræ¨¡å‹
åŸºäºattentionæœºåˆ¶
- è§£å†³long memory problem
- å®ç°äº†éƒ¨åˆ†å¹¶è¡Œè¿ç®—ï¼Œæå¤§ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´
- æé«˜äº†å‡†ç¡®ç‡
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSBNAHsyf_HP3_CkV1cygicnt0LhGxWcvw2PofecPP9TYJj41bghsAXTM6l6OSonSMvAjjgFInVDxC4/pub?w=961&h=590)

### æ¨¡å‹æ¶æ„
æ•´ä½“æ¶æ„ä¸Šçœ‹ï¼Œtransformerä»å±äºEncoder-Decoderæ¶æ„ï¼Œé€šè¿‡encoderå°†è¾“å…¥åºåˆ—è½¬æ¢æˆå†…éƒ¨è¡¨ç¤ºï¼Œåœ¨é€šè¿‡ä¸åŒdecoderå®ç°ä¸åŒçš„é¢„æµ‹åŠŸèƒ½ã€‚
![enter image description here](http://armancohan.com/img/transformer-1.png)
Transformerçš„æœ€å¤§çš„åˆ›æ–°åœ¨äºå®ƒä½¿ç”¨åªattentionæœºåˆ¶æ¥å®ç°seq2seq taskï¼Œé¿å…ä½¿ç”¨RNNå’ŒCNNä»è€Œä½¿å¾—åœ¨è®­ç»ƒé€Ÿåº¦å’Œå‡†ç¡®ç‡ä¸Šå…¨é¢è¶…è¶Šäº†å·²æœ‰çš„æ–¹æ³•ã€‚å…·ä½“æ¥è®²
![enter image description here](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif)

### ä¸ºä»€ä¹ˆAttention is all you need?
Attentionæ˜¯transformerçš„æ ¸å¿ƒï¼Œå®ƒä¸ä»…ä½œç”¨åœ¨encoderåˆ°docoderçš„è½¬æ¢ä¸­ï¼Œè¿˜è¢«ç”¨åœ¨encoderå’Œdecoderå†…éƒ¨ï¼Œä¹Ÿè¢«ç§°ä¸ºself-attentionã€‚
- encoder-decoder attention
- encoder attention
- decoder attention
#### è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰
æ—¶åºé—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯NLPé—®é¢˜ï¼‰ä¸­çš„åºåˆ—å…ƒç´ è¡¨ç¤ºçš„å«ä¹‰é€šå¸¸ä¸æ­¢è¯¥å•ä¸ªå…ƒç´ çš„çš„å­—é¢æ„ä¹‰ï¼Œè€Œæ˜¯ä¸æ•´ä¸ªåºåˆ—ä¸Šä¸‹æ–‡æœ‰å…³ç³»ï¼Œå› æ­¤åœ¨encodingè¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘æ•´ä¸ªåºåˆ—æ¥å†³å®šå…¶ä¸­æ¯ä¸ªå…ƒç´ çš„æ„ä¹‰ã€‚self-attentionæœºåˆ¶å°±æ˜¯åŸºäºè¿™ç§ç”±å…¨å±€ç¡®å®šå±€éƒ¨çš„æ€æƒ³ï¼Œç®€å•æ¥è¯´å®ƒä½¿ç”¨æ•´ä¸ªåºåˆ—æ‰€æœ‰å…ƒç´ çš„**åŠ æƒ**å¹³å‡æ¥ç¡®å®šæ¯ä¸€ä¸ªå…ƒç´ åœ¨æ‰€å¤„åºåˆ—ï¼ˆä¸Šä¸‹æ–‡ï¼‰ä¸­çš„å«ä¹‰ã€‚
åœ¨encoder-decoderæ¨¡å‹ä¸­encoderè´Ÿè´£å°†è¾“å…¥è½¬åŒ–ä¸ºè¾“å…¥åºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ï¼Œä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨RNNé€šè¿‡ä¸€æ­¥æ­¥çš„å åŠ åˆ†æè¿‡çš„è¾“å…¥æ¥å¾—åˆ°æ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆå›ºå®šé•¿åº¦ï¼‰ï¼ŒTransformeræ¨¡å‹ä¸­ä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰æœºåˆ¶æ¥å®ç°encodingï¼Œä¹‹æ‰€ä»¥ç§°ä½œè‡ªæ³¨æ„åŠ›æ˜¯å› ä¸ºè¿™æ˜¯åœ¨è¾“å…¥åºåˆ—å†…éƒ¨è¿›è¡Œçš„attentionæ“ä½œï¼Œç”±äºattentionæ“ä½œå°±æ˜¯å¯¹å…ƒç´ è¿›è¡Œé‡æ–°å®šä¹‰ä½¿å…¶åŒ…å«åºåˆ—ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨è¾“å…¥åºåˆ—å…ƒç´ è¿›è¡Œattentionçš„æ“ä½œç»“æœå°±æ˜¯ä½¿è¯¥å…ƒç´ åŒ…å«è¾“å…¥åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤ç»è¿‡self attentionè¿ç®—çš„æ•´ä¸ªè¾“å…¥åºåˆ—çš„ç»“æœå°±æ˜¯å’Œä¸€ä¸ªè¾“å…¥åºåˆ—å¤§å°ä¸€è‡´çš„context vectorã€‚æ˜¾ç„¶ï¼Œself attentionä¸éœ€è¦æƒ³RNNé‚£æ ·ä¸€æ­¥æ­¥çš„å‡ºå…¥è¾“å…¥ï¼Œè€Œæ˜¯å¯ä»¥åŒæ—¶å¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œattentionè¿ç®—ï¼Œå¦‚å›¾æ‰€ç¤º
![enter image description here](!%5Benter%20image%20description%20here%5D%28https://docs.google.com/drawings/d/e/2PACX-1vQZ5I4YZtpZOU8xnxqqJ2WVd7o9eeo0sHQa119cWm4qR85KanMs7-Z1DV1EfKxJLQrZaVglHLUJGPF2/pub?w=856&h=225%29)
> ä¸ºä»€ä¹ˆéœ€è¦åœ¨encoderä¸­åšself-attention
> â€`The animal didn't cross the street because it was too tired`â€
> What does â€œitâ€ in this sentence refer to? Is it referring to the street or to the animal? Itâ€™s a simple question to a human, but not as simple to an algorithm.

> å¯¹äºä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åŸå› ï¼Œè®ºæ–‡ä¸­æåˆ°ä¸»è¦ä»ä¸‰ä¸ªæ–¹é¢è€ƒè™‘ï¼ˆæ¯ä¸€å±‚çš„å¤æ‚åº¦ï¼Œæ˜¯å¦å¯ä»¥å¹¶è¡Œï¼Œé•¿è·ç¦»ä¾èµ–å­¦ä¹ ï¼‰ï¼Œå¹¶ç»™å‡ºäº†å’ŒRNNï¼ŒCNNè®¡ç®—å¤æ‚åº¦çš„æ¯”è¾ƒã€‚å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœè¾“å…¥åºåˆ—nå°äºè¡¨ç¤ºç»´åº¦dçš„è¯ï¼Œæ¯ä¸€å±‚çš„æ—¶é—´å¤æ‚åº¦self-attentionæ˜¯æ¯”è¾ƒæœ‰ä¼˜åŠ¿çš„ã€‚å½“næ¯”è¾ƒå¤§æ—¶ï¼Œä½œè€…ä¹Ÿç»™å‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆself-attentionï¼ˆrestrictedï¼‰å³æ¯ä¸ªè¯ä¸æ˜¯å’Œæ‰€æœ‰è¯è®¡ç®—attentionï¼Œè€Œæ˜¯åªä¸é™åˆ¶çš„rä¸ªè¯å»è®¡ç®—attentionã€‚åœ¨å¹¶è¡Œæ–¹é¢ï¼Œå¤šå¤´attentionå’ŒCNNä¸€æ ·ä¸ä¾èµ–äºå‰ä¸€æ—¶åˆ»çš„è®¡ç®—ï¼Œå¯ä»¥å¾ˆå¥½çš„å¹¶è¡Œï¼Œä¼˜äºRNNã€‚åœ¨é•¿è·ç¦»ä¾èµ–ä¸Šï¼Œç”±äºself-attentionæ˜¯æ¯ä¸ªè¯å’Œæ‰€æœ‰è¯éƒ½è¦è®¡ç®—attentionï¼Œæ‰€ä»¥ä¸ç®¡ä»–ä»¬ä¸­é—´æœ‰å¤šé•¿è·ç¦»ï¼Œæœ€å¤§çš„è·¯å¾„é•¿åº¦ä¹Ÿéƒ½åªæ˜¯1ã€‚å¯ä»¥æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
> In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.

> Authors motivates the use of self-attention layers instead of recurrent or convolutional layers with three desiderata:

1.  Minimize total computational complexity per layer
    
    -   **Pros:**  self-attention layers connects all positions with  O(1)O(1)  number of sequentially executed operations (eg. vs  O(n)O(n)  in RNN)
2.  Maximize amount of parallelizable computations, measured by minimum number of sequential operations required
    
    -   **Pros:**  for sequence length  nn  < representation dimensionality  dd  (true for SOTA sequence representation models like  _word-piece, byte-pair_). For very long sequences  n>dn>d  self-attention can consider only neighborhood of some size  rr  in the input sequence centered around the respective output position, thus increasing the max path length to  O(n/r)O(n/r)
3.  Minimize maximum path length between any two input and output positions in network composed of the different layer types . The shorter the path between any combination of positions in the input and output sequences, the easier to learn long-range dependencies. (See why  [Hochreiter et al, 2001](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321) ) 

**Scaled Dot-Product Attention**
å…¶ä¸­çš„æƒå€¼æ¥è‡ªè¯¥å…ƒç´ ä¸å…¶ä»–å…ƒç´ çš„ç›¸ä¼¼åº¦ï¼Œè¿™æ˜¯åŸºäºè¿™æ ·çš„å‡è®¾-ç›¸ä¼¼åº¦è¶Šé«˜çš„å…ƒç´ å¯¹ç¡®å®šè¯¥å…ƒç´ åœ¨æ•´ä¸ªåºåˆ—ä¸­çš„å«ä¹‰çš„è´¡çŒ®åº¦è¶Šå¤§ï¼Œç”±äºåºåˆ—å…ƒç´ ä»¥å‘é‡è¡¨ç¤ºï¼ˆword4vecï¼‰ï¼Œåœ¨transformerä¸­ä½¿ç”¨ç‚¹ç§¯è¿ç®—æ¥ç¡®å®šç›¸ä¼¼åº¦ï¼Œå…¶ç»“æœæ˜¯ä¸€ä¸ªæ•°å€¼ã€‚å½¢å¼åŒ–çš„å®šä¹‰ä¸º
$W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$
$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$
![enter image description here](https://miro.medium.com/max/410/1*NlQPdpNY4d26l8Vu92a0Wg.png)

comparison with RNN and CNN
- less complex
- can be paralleled, faster
- easy to learn distant dependency

![enter image description here](http://www.c-jump.com/bcc/common/Talk3/Math/Vectors/const_images/v06_dot.png)
![enter image description here](https://miro.medium.com/max/1452/1*oosK1XGaYr0AoSxfs9fx5A.png)


å¹³å‡æ˜¯æŒ‡â€”â€”â€”â€”â€”â€”
åœ¨transformerä¸­çš„encoderå’Œdecoderä¸­éƒ½ä½¿ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»–ä»¬çš„å®ç°åŸºæœ¬ç›¸åŒï¼Œç¨æœ‰ä¸åŒçš„æ˜¯åœ¨decoderä¸­ä½¿ç”¨maskæ¥*å±è”½å½“å‰å…ƒç´ ä¹‹åçš„å…ƒç´ *
#### encoder-decoder attention
In terms of encoder-decoder, the **query** is usually the hidden state of the _decoder_. Whereas **key**, is the hidden state of the _encoder_, and the corresponding **value** is normalized weight, representing how much attention a _key_ gets. Output is calculated as a wighted sum â€“ here the dot product of _query_ and _key_ is used to get a _value_.

![enter image description here](https://cntk.ai/jup/cntk204_s2s2.png)


### ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰
ä¹‹æ‰€ä»¥ä½¿ç”¨è¿™ç§æŠ€æœ¯ï¼Œæ˜¯å› ä¸ºåœ¨æ‰€æå‡ºçš„ä½“ç³»ç»“æ„ä¸­æ²¡æœ‰è¯åºçš„æ¦‚å¿µï¼ˆç¬¬ä¸€ä¸ªè¯ï¼Œç¬¬äºŒä¸ªè¯ç­‰ï¼‰ã€‚ è¾“å…¥åºåˆ—çš„æ‰€æœ‰å•è¯éƒ½ä»¥æ²¡æœ‰ç‰¹æ®Šé¡ºåºæˆ–ä½ç½®çš„æ–¹å¼é¦ˆå…¥ç½‘ç»œï¼ˆä¸æ™®é€šçš„RNNæˆ–ConvNetä½“ç³»ç»“æ„ä¸åŒï¼‰ï¼Œå› æ­¤æ¨¡å‹ä¸çŸ¥é“å•è¯çš„é¡ºåºã€‚ å› æ­¤ï¼Œä¸ä½ç½®ç›¸å…³çš„ä¿¡å·ä¼šæ·»åŠ åˆ°æ¯ä¸ªè¯åµŒå…¥ä¸­ï¼Œä»¥å¸®åŠ©æ¨¡å‹åˆå¹¶è¯çš„é¡ºåºã€‚ æ ¹æ®å®éªŒï¼Œè¿™ç§å¢åŠ ä¸ä»…é¿å…ç ´ååµŒå…¥ä¿¡æ¯ï¼Œè€Œä¸”è¿˜å¢åŠ äº†é‡è¦ä½ç½®ä¿¡æ¯ã€‚ å¯¹äºRNNï¼Œæˆ‘ä»¬å°†å•è¯é¡ºåºåœ°é¦ˆé€åˆ°RNNï¼Œå³åœ¨æ­¥éª¤né¦ˆé€ç¬¬nä¸ªå•è¯ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹åˆå¹¶å•è¯çš„é¡ºåºã€‚
ä½ç½®ç¼–ç æ˜¯å•è¯å€¼åŠå…¶åœ¨å¥å­ä¸­ä½ç½®çš„é‡æ–°è¡¨ç¤ºï¼ˆå‡å®šå¼€å¤´å’Œç»“å°¾æˆ–ä¸­é—´çš„å¼€å¤´å’Œå¼€å¤´ä¸ç›¸åŒï¼‰ã€‚ä½†æ˜¯æ‚¨å¿…é¡»è€ƒè™‘åˆ°å¥å­çš„é•¿åº¦å¯ä»¥æ˜¯ä»»æ„é•¿åº¦ï¼Œå› æ­¤ï¼Œå¦‚æœå¥å­çš„é•¿åº¦ä¸åŒï¼Œåˆ™è¯´â€œ Xâ€æ˜¯å¥å­ä¸­çš„ç¬¬ä¸‰ä¸ªå•è¯æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼š3è¯å¥ä¸­çš„ç¬¬3ä¸ªå®Œå…¨æ˜¯ åœ¨20ä¸ªå•è¯çš„å¥å­ä¸­ä¸åŒäºç¬¬ä¸‰ã€‚ä½ç½®ç¼–ç å™¨çš„ä½œç”¨æ˜¯è·å¾—sinï¼ˆxï¼‰å’Œcosï¼ˆxï¼‰å‡½æ•°çš„å¾ªç¯ç‰¹æ€§çš„å¸®åŠ©ï¼Œä»¥è¿”å›å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚
> é€šå¸¸ï¼Œå°†ä½ç½®ç¼–ç æ·»åŠ åˆ°è¾“å…¥åµŒå…¥æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„è¯é¢˜ã€‚ä¸€ç§æ–¹æ³•æ˜¯åµŒå…¥è¾“å…¥å…ƒç´ çš„ç»å¯¹ä½ç½®ï¼ˆå¦‚åœ¨ConvS2Sä¸­ä¸€æ ·ï¼‰ã€‚ä½†æ˜¯ï¼Œä½œè€…ä½¿ç”¨â€œä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°â€ã€‚ â€œæ­£å¼¦æ³¢â€ç‰ˆæœ¬éå¸¸å¤æ‚ï¼ŒåŒæ—¶å…·æœ‰ä¸ç»å¯¹ä½ç½®ç‰ˆæœ¬ç›¸ä¼¼çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œé—®é¢˜çš„å…³é”®åœ¨äºï¼Œå®ƒå¯ä»¥ä½¿æ¨¡å‹åœ¨æµ‹è¯•æ—¶å¯¹æ›´é•¿çš„å¥å­äº§ç”Ÿæ›´å¥½çš„ç¿»è¯‘ï¼ˆè‡³å°‘æ¯”è®­ç»ƒæ•°æ®ä¸­çš„å¥å­æ›´é•¿ï¼‰ã€‚é€šè¿‡è¿™ç§æ­£å¼¦æ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥å¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—é•¿åº¦3ã€‚

ç”±äºattentionæœºåˆ¶ä¸è€ƒè™‘ä½ç½®å…³ç³»ï¼Œå› æ­¤å¿…é¡»è¦åœ¨åœ¨attentionæ“ä½œå‰å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ åŠ å…¥ä½ç½®ä¿¡æ¯ã€‚ä¸€ä¸ªæœ€ç›´æ¥çš„æ–¹æ³•å°±æ˜¯å¯¹è¾“å…¥åŠ å…¥åºå·ï¼Œä½†æ˜¯è¿™ç§æ–¹æ³•çš„é—®é¢˜åœ¨äºæ— æ³•å¤„ç†é•¿åº¦è¶…è¿‡è®­ç»ƒæ•°æ®çš„è¾“å…¥åºåˆ—ã€‚åœ¨Transformeræ¨¡å‹ä¸­ä½¿ç”¨çš„æ˜¯sin/coså‡½æ•°è¿›è¡Œä½ç½®ç¼–ç ï¼Œä¸»è¦ç›®çš„æ˜¯åˆ©ç”¨sin/coså‡½æ•°çš„å‘¨æœŸæ€§æ¥è¿›è¡Œä»»æ„é•¿åº¦åºåˆ—çš„ä½ç½®ç¼–ç ã€‚

sin/cos embedding has 2 advantage
	- always between -1 and 1, good for computation
	- most benefit of relative positional encoding is it can work with any size of the input, which is longer than the longest of the training data
$$PE_{{pos,2i}}=sin(pos/10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$

> In attention, we basically take two word embeddings (x and y), pass one through a Query transformation matrix (Q) and the second through a Key transformation matrix (K), and compare how similar the resulting query and key vectors are by their dot product. So, basically, we want the dot product between Qx and Ky, which we write as:

(Qx)'(Ky) = x' (Q'Ky). So equivalently we just need to learn one joint Query-Key transformation (Q'K) that transform the secondary inputs y into a new space in which we can compare x.

By adding positional encodings e and f to x and y, respectively, we essentially change the dot product to

(Q(x+e))' (K(y+f)) = (Qx+Qe)' (Ky+Kf) = (Qx)' Ky + (Qx)' Kf + (Qe)' Ky + (Qe)' Kf = x' (Q'Ky) + x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), where in addition to the original x' (Q'Ky) term, which asks the question "how much attention should we pay to word x given word y", we also have x' (Q'Kf) + e' (Q'Ky) + e' (Q'K f), which ask the additional questions, "how much attention should we pay to word x given the position f of word y", "how much attention should we pay to y given the position e of word x", and "how much attention should we pay to the position e of word x given the position f of word y".

Essentially, the learned transformation matrix Q'K with positional encodings has to do all four of these tasks simultaneously. This is the part that may appear inefficient, since intuitively, there should be a trade-off in the ability of Q'K to do four tasks simultaneously and well.

HOWEVER, MY GUESS is that there isn't actually a trade-off when we force Q'K to do all four of these tasks, because of some approximate orthogonality condition that is satisfied of in high dimensions. The intuition for this is that randomly chosen vectors in high dimensions are almost always approximately orthogonal. There's no reason to think that the word vectors and position encoding vectors are related in any way. If the word embeddings form a smaller dimensional subspace and the positional encodings form another smaller dimensional subspace, then perhaps the two subspaces themselves are approximately orthogonal, so presumably these subspaces can be transformed approx. independently through the same learned Q'K transformation (since they basically exist on different axes in high dimensional space). I don't know if this is true, but it seems intuitively possible.

If true, this would explain why adding positional encodings, instead of concatenation, is essentially fine. Concatenation would ensure that the positional dimensions are orthogonal to the word dimensions, but my guess is that, because these embedding spaces are so high dimensional, you can get approximate orthogonality for free even when adding, without the costs of concatenation (many more parameters to learn). Adding layers would only help with this, by allowing for nonlinearities.

We also ultimately want e and f to behave in some nice ways, so that there's some kind of "closeness" in the vector representation with respect to small changes in positions. The sin and cos representation is nice since nearby positions have high similarity in their positional encodings, which may make it easier to learn transformations that "preserve" this desired closeness.

(Maybe I'm wrong, and the approximate orthogonality arises from stacking multiple layers or non-linearities in the fully-connected parts of the transformer).

tl;dr: It is intuitively possible that, in high dimensions, the word vectors form a smaller dimensional subspace within the full embedding space, and the positional vectors form a different smaller dimensional subspace approximately orthogonal to the one spanned by word vectors. Thus despite vector addition, the two subspaces can be manipulated essentially independently of each other by some single learned transformation. Thus, concatenation doesn't add much, but greatly increases cost in terms of parameters to learn.

![enter image description here](https://www.researchgate.net/publication/327068570/figure/fig3/AS:660457148928000@1534476663109/The-original-positional-encoding-used-in-Attention-Is-All-You-Need-VSP-17-composed.png)
![enter image description here](https://www.d2l.ai/_images/output_transformer_ee2e4a_21_0.svg)
ç”±äºtransformerä¸ä½¿ç”¨RNNå’ŒCNNï¼Œä»…ä»…è®¡ç®—ä¸åŒå…ƒç´ ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå› æ­¤å¿…é¡»åŠ å…¥ä½ç½®ä¿¡æ¯æ¥ä¿è¯transformeræ­£ç¡®çš„ç†è§£è¾“å…¥åºåˆ—ã€‚æœ€ç®€å•çš„ä½ç½®ç¼–ç æ˜¯ç›´æ¥ä½¿ç”¨å…ƒç´ çš„åºå·ï¼Œä½†è¿™ç§æ–¹å¼å¯¹è¾“å…¥åºåˆ—çš„é•¿åº¦è¿‡äºæ•æ„Ÿï¼Œå¯¹ç›¸å¯¹ä½ç½®å…³ç³»çš„è¡¨è¾¾â€”â€”â€”â€”â€”â€”ã€‚ extrapolate training samples
Transformerä¸­ä½¿ç”¨äº†sin/cosä½ç½®ç¼–ç 
	1. è®¡ç®—æ–¹ä¾¿
	2. èƒ½å¤Ÿä½“ç°ç›¸å¯¹ä½ç½®å…³ç³»
	3. å¯å¤„ç†å˜é•¿åºåˆ—
### å¤šå¤´æ³¨æ„åŠ›ï¼ˆ Multiple Headed Attention)
![enter image description here](https://miro.medium.com/max/600/1*Vb9UizPn0AHejEYW9CWxNQ.png)
Transformerä»…ä»…ä½¿ç”¨attentionè¿›è¡Œè¾“å…¥encodingï¼Œç”±äºattentionæœ¬è´¨ä¸Šåªæ˜¯å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå¹³å‡è¿ç®—ï¼Œè¿™å¯¼è‡´ç‰¹å¾æå–èƒ½åŠ›ä¸è¶³ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ä½œè€…æå‡ºäº†å¤šå¤´æ³¨æ„åŠ›ï¼ˆï¼‰çš„æ–¹æ³•ã€‚å¤šå¤´æ³¨æ„åŠ›çš„åŸºæœ¬æ€æƒ³é€šè¿‡å¤šæ¬¡åˆå§‹åŒ–è¿‡ç¨‹å¢åŠ æ¨¡å‹æå–ä¸åŒç‰¹å¾çš„æœºä¼šï¼Œè€ƒè™‘ä¸‹å›¾ä¸­é€šè¿‡ä¸‰æ¬¡åˆå§‹åŒ–åˆ†åˆ«å¾—åˆ°äº†ä¸‰ç§ç‰¹å¾ï¼šçº¢è‰²è¡¨ç¤ºåŠ¨ä½œï¼Œç»¿è‰²è¡¨åšåŠ¨ä½œæ–½åŠ è€…ï¼Œè“è‰²è¡¨ç¤ºåŠ¨ä½œæ‰¿å—ç€ï¼Œå¯ä»¥çœ‹åˆ°åœ¨å¯¹â€œè¸¢â€œè¿›è¡Œäº†ä¸‰æ¬¡self attentionè¿ç®—ï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ç§ç‰¹å¾ã€‚åœ¨å¯¹äºåŠ¨ä½œä¿¡æ¯çš„self attentionä¸­ï¼Œ"æˆ‘â€œå’Œâ€çƒâ€œçš„æƒå€¼ï¼ˆç°è‰²ç»†çº¿è¡¨ç¤ºï¼‰æ¯”â€œè¸¢â€çš„æƒå€¼ï¼ˆçº¢è‰²ç²—çº¿ï¼‰è¦å°å¾ˆå¤šï¼›åŒæ ·ï¼Œå¯¹åŠ¨ä½œæ–½åŠ è€…çš„self attentionä¸­ï¼Œâ€œæˆ‘â€ï¼ˆç»¿è‰²ç²—çº¿ï¼‰åˆ™æ˜¯ä¸»è¦è´¡çŒ®è€…ã€‚åœ¨å°†ä¸‰æ¬¡self attentionçš„ç»“æœç›¸åŠ åï¼Œå¾—åˆ°çš„æ–°çš„â€œè¸¢â€çš„ç¼–ç ä¸­å°±åŒ…å«äº†ä¸‰ç§ç‰¹å¾çš„ä¿¡æ¯ã€‚
å…·ä½“æ–¹æ³•æ˜¯å¯¹åŒä¸€ä¸ªå…ƒç´ è¿›è¡Œå¤šæ¬¡attentionè¿ç®—ï¼Œ æ¯æ¬¡attentionéƒ½ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–å‚æ•°Wï¼Œæœ€ååœ¨å°†å¤šæ¬¡attentionçš„ç»“æœç›¸åŠ ã€‚
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vT4_Vn34rr1zN4OhXIo7oCGkzXDF__Y3CIVnZ_12fjqLHtKoRSJaVIyoR7ndQHtRlfNUmgecF5mucNg/pub?w=538&h=363)
> In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.

different random initial weights matrix may lead to different representation subspace, thus give transformer ability to understand different meaning of a word
- **multi-head attention** VS convolution on multiple channels
	- Convolution: Different linear transformations by relative position
	- MHA: a weighted average 
	- It is found empirically that multi-head attention works better than the usual â€œsingle-headâ€ in the context of machine translation. And the intuition behind such an improvement is that â€œmulti-head attention allows the model to jointly attend to information from **different representation subspaces at different positions**â€

> Transformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a O(1)O(1). However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions.
> To reduce this cost authors propose the multi-head attention:
> Transformer use multi-head (dmodel/hdmodel/h  parallel attention functions) attention instead of single (dmodeldmodel-dimensional) attention function (i.e.  q,k,vq,k,v  all  dmodeldmodel-dimensional). It is at similar computational cost as in the case of single-head attention due to reduced dimensions of each head.
> Transformer imitates the classical attention mechanism (known e.g. from  [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473) or Conv2S2) where in encoder-decoder attention layers  _queries_  are form previous decoder layer, and the (memory)  _keys_  and  _values_  are from output of the encoder. Therefore, each position in decoder can attend over all positions in the input sequence.

### Why multiple layer of attention layers?


### point-wise FFN
point-wise å¯¹åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ åˆ†åˆ«è¿›è¡Œ2å±‚å…¨è¿æ¥è¿ç®—ï¼Œç›®çš„ä¸»è¦æ˜¯ä¸ºäº†æä¾›å¯¹multi-attentionæå–å‡ºçš„featureè¿›è¡Œå¤æ‚ï¼ˆéçº¿æ€§ï¼‰åˆæˆçš„èƒ½åŠ›
> Like the name indicates, this is a regular feedforward network applied to _each_ time step of the Multi Head attention outputs. The network has three layers with a non-linearity like ReLU for the hidden layer. You might be wondering why do we need a feedforward network after attention; after all isnâ€™t attention all we need ğŸ˜ˆ ? I suspect it is needed to improve model expressiveness. As we saw earlier the multi head attention partitioned the inputs and applied attention independently. There was only a linear projection to the outputs, i.e. the partitions were combined only linearly. The _Positionwise Feedforward_ network thus brings in some non-linear â€˜mixingâ€™ if we call it that. In fact for the sequence tagging task we use convolutions instead of fully connected layers. A filter of width 3 allows interactions to happen with adjacent time steps to improve performance.
### Mask
> -   In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.
> -   In the decoder: To prevent the decoder â€˜peakingâ€™ ahead at the rest of the translated sentence when predicting the next word.

ç”±äºattentionæœºåˆ¶å¯ä»¥çœ‹åˆ°å…¨éƒ¨è¾“å…¥ï¼Œæ‰€ä»¥éœ€è¦maskæ¥é˜²æ­¢attentionåœ¨è®­ç»ƒæ—¶çœ‹åˆ°æ­£ç¡®çš„è¾“å‡º 
> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ii can depend only on the known outputs at positions less than ii.
> I mentioned I would cover attention bias mask later when going through the code of  `MultiHeadAttention`. For tasks like translation the decoder is fed previous outputs as input to predict the next output. During training the quick way to get the previous outputs is to  _shift_  the training labels right (The first time step gets a special symbol) and feed them as decoder inputs â€” a technique known as  _Teacher Forcing_  in machine learning parlance. However this presents a problem for the Transformer decoder as it can â€˜cheatâ€™ by using inputs from future time steps. The places where the short circuiting can happen is the self attention step and both the feedforward steps. (Can you figure out why it cannot happen in the normal attention step?)

> In the self attention step we feed values from all time steps to the  `MultiHeadAttention`  component. Recall that we do a weighted linear combination of the  _Values_  input:

![](https://miro.medium.com/max/504/1*aJiWfOaTCktprHEgNdeJow.png)

Consider the first row of  _OUTPUT_  in the above diagram. It corresponds to the attention output at time  _t=1_. But it is computed from values right up till  _t=10_  which are future time steps. To prevent reading these future values we zero out all weights in the  _WEIGHTS_  tensor above the main diagonal. This will ensure that future values cannot creep in:

![](https://miro.medium.com/max/204/1*6aTQQSmXUfCQxj3drNEweg.png)


## Transformerä¼˜åŒ–æŠ€å·§

### residual connection
- Help gradient propagated back through stacked decoders and encoders
- Residuals carry positional information to higher layers, among other information.
### warn-up learning rate
> If your data set is highly differentiated, you can suffer from a sort of "early over-fitting". If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all. Warm-up is a way to reduce the primacy effect of the early training examples. Without it, you may need to run a few extra epochs to get the convergence desired, as the model un-trains those early superstitions.
> Many models afford this as a command-line option. The learning rate is increased linearly over the warm-up period. If the target learning rate is  `p`  and the warm-up period is  `n`, then the first batch iteration uses  `1*p/n`  for its learning rate; the second uses  `2*p/n`, and so on: iteration  `i`  uses  `i*p/n`, until we hit the nominal rate at iteration  `n`.
> This means that the first iteration gets only 1/n of the primacy effect. This does a reasonable job of balancing that influence.
> Note that the ramp-up is commonly on the order of one epoch -- but is occasionally longer for particularly skewed data, or shorter for more homogeneous distributions. You may want to adjust, depending on how functionally extreme your batches can become when the shuffling algorithm is applied to the training set.
### regularization
- dropout
- layer normalization
- label smoothing

## Transformerçš„æ”¹è¿›
Despite not having any explicit recurrency, implicitly the model is built as an autoregressive one. It implies that in order to generate an output (both while training or during inference), the model needs to compute previous outputs, which is extremely costly, for the whole net has to be run for every output. Thatâ€™s the main idea to overcome in a recent paper by researchers at [_Salesforce Research_](https://einstein.ai/research/non-autoregressive-neural-machine-translation) and the University of Hong Kong, who tried to make the whole process parallelizable[23](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html#fn:23). Their proposal is to compute _fertilities_ for every input word in the sequence, and use it instead of previous outputs in order to compute the current output. This is summarized in the figure below.
## æ€»ç»“
Transformerä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå®ƒåœ¨NLPé¢†åŸŸå–å¾—çªç ´æ€§æˆç»©æ˜¯ç”±äºå®ƒé’ˆå¯¹æœºå™¨ç¿»è¯‘é¢†åŸŸåšäº†é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œæ¯”å¦‚positional enbemddingï¼Œ self attentionï¼Œ multihead attentionï¼Œå¹¶ç»“åˆäº†å¤šç§ç›¸å…³çš„ä¼˜åŒ–æŠ€å·§ï¼Œå¦‚residual connectionï¼Œlayer normalizationç­‰ã€‚
å› æ­¤ï¼Œå¯¹äºä»»ä½•ä»»åŠ¡ï¼Œéƒ½éœ€è¦é’ˆå¯¹ä»»åŠ¡ç›®æ ‡è¿›è¡Œç›¸å¯¹åº”è®¾è®¡ï¼Œå¹¶ä¸”è¦è¿›è¡Œä¼˜åŒ–æ‰èƒ½å……åˆ†å‘æŒ¥æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
ä¸€ä¸ªå¥½çš„æ¨¡å‹ä¸ä¼šä»å¤©è€Œé™ï¼Œè€Œæ˜¯éœ€è¦ä¸æ–­åœ°åˆ†æè§‰æ¥é—®é¢˜æ‰èƒ½é€æ¸å®Œå–„ï¼Œé€šè¿‡å¯¹Transformerçš„å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æŒæ¡å¯¹å·²æœ‰æ¨¡å‹è¿›è¡Œæ”¹è¿›çš„åŸºæœ¬æ€è·¯ï¼Œ1. æ‰¾åˆ°ç—›ç‚¹å¹¶é’ˆå¯¹ä¸»è¦é—®é¢˜è¿›è¡Œè®¾è®¡ï¼›2. å»ºç«‹æ ¸å¿ƒæ¨¡å‹åè¦å¯¹éšä¹‹äº§ç”Ÿçš„æ–°é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆï¼›3.é€šè¿‡å®éªŒè¿›è¡ŒéªŒè¯ï¼Œè¿˜æœ‰åˆ©ç”¨å·²æœ‰çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚

## Resources
[Attention is all you need review]([https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html))
[The transformer - Attention is all you need]([https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY))
[Building the Mighty Transformer for Sequence Tagging in PyTorch](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8))
[Walkthrough: The Transformer Architecture](https://www.lesswrong.com/posts/qscAeYE67GoSffDDA/walkthrough-the-transformer-architecture-part-1-2)
[The Transformer: Attention Is All You Need](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)
[How to code The Transformer in PyTorch](https://blog.floydhub.com/the-transformer-in-pytorch/)
[https://www.d2l.ai/chapter_attention-mechanism/transformer.html](https://www.d2l.ai/chapter_attention-mechanism/transformer.html)
[What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
[Paper Dissected: â€œAttention is All You Needâ€ Explained](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
[https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html](https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html)
[https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network](https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network)
[Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)
[The Transformer â€“ Attention is all you need.](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
[Create The Transformer With Tensorflow 2.0](https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/)
[æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://blog.csdn.net/songbinxu/article/details/80739447)
[nlpä¸­çš„Attentionæ³¨æ„åŠ›æœºåˆ¶+Transformerè¯¦è§£](https://zhuanlan.zhihu.com/p/53682800)
[Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc)
[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
[https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis](https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis)
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTUwNjU5NTgsLTE0NjcyMTE2NzYsMTYwMz
U4MDYyNSwtMjcxNTUzNTQ1LDE5MjExNDAwOTQsLTY0ODMzNTc2
LDEzNTI4MDAzNTUsLTEyMzI3NzQzNzksMjQxMTEzMjI0LC0xOT
MxMDc2NzA1LC0xOTYyMjcwODU1LDE5MzY4Mzc3MTksLTE3MzE0
MzcyOTgsMzI3NjQ4NDU5LC02MzQ2NjYzMDIsLTE3ODIwOTE5NT
QsNjQ1ODQ2NDMyLDYzNjAxNDQwNCwxMzI4NTQ4MzI1LC0xNTYz
Mjg4OTE3XX0=
-->