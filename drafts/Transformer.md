# Transformer-Â¶Ç‰ΩïËÆæËÆ°ÂíåÊûÑÂª∫È´òÊïàÁöÑÊó∂Â∫èÊ®°Âûã
Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ(NLP)È¢ÜÂüüÔºåRNN‰∏ÄÁõ¥ÊòØË¢´ÊúÄÂπøÊ≥õ‰ΩøÁî®ÁöÑÊ∑±Â∫¶Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåËøëÂπ¥Êù•CNN‰πüÈÄêÊ∏êË¢´Áî®‰∫éËøõË°å„ÄÇ„ÄÇ„ÄÇÁÑ∂ËÄåËøô‰∏§Á±ªÊ®°ÂûãÈÉΩÊúâ‰∏Ä‰∫õÈöæ‰ª•ÂÖãÊúçÁöÑÈóÆÈ¢òÔºåTransformerÂ∞±ÊòØ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÁöÑÊñ∞ÂûãÊ®°ÂûãÔºåÂπ∂ÂèñÂæó‰∫ÜÈùûÂ∏∏Â•ΩÁöÑÊïàÊûúÔºåÂ§ßÊúâÂèñ‰ª£RNNÂú®NLPÈ¢ÜÂüüÁöÑÁªüÊ≤ªÂú∞‰ΩçÁöÑË∂ãÂäøÔºåÊú¨ÊñáÊàë‰ª¨Â∞±Êù•‰∏ÄÊ≠•Ê≠•ÁöÑÂàÜÊûêÂíåÁêÜËß£Ëøô‰∏™‰ºòÁßÄÁöÑseq2seqÊ®°Âûã„ÄÇ

## Â∫èÂàóÂà∞Â∫èÂàóÈóÆÈ¢òÔºàseq2seqÔºâ
seq2seqÈóÆÈ¢òÊòØ‰ΩøÁî®Êú∫Âô®Â≠¶‰π†ÔºàÁâπÂà´ÊòØÊ∑±Â∫¶Â≠¶‰π†ÔºâËß£ÂÜ≥ÁöÑ‰∏ÄÁ±ªÂ∏∏ËßÅÈóÆÈ¢òÔºå‰æãÂ¶ÇÊú∫Âô®ÁøªËØëÔºåËØ≠ÊÄÅÂàÜÊûêÔºåÊëòË¶ÅÁîüÊàêÁ≠âËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈóÆÈ¢òÔºàNLPÔºâÔºåËøòÂåÖÊã¨_______„ÄÇ ËøôÁ±ªÈóÆÈ¢òÁöÑÊúÄÂ§ßÁâπÁÇπÊòØËæìÂÖ•ÔºàÊàñËæìÂá∫Ôºâ‰ª•Â∫èÂàóÁöÑÂΩ¢ÂºèÂá∫Áé∞ÔºåÂ∫èÂàóÁöÑÈïøÂ∫¶ÂèØÂèòÔºå‰ªªÂä°ÈÄöÂ∏∏Ë¶ÅÊ±ÇÂàÜÊûêÊï¥‰∏™Â∫èÂàóÊâçËÉΩ‰∫ßÁîüËæìÂá∫‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî„ÄÇ‰ΩøÁî®Êú∫Âô®Â≠¶‰π†ÔºàÊ∑±Â∫¶Â≠¶‰π†ÔºâÂ§ÑÁêÜseq2seq‰ªªÂä°ÔºåÈÄöÂ∏∏‰ΩøÁî®ÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®Ôºàencoder-decoderÔºâÊû∂ÊûÑÔºåÁºñÁ†ÅÂô®Ë¥üË¥£Â∞ÜËæìÂÖ•Â∫èÂàóËΩ¨Êç¢‰∏∫Êï¥‰∏™Â∫èÂàóÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºàcontext vectorÔºâÔºåËß£Á†ÅÂô®ÂàôÂØπËøô‰∏™ÂÜÖÈÉ®Ë°®Á§∫ËøõË°åËß£Èáä„ÄÇ
![enter image description here](https://img-blog.csdn.net/20180627114128329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hwdWxmYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)‰º†Áªü‰∏äÊúâ‰∏§Á±ªÊ®°ÂûãÔºö
- RNN
Â§ÑÁêÜseq2seqÈóÆÈ¢òÁöÑ‰º†ÁªüÊñπÊ≥ïÊòØ‰ΩøÁî®RNNÊ®°ÂûãÔºåRNNËÉΩÂ§ü‰øùÂ≠òÁä∂ÊÄÅÔºåÂÆÉÂ∞ÜËæìÂÖ•ÂàÜ‰∏∫Â§öÊ≠•Ôºå‰æùÈù†ÊØèÊ≠•ËæìÂÖ•Âíå‰∏ä‰∏ÄÊ≠•ÁöÑÁä∂ÊÄÅÊõ¥Êñ∞ÂΩìÂâçÁöÑÁä∂ÊÄÅÔºàÂíåËæìÂá∫ÔºâÔºåÈÄöËøáÈáçÂ§çËøôÁßçÊ≠•È™§Âú®ËØªÂÖ•ÊâÄÊúâÂ∫èÂàóÂÖÉÁ¥†ÂêéÂæóÂà∞Êï¥‰∏™Â∫èÂàóÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºàlatent feature vectorÔºâ„ÄÇ![enter image description here](https://miro.medium.com/max/2658/1*Ismhi-muID5ooWf3ZIQFFg.png)
‰ªéÊ®°ÂûãÁªìÊûÑ‰∏äÊù•ËØ¥ÁâπÂà´ÈÄÇÂêàÂ∫èÂàóÂà∞Â∫èÂàóÈóÆÈ¢ò„ÄÇÈóÆÈ¢òÊúâ‰∏âÁÇπ
1. ÈïøÂ∫èÂàóÁöÑËÆ≠ÁªÉÂæàÂõ∞Èöæ
2. Âè™ËÉΩÈ°∫Â∫èÊâßË°åÔºåËÆ≠ÁªÉÈÄüÂ∫¶ÂæàÊÖ¢
3. Âõ∫ÂÆöÁöÑÂ≠òÂÇ®‰∏çÈÄÇÂêàÈïøÂ∫èÂàó
- CNN
CNNÂèØ‰ª•ÂêåÊó∂Â§ÑÁêÜÂ∫èÂàó‰∏≠ÁöÑÊâÄÊúâÂÖÉÁ¥†Ôºå‰ΩÜÊòØÁî±‰∫éÂç∑ÁßØËøêÁÆóÁöÑËßÜÂüüÊúâÈôêÔºå‰∏ÄÊ¨°Âç∑ÁßØÊìç‰ΩúÂè™ËÉΩÂ§ÑÁêÜÊúâÈôêÁöÑÂÖÉÁ¥†ÔºåÂØπ‰∫éËæÉÈïøÁöÑÂ∫èÂàóÊó†Ê≥ïÂ§ÑÁêÜ„ÄÇËß£ÂÜ≥ÂäûÊ≥ïÊòØÈÄöËøáÂè†Âä†Â§öÂ±ÇÂç∑ÁßØÊìç‰ΩúÊù•ÈÄêÊ∏êÂ¢ûÂä†ËßÜÂüüÔºå‰ΩÜËøôÊ†∑‰ºö‰∏çÂèØÈÅøÂÖçÁöÑÂØºËá¥‰ø°ÊÅØ‰∏¢Â§±ÔºåÂπ∂‰∏î‰ªçÊ≤°ÊúâÂÆåÂÖ®Ëß£ÂÜ≥ÈïøÂ∫èÂàóËæìÂÖ•ÁöÑÂ§ÑÁêÜÈóÆÈ¢òÔºå‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚ÄîËÄå‰∏îÂ¢ûÂä†‰∫ÜÊ®°ÂûãÁöÑÂ§çÊùÇÂ∫¶Ôºå‰ΩøËøêÁÆóÂèòÊÖ¢ÔºåËøôÂíåÂàùË°∑‰∏çÁ¨¶„ÄÇ

ÊÄªÁªì‰∏Ä‰∏ãÔºå‰∏äËø∞‰∏§ÁßçÊ®°ÂûãÂØπ‰∫éÈïøÂ∫èÂàóÁöÑÂ§ÑÁêÜÈÉΩÊúâÁº∫Èô∑„ÄÇRNNÈúÄË¶Å‰∏ÄÊ≠•‰∏ÄÊ≠•ÁöÑÂ§ÑÁêÜËæìÂÖ•Â∫èÂàóÔºåCNNÂÅöÂá∫‰∫Ü‰∏Ä‰∫õÊîπËøõ‰ΩÜÂπ∂‰∏çÂΩªÂ∫ï„ÄÇ‰ªéÊ†πÊú¨‰∏äÁöÑËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÈúÄË¶ÅËÉΩ‰∏ÄÊ¨°ÊÄßÁöÑÂ§ÑÁêÜÂÖ®ÈÉ®ËæìÂÖ•ÔºàÊó†ËÆ∫Â∫èÂàóÊúâÂ§öÈïøÔºâÔºåÂπ∂‰∏îËÉΩÊ†πÊçÆËøô‰∫õËæìÂÖ•‰ø°ÊÅØÂàÜÊûêÂ∫èÂàóÂÖÉÁ¥†‰πãÈó¥ÁöÑÂÖ≥ËÅîÂÖ≥Á≥ª„ÄÇ‰∫∫‰ª¨‰ªéËá™Â∑±Âø´ÈÄüÊµèËßàÁöÑÊñπÂºèËé∑Âæó‰∫ÜÂêØÂèëÔºåÂΩì‰∫∫‰ª¨ÈúÄË¶ÅÂø´ÈÄüÊµèËßàÁöÑÊó∂ÂÄô‰∏ÄËà¨‰∏ç‰ºö‰∏ÄÂ≠ó‰∏ÄÂè•ÁöÑÈòÖËØªÔºåËÄå‰ºöÁõ¥Êé•Ë∑≥Âà∞ÈúÄË¶ÅÂÖ≥Ê≥®ÁöÑÁöÑÈÉ®ÂàÜÔºåËøôÁßçÊ†πÊçÆÈúÄË¶ÅÂú®‰∏çÂêå‰ΩçÁΩÆË∑≥Ë∑ÉÁöÑÈòÖËØªÊñπÂºèÂíåÊ≥®ÊÑèÂäõÁõ∏ÂÖ≥ÔºåÂõ†Ê≠§ËøôÁßçÊñ∞ÁöÑÂ∫èÂàóÂ§ÑÁêÜÊñπÂºèË¢´ÂëΩÂêç‰∏∫Ê≥®ÊÑèÂäõÊú∫Âà∂

AttentionÊú∫Âà∂Êù•Ëá™‰∫é‰∫∫Á±ªËßÜËßâÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇ‰∫∫‰ª¨ËßÜËßâÂú®ÊÑüÁü•‰∏úË•øÁöÑÊó∂ÂÄô‰∏ÄËà¨‰∏ç‰ºöÊòØ‰∏Ä‰∏™Âú∫ÊôØ‰ªéÂà∞Â§¥ÁúãÂà∞Â∞æÊØèÊ¨°ÂÖ®ÈÉ®ÈÉΩÁúãÔºåËÄåÂæÄÂæÄÊòØÊ†πÊçÆÈúÄÊ±ÇËßÇÂØüÊ≥®ÊÑèÁâπÂÆöÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇËÄå‰∏îÂΩì‰∫∫‰ª¨ÂèëÁé∞‰∏Ä‰∏™Âú∫ÊôØÁªèÂ∏∏Âú®ÊüêÈÉ®ÂàÜÂá∫Áé∞Ëá™Â∑±ÊÉ≥ËßÇÂØüÁöÑ‰∏úË•øÊó∂Ôºå‰∫∫‰ª¨‰ºöËøõË°åÂ≠¶‰π†Âú®Â∞ÜÊù•ÂÜçÂá∫Áé∞Á±ª‰ººÂú∫ÊôØÊó∂ÊääÊ≥®ÊÑèÂäõÊîæÂà∞ËØ•ÈÉ®ÂàÜ‰∏ä„ÄÇ
> In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.

Âõæ
	- Use of self-attention to improve accuracy
	- Assumption: the more similar the more it contribute
	- Essence of Attention mechanism: **Feature reconstruction** based on all other inputs
	- Mathematically: weighted average
	- can be used in different tasks (text, visual, voice ...)
	- 3 types of attention
> Attention is a method for aggregating a set of vectors  vivi  into just one vector, often via a lookup vector  uu. Usually,  vivi  is either the inputs to the model or the hidden states of previous time-steps, or the hidden states one level down (in the case of stacked LSTMs).
> 
> The result is often called the context vector  cc, since it contains
> the  _context_  relevant to the current time-step.
> 
> This additional context vector  cc  is then fed into the RNN/LSTM as
> well (it can be simply concatenated with the original input).
> Therefore, the context can be used to help with prediction.
> 
> The simplest way to do this is to compute probability vector 
> p=softmax(VTu)p=softmax(VTu)  and  c=‚àëipivic=‚àëipiviwhere  VV  is the
> concatenation of all previous  vivi. A common lookup vector  uu  is
> the current hidden state  htht.
> 
> There are many variations on this, and you can make things as
> complicated as you want. For example, instead using  vTiuviTu  as the
> logits, one may choose  f(vi,u)f(vi,u)  instead, where  ff  is an
> arbitrary neural network.
> 
> A common attention mechanism for sequence-to-sequence models uses 
> p=softmax(qTtanh(W1vi+W2ht))p=softmax(qTtanh‚Å°(W1vi+W2ht)), where  vv 
> are the hidden states of the encoder, and  htht  is the current hidden
> state of the decoder.  qq  and both  WWs are parameters.
> 
> Some papers which show off different variations on the attention idea:
> 
> [Pointer Networks](https://arxiv.org/abs/1506.03134)  use attention to
> reference inputs in order to solve combinatorial optimization
> problems.
> 
> [Recurrent Entity Networks](https://arxiv.org/abs/1612.03969) 
> maintain separate memory states for different entities
> (people/objects) while reading text, and update the correct memory
> state usingf ttention attention.
> 
> [Transformer](https://arxiv.org/pdf/1706.03762.pdf)  models also make
> extensive use of attention. Their formulation of attention is slightly
> more general and also involves key vectors  kiki: the attention
> weights  pp  are actually computed between the keys and the lookup,
> and the context is then constructed with the  vivi.
## Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºàattention mechanismÔºâ
Âü∫‰∫éÁªÑÊàêÊï¥‰ΩìÁöÑÂêÑ‰∏™ÂÖÉÁ¥†Âú®Êï¥‰Ωì‰∏≠ÂèëÊå•ÁöÑ‰ΩúÁî®‰∏çÁõ∏ÂêåËøôÊ†∑‰∏Ä‰∏™‰∫ãÂÆûÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÈÄöÂØπ‰ΩøÁî®‰∏çÂêåÁöÑÊùÉÈáçÁªÑÂêàÂêÑ‰∏™Â∫èÂàóÂÖÉÁ¥†Êù•ÊèèËø∞Êï¥‰ΩìÔºåËøôÂ∞±Â•ΩÂÉèÊàë‰ª¨Âú®Âø´ÈÄüËßÇÂØü‰∫∫Áâ©ÁöÑÁÖßÁâáÊó∂‰ºöÊääÊ≥®ÊÑèÂäõÊõ¥Â§öÁöÑÊîæÂú®‰∫∫Áâ©ÁöÑÈù¢ÈÉ®ËÄåÂá†‰πé‰∏ç‰ºöÁïôÊÑèËÉåÊôØ‰∏≠ÁöÑÊüê‰∏ÄÊ£µÂ∞èËçâ„ÄÇÊú¨Ë¥®‰∏äÊù•ËÆ≤ÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂ÊòØÂØπÁªÑÊàêÊï¥‰ΩìÁöÑÂÖÉÁ¥†Âä†ÊùÉÊ±ÇÂíåÁöÑËøáÁ®ã„ÄÇÊùÉÂÄºÁöÑËÆ°ÁÆóÊñπÊ≥ïÁî±‰ªªÂä°ÁõÆÊ†áÊù•Á°ÆÂÆöÔºåËøôÂ∞±Â•ΩÂÉè„ÄÇ„ÄÇ„ÄÇÂØπ„ÄÇ„ÄÇ„ÄÇÁöÑÂÖ≥Ê≥®Á®ãÂ∫¶‰∏ç‰∏ÄËá¥ÊòØ‰∏Ä‰∏™ÈÅìÁêÜ„ÄÇÂú®Êú∫Âô®ÁøªËØëÔºà‰∏ÄÁßçÂ∏∏ËßÅÁöÑseq2seq‰ªªÂä°Ôºâ‰∏≠‰∏ÄÁßçÂ∏∏ËßÅÁöÑÊùÉÂÄºË°°ÈáèÊñπÊ≥ïÊòØËÆ°ÁÆóÂ∫èÂàóÂÖÉÁ¥†ÔºàÂçïËØçÔºâ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇ
Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏ªË¶ÅÁî®‰∫éseq2seq‰ªªÂä°ÔºåÂÆÉÁöÑÂü∫Êú¨ÊÄùÊÉ≥Â∞±ÊòØÂ∞Ü‰∫∫Á±ªÂø´ÈÄüÈòÖËØªÁöÑÊñπÂºèÂ∫îÁî®Âú®Â∫èÂàóÂàÜÊûê‰∏ä„ÄÇ‰∏çÂêå‰∫éRNN‰∏≠ÂÖàÈÄöËøá‰æùÊ¨°ÂàÜÊûêËæìÂÖ•ÂÖÉÁ¥†Êù•ÈÄêÊ≠•ÁîüÊàêcontext vectorÁöÑÊñπÂºèÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂ÂØπËøô‰∫õËæìÂÖ•ÂÖÉÁ¥†ËøõË°åÂä†ÊùÉÂπ≥ÂùáÁöÑÊñπÂºèÊù•‰∏ÄÊ≠•ÁîüÊàêcontext vector„ÄÇËøôÊ†∑ÂÅöÁöÑÂ•ΩÂ§Ñ‰∏ç‰ªÖÂ§ßÂ§ßÂä†ÈÄü‰∫Ücontext vectorÁöÑÁîüÊàêÔºåËÄå‰∏îÈÅøÂÖç‰∫ÜRNNÁöÑÈïøÂ∫èÂàóËÆ≠ÁªÉÂõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇ
-   **È¶ñÂÖà**Ôºå‰ªéÊï∞Â≠¶ÂÖ¨Âºè‰∏äÂíå‰ª£Á†ÅÂÆûÁé∞‰∏äAttentionÂèØ‰ª•ÁêÜËß£‰∏∫**Âä†ÊùÉÊ±ÇÂíå**„ÄÇ
-   **ÂÖ∂Ê¨°**Ôºå‰ªéÂΩ¢Âºè‰∏äAttentionÂèØ‰ª•ÁêÜËß£‰∏∫**ÈîÆÂÄºÊü•ËØ¢**„ÄÇ
-   **ÊúÄÂêé**Ôºå‰ªéÁâ©ÁêÜÊÑè‰πâ‰∏äAttentionÂèØ‰ª•ÁêÜËß£‰∏∫**Áõ∏‰ººÊÄßÂ∫¶Èáè**„ÄÇ
Âõæattention mechanism
![enter image description here](https://oscimg.oschina.net/oscnet/5bdc25e12070e665409112ee13ac9e76603.jpg)

![ÂÖ¨Âºè](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicMiaqpI5cdFEvj2sOZVykZic5SwVXksjias1lA5ukFcJ4ficRgmwIyBLK8PcibmvT8Tq4iaIqMl0IaQfVw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
let's think about how to design a seq-seq model with only attention.


- bear minimal core model
- ![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSqvRProLcHw7CSGx4TGpXvLAKwMiy6dEfUa1vnlhQqkx0liKnOUmE1Bm-LQogtMfT1L44jo8y_wjPB/pub?w=883&h=458)
	- embedding -> attention -> 
- add position into model
	- embedding -> positional encoding -> attention -> Dense -> softmax
- add self-attention
	- embedding -> positional encoding -> self-attention -> encoder-decoder attention -> self-attention ->
- fix attention side affect (average)
	- embedding -> positional encoding -> mutiple-headed attention -> FFN -> encoder-decoder attention -> Dense -> softmax
- 
## TransformerÊ®°Âûã
Âü∫‰∫éattentionÊú∫Âà∂
- Ëß£ÂÜ≥long memory problem
- ÂÆûÁé∞‰∫ÜÈÉ®ÂàÜÂπ∂Ë°åËøêÁÆóÔºåÊûÅÂ§ßÁº©Áü≠‰∫ÜËÆ≠ÁªÉÊó∂Èó¥
- ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÁéá

### Ê®°ÂûãÊû∂ÊûÑ
Êï¥‰ΩìÊû∂ÊûÑ‰∏äÁúãÔºåtransformer‰ªçÂ±û‰∫éEncoder-DecoderÊû∂ÊûÑÔºåÈÄöËøáencoderÂ∞ÜËæìÂÖ•Â∫èÂàóËΩ¨Êç¢ÊàêÂÜÖÈÉ®Ë°®Á§∫ÔºåÂú®ÈÄöËøá‰∏çÂêådecoderÂÆûÁé∞‰∏çÂêåÁöÑÈ¢ÑÊµãÂäüËÉΩ„ÄÇ
![enter image description here](http://armancohan.com/img/transformer-1.png)
TransformerÁöÑÊúÄÂ§ßÁöÑÂàõÊñ∞Âú®‰∫éÂÆÉ‰ΩøÁî®Âè™attentionÊú∫Âà∂Êù•ÂÆûÁé∞seq2seq taskÔºåÈÅøÂÖç‰ΩøÁî®RNNÂíåCNN‰ªéËÄå‰ΩøÂæóÂú®ËÆ≠ÁªÉÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÁéá‰∏äÂÖ®Èù¢Ë∂ÖË∂ä‰∫ÜÂ∑≤ÊúâÁöÑÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËÆ≤
![enter image description here](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif)

#### Attention
AttentionÊòØtransformerÁöÑÊ†∏ÂøÉÔºåÂÆÉ‰∏ç‰ªÖ‰ΩúÁî®Âú®encoderÂà∞docoderÁöÑËΩ¨Êç¢‰∏≠ÔºåËøòË¢´Áî®Âú®encoderÂíådecoderÂÜÖÈÉ®Ôºå‰πüË¢´Áß∞‰∏∫self-attention„ÄÇ
#### Ëá™Ê≥®ÊÑèÂäõÔºàself attentionÔºâ
Êó∂Â∫èÈóÆÈ¢òÔºàÁâπÂà´ÊòØNLPÈóÆÈ¢òÔºâ‰∏≠ÁöÑÂ∫èÂàóÂÖÉÁ¥†Ë°®Á§∫ÁöÑÂê´‰πâÈÄöÂ∏∏‰∏çÊ≠¢ËØ•Âçï‰∏™ÂÖÉÁ¥†ÁöÑÁöÑÂ≠óÈù¢ÊÑè‰πâÔºåËÄåÊòØ‰∏éÊï¥‰∏™Â∫èÂàó‰∏ä‰∏ãÊñáÊúâÂÖ≥Á≥ªÔºåÂõ†Ê≠§Âú®encodingËøáÁ®ã‰∏≠ÈúÄË¶ÅËÄÉËôëÊï¥‰∏™Â∫èÂàóÊù•ÂÜ≥ÂÆöÂÖ∂‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÁöÑÊÑè‰πâ„ÄÇself-attentionÊú∫Âà∂Â∞±ÊòØÂü∫‰∫éËøôÁßçÁî±ÂÖ®Â±ÄÁ°ÆÂÆöÂ±ÄÈÉ®ÁöÑÊÄùÊÉ≥ÔºåÁÆÄÂçïÊù•ËØ¥ÂÆÉ‰ΩøÁî®Êï¥‰∏™Â∫èÂàóÊâÄÊúâÂÖÉÁ¥†ÁöÑ**Âä†ÊùÉ**Âπ≥ÂùáÊù•Á°ÆÂÆöÊØè‰∏Ä‰∏™ÂÖÉÁ¥†Âú®ÊâÄÂ§ÑÂ∫èÂàóÔºà‰∏ä‰∏ãÊñáÔºâ‰∏≠ÁöÑÂê´‰πâ„ÄÇ
![enter image description here](https://miro.medium.com/max/410/1*NlQPdpNY4d26l8Vu92a0Wg.png)
Scaled Dot-Product Attention
ÂÖ∂‰∏≠ÁöÑÊùÉÂÄºÊù•Ëá™ËØ•ÂÖÉÁ¥†‰∏éÂÖ∂‰ªñÂÖÉÁ¥†ÁöÑÁõ∏‰ººÂ∫¶ÔºåËøôÊòØÂü∫‰∫éËøôÊ†∑ÁöÑÂÅáËÆæ-Áõ∏‰ººÂ∫¶Ë∂äÈ´òÁöÑÂÖÉÁ¥†ÂØπÁ°ÆÂÆöËØ•ÂÖÉÁ¥†Âú®Êï¥‰∏™Â∫èÂàó‰∏≠ÁöÑÂê´‰πâÁöÑË¥°ÁåÆÂ∫¶Ë∂äÂ§ßÔºåÁî±‰∫éÂ∫èÂàóÂÖÉÁ¥†‰ª•ÂêëÈáèË°®Á§∫Ôºàword4vecÔºâÔºåÂú®transformer‰∏≠‰ΩøÁî®ÁÇπÁßØËøêÁÆóÊù•Á°ÆÂÆöÁõ∏‰ººÂ∫¶ÔºåÂÖ∂ÁªìÊûúÊòØ‰∏Ä‰∏™Êï∞ÂÄº„ÄÇ
comparison with RNN and CNN
- less complex
- can be paralleled, faster
- easy to learn distant dependency

![enter image description here](http://www.c-jump.com/bcc/common/Talk3/Math/Vectors/const_images/v06_dot.png)

> *self-attentionÂ±ÇÁöÑÂ•ΩÂ§ÑÊòØËÉΩÂ§ü‰∏ÄÊ≠•Âà∞‰ΩçÊçïÊçâÂà∞ÂÖ®Â±ÄÁöÑËÅîÁ≥ªÔºåËß£ÂÜ≥‰∫ÜÈïøË∑ùÁ¶ª‰æùËµñÔºåÂõ†‰∏∫ÂÆÉÁõ¥Êé•ÊääÂ∫èÂàó‰∏§‰∏§ÊØîËæÉÔºà‰ª£‰ª∑ÊòØËÆ°ÁÆóÈáèÂèò‰∏∫ O(n2)ÔºåÂΩìÁÑ∂Áî±‰∫éÊòØÁ∫ØÁü©ÈòµËøêÁÆóÔºåËøô‰∏™ËÆ°ÁÆóÈáèÁõ∏ÂΩì‰πü‰∏çÊòØÂæà‰∏•ÈáçÔºâÔºåËÄå‰∏îÊúÄÈáçË¶ÅÁöÑÊòØÂèØ‰ª•ËøõË°åÂπ∂Ë°åËÆ°ÁÆó„ÄÇ Áõ∏ÊØî‰πã‰∏ãÔºåRNN
> ÈúÄË¶Å‰∏ÄÊ≠•Ê≠•ÈÄíÊé®ÊâçËÉΩÊçïÊçâÂà∞ÔºåÂπ∂‰∏îÂØπ‰∫éÈïøË∑ùÁ¶ª‰æùËµñÂæàÈöæÊçïÊçâ„ÄÇËÄå CNN ÂàôÈúÄË¶ÅÈÄöËøáÂ±ÇÂè†Êù•Êâ©Â§ßÊÑüÂèóÈáéÔºåËøôÊòØ Attention Â±ÇÁöÑÊòéÊòæ‰ºòÂäø„ÄÇ*
self-attentionÂÖ∂ÂÆûÂíåcnnÔºårnn‰∏ÄÊ†∑Ôºå‰πüÊòØ‰∏∫‰∫ÜÂØπËæìÂÖ•ËøõË°åÁºñÁ†ÅÔºå‰∏∫‰∫ÜËé∑ÂæóÊõ¥Â§öÁöÑ‰ø°ÊÅØ„ÄÇÊâÄ‰ª•Â∫îÊääself-attention‰πüÁúãÊàêÁΩëÁªú‰∏≠ÁöÑ‰∏Ä‰∏™Â±ÇÂä†ËøõÂéª„ÄÇ

> ÂØπ‰∫é‰ΩøÁî®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂéüÂõ†ÔºåËÆ∫Êñá‰∏≠ÊèêÂà∞‰∏ªË¶Å‰ªé‰∏â‰∏™ÊñπÈù¢ËÄÉËôëÔºàÊØè‰∏ÄÂ±ÇÁöÑÂ§çÊùÇÂ∫¶ÔºåÊòØÂê¶ÂèØ‰ª•Âπ∂Ë°åÔºåÈïøË∑ùÁ¶ª‰æùËµñÂ≠¶‰π†ÔºâÔºåÂπ∂ÁªôÂá∫‰∫ÜÂíåRNNÔºåCNNËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁöÑÊØîËæÉ„ÄÇÂèØ‰ª•ÁúãÂà∞ÔºåÂ¶ÇÊûúËæìÂÖ•Â∫èÂàónÂ∞è‰∫éË°®Á§∫Áª¥Â∫¶dÁöÑËØùÔºåÊØè‰∏ÄÂ±ÇÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶self-attentionÊòØÊØîËæÉÊúâ‰ºòÂäøÁöÑ„ÄÇÂΩìnÊØîËæÉÂ§ßÊó∂Ôºå‰ΩúËÄÖ‰πüÁªôÂá∫‰∫Ü‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àself-attentionÔºàrestrictedÔºâÂç≥ÊØè‰∏™ËØç‰∏çÊòØÂíåÊâÄÊúâËØçËÆ°ÁÆóattentionÔºåËÄåÊòØÂè™‰∏éÈôêÂà∂ÁöÑr‰∏™ËØçÂéªËÆ°ÁÆóattention„ÄÇÂú®Âπ∂Ë°åÊñπÈù¢ÔºåÂ§öÂ§¥attentionÂíåCNN‰∏ÄÊ†∑‰∏ç‰æùËµñ‰∫éÂâç‰∏ÄÊó∂ÂàªÁöÑËÆ°ÁÆóÔºåÂèØ‰ª•ÂæàÂ•ΩÁöÑÂπ∂Ë°åÔºå‰ºò‰∫éRNN„ÄÇÂú®ÈïøË∑ùÁ¶ª‰æùËµñ‰∏äÔºåÁî±‰∫éself-attentionÊòØÊØè‰∏™ËØçÂíåÊâÄÊúâËØçÈÉΩË¶ÅËÆ°ÁÆóattentionÔºåÊâÄ‰ª•‰∏çÁÆ°‰ªñ‰ª¨‰∏≠Èó¥ÊúâÂ§öÈïøË∑ùÁ¶ªÔºåÊúÄÂ§ßÁöÑË∑ØÂæÑÈïøÂ∫¶‰πüÈÉΩÂè™ÊòØ1„ÄÇÂèØ‰ª•ÊçïËé∑ÈïøË∑ùÁ¶ª‰æùËµñÂÖ≥Á≥ª„ÄÇ
> In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.

Âπ≥ÂùáÊòØÊåá‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
Âú®transformer‰∏≠ÁöÑencoderÂíådecoder‰∏≠ÈÉΩ‰ΩøÁî®‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ªñ‰ª¨ÁöÑÂÆûÁé∞Âü∫Êú¨Áõ∏ÂêåÔºåÁ®çÊúâ‰∏çÂêåÁöÑÊòØÂú®decoder‰∏≠‰ΩøÁî®maskÊù•*Â±èËîΩÂΩìÂâçÂÖÉÁ¥†‰πãÂêéÁöÑÂÖÉÁ¥†*
#### encoder-decoder attention

![enter image description here](https://cntk.ai/jup/cntk204_s2s2.png)Êñ∞ÈóÆÈ¢ò
- ‰ΩçÁΩÆÁºñÁ†ÅPositional encoding
![enter image description here](https://www.researchgate.net/publication/327068570/figure/fig3/AS:660457148928000@1534476663109/The-original-positional-encoding-used-in-Attention-Is-All-You-Need-VSP-17-composed.png)
![enter image description here](https://www.d2l.ai/_images/output_transformer_ee2e4a_21_0.svg)
Áî±‰∫étransformer‰∏ç‰ΩøÁî®RNNÂíåCNNÔºå‰ªÖ‰ªÖËÆ°ÁÆó‰∏çÂêåÂÖÉÁ¥†‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÂõ†Ê≠§ÂøÖÈ°ªÂä†ÂÖ•‰ΩçÁΩÆ‰ø°ÊÅØÊù•‰øùËØÅtransformerÊ≠£Á°ÆÁöÑÁêÜËß£ËæìÂÖ•Â∫èÂàó„ÄÇÊúÄÁÆÄÂçïÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊòØÁõ¥Êé•‰ΩøÁî®ÂÖÉÁ¥†ÁöÑÂ∫èÂè∑Ôºå‰ΩÜËøôÁßçÊñπÂºèÂØπËæìÂÖ•Â∫èÂàóÁöÑÈïøÂ∫¶Ëøá‰∫éÊïèÊÑüÔºåÂØπÁõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ªÁöÑË°®Ëææ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî„ÄÇ extrapolate training samples
Transformer‰∏≠‰ΩøÁî®‰∫Üsin/cos‰ΩçÁΩÆÁºñÁ†Å
	1. ËÆ°ÁÆóÊñπ‰æø
	2. ËÉΩÂ§ü‰ΩìÁé∞Áõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª
	3. ÂèØÂ§ÑÁêÜÂèòÈïøÂ∫èÂàó
### Â§öÂ§¥Ê≥®ÊÑèÂäõÔºà Multiple Headed Attention)
![enter image description here](https://miro.medium.com/max/600/1*Vb9UizPn0AHejEYW9CWxNQ.png)
different random initial weights matrix may lead to different representation subspace, thus give transformer ability to understand different meaning of a word
- stack of encoder/decoder layer
	- - ‰ΩçÁΩÆÁºñÁ†ÅPosiStacking of encoder/decoder
	- self attentionalÔºå encoding
Áî±‰∫étransformer‰∏ç‰ΩøÁî®RNN ÂíåCNN free - help to speed up training
	- Stacking of encoder/decoder
	- sel-decoding attention
- **multi-head attention** VS convolution on multiple channels
	- Convolution: Different linear transformations by relative position
	- MHA: a weighted average 
	- It is found empirically that multi-head attention works better than the usual ‚Äúsingle-head‚Äù in the context of machine translation. And the intuition behind such an improvement is that ‚Äúmulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions‚Äù
### Why multiple layer of attention layers?
### Positional encoding
- why not positional index? 
### point-wise FFN
point-wise ÂØπÂ∫èÂàó‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÂàÜÂà´ËøõË°å2Â±ÇÂÖ®ËøûÊé•ËøêÁÆó
> Like the name indicates, this is a regular feedforward network applied to _each_ time step of the Multi Head attention outputs. The network has three layers with a non-linearity like ReLU for the hidden layer. You might be wondering why do we need a feedforward network after attention; after all isn‚Äôt attention all we need üòà ? I suspect it is needed to improve model expressiveness. As we saw earlier the multi head attention partitioned the inputs and applied attention independently. There was only a linear projection to the outputs, i.e. the partitions were combined only linearly. The _Positionwise Feedforward_ network thus brings in some non-linear ‚Äòmixing‚Äô if we call it that. In fact for the sequence tagging task we use convolutions instead of fully connected layers. A filter of width 3 allows interactions to happen with adjacent time steps to improve performance.
### Mask
> -   In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.
> -   In the decoder: To prevent the decoder ‚Äòpeaking‚Äô ahead at the rest of the translated sentence when predicting the next word.

Áî±‰∫éattentionÊú∫Âà∂ÂèØ‰ª•ÁúãÂà∞ÂÖ®ÈÉ®ËæìÂÖ•ÔºåÊâÄ‰ª•ÈúÄË¶ÅmaskÊù•Èò≤Ê≠¢attentionÂú®ËÆ≠ÁªÉÊó∂ÁúãÂà∞Ê≠£Á°ÆÁöÑËæìÂá∫ 
> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ii can depend only on the known outputs at positions less than ii.
> I mentioned I would cover attention bias mask later when going through the code of  `MultiHeadAttention`. For tasks like translation the decoder is fed previous outputs as input to predict the next output. During training the quick way to get the previous outputs is to  _shift_  the training labels right (The first time step gets a special symbol) and feed them as decoder inputs ‚Äî a technique known as  _Teacher Forcing_  in machine learning parlance. However this presents a problem for the Transformer decoder as it can ‚Äòcheat‚Äô by using inputs from future time steps. The places where the short circuiting can happen is the self attention step and both the feedforward steps. (Can you figure out why it cannot happen in the normal attention step?)

> In the self attention step we feed values from all time steps to the  `MultiHeadAttention`  component. Recall that we do a weighted linear combination of the  _Values_  input:

![](https://miro.medium.com/max/504/1*aJiWfOaTCktprHEgNdeJow.png)

Consider the first row of  _OUTPUT_  in the above diagram. It corresponds to the attention output at time  _t=1_. But it is computed from values right up till  _t=10_  which are future time steps. To prevent reading these future values we zero out all weights in the  _WEIGHTS_  tensor above the main diagonal. This will ensure that future values cannot creep in:

![](https://miro.medium.com/max/204/1*6aTQQSmXUfCQxj3drNEweg.png)
## TransformerÁöÑÊîπËøõ
Despite not having any explicit recurrency, implicitly the model is built as an autoregressive one. It implies that in order to generate an output (both while training or during inference), the model needs to compute previous outputs, which is extremely costly, for the whole net has to be run for every output. That‚Äôs the main idea to overcome in a recent paper by researchers at [_Salesforce Research_](https://einstein.ai/research/non-autoregressive-neural-machine-translation) and the University of Hong Kong, who tried to make the whole process parallelizable[23](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html#fn:23). Their proposal is to compute _fertilities_ for every input word in the sequence, and use it instead of previous outputs in order to compute the current output. This is summarized in the figure below.
## TransformerÂÆûÁé∞
### layer normalization
### residual connection
- Help gradient propagated back through stacked decoders and encoders
- Residuals carry positional information to higher layers, among other information.
### warn-up learning rate
### regularization
- dropout
- layer normalization
## ÊÄªÁªì

## Resources
[Attention is all you need review]([https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html))
[The transformer - Attention is all you need]([https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY))
[Building the Mighty Transformer for Sequence Tagging in PyTorch](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8))
[Walkthrough: The Transformer Architecture](https://www.lesswrong.com/posts/qscAeYE67GoSffDDA/walkthrough-the-transformer-architecture-part-1-2)
[The Transformer: Attention Is All You Need](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)
[How to code The Transformer in PyTorch](https://blog.floydhub.com/the-transformer-in-pytorch/)
[https://www.d2l.ai/chapter_attention-mechanism/transformer.html](https://www.d2l.ai/chapter_attention-mechanism/transformer.html)
[What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
[Paper Dissected: ‚ÄúAttention is All You Need‚Äù Explained](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
[https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html](https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html)
[https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network](https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network)
[Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)
[The Transformer ‚Äì Attention is all you need.](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
[Create The Transformer With Tensorflow 2.0](https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/)
[Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂](https://blog.csdn.net/songbinxu/article/details/80739447)
<!--stackedit_data:
eyJoaXN0b3J5IjpbODI3MjkxMDE1LDE5NjA4MDI5MDUsODExNT
EyMjEyLC05MTA4OTM1NzgsMzc2NTE1NjYyLDEwODc4MzA2NjYs
NjQzODMwNjUsNTE2NTAyOTY1LC01NTQzOTUzNTYsLTk5MzcyMj
Q1NCwzMTc1MjEwMDYsNTA4NzU1NTU3LDE1NzgzNDA4NTksLTE5
OTI3MTYxODMsMTE5NzUyNzMwMCw5MTU1NTYyNjEsLTI0Nzc3MT
Q0MywxNzcxNDYxOTMwLC0xMDAxNTMwMjEzLC00MjEzNzU1MjNd
fQ==
-->