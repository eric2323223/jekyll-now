# Transformer-è®¾è®¡å’Œæ„å»ºé«˜æ•ˆçš„æ—¶åºæ¨¡å‹
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¾ªç¯ç¥ç»ç½‘ç»œRNNä¸€ç›´æ˜¯è¢«æœ€å¹¿æ³›ä½¿ç”¨çš„æ·±åº¦æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿‘å¹´æ¥å·ç§¯ç¥ç»ç½‘ç»œCNNä¹Ÿé€æ¸è¢«å¼•å…¥ç”¨æ¥æå‡è®­ç»ƒæ•ˆæœã€‚ç„¶è€Œè¿™ä¸¤ç±»æ¨¡å‹éƒ½æœ‰ä¸€äº›éš¾ä»¥å…‹æœçš„é—®é¢˜ï¼ŒTransformeræ¨¡å‹ä»¥æ³¨æ„åŠ›æœºåˆ¶ä¸ºæ ¸å¿ƒï¼Œå¹¶é’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶çš„ä¸è¶³åšäº†ç›¸å…³çš„è®¾è®¡å’Œä¼˜åŒ–ï¼Œå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚æœ¬æ–‡æˆ‘ä»¬å°±æ¥ä¸€æ­¥æ­¥çš„åˆ†æå’Œç†è§£è¿™ä¸ªä¼˜ç§€çš„æ—¶åºæ¨¡å‹ã€‚

## æ—¶åºï¼ˆseq2seqï¼‰é—®é¢˜
æ—¶åºé—®é¢˜æ˜¯åº”ç”¨æœºå™¨å­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ï¼‰è§£å†³çš„ä¸€ç±»å¸¸è§é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œè¯­æ€åˆ†æï¼Œæ‘˜è¦ç”Ÿæˆç­‰è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ï¼ˆNLPï¼‰ï¼Œ è¿™ç±»é—®é¢˜çš„æœ€å¤§ç‰¹ç‚¹æ˜¯è¾“å…¥ï¼ˆæˆ–è¾“å‡ºï¼‰ä»¥åºåˆ—çš„å½¢å¼å‡ºç°ï¼Œåºåˆ—çš„é•¿åº¦å¯å˜ï¼Œå¸¸è§çš„NLPä»»åŠ¡é€šå¸¸è¦æ±‚åœ¨åˆ†ææ•´ä¸ªè¾“å…¥åºåˆ—çš„åŸºç¡€ä¸Šæ‰èƒ½äº§ç”Ÿè¾“å‡ºã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰å¤„ç†æ—¶åºä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆencoder-decoderï¼‰æ¶æ„ï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºåŒ…å«æ•´ä¸ªåºåˆ—æ‰€æœ‰ç‰¹å¾çš„**åºåˆ—ç¼–ç **ï¼ˆcontext vectorï¼‰ï¼Œè§£ç å™¨è´Ÿè´£å¯¹è¿™ä¸ªå†…éƒ¨è¡¨ç¤ºè¿›è¡Œè§£é‡Šã€‚
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQpyCEO_5eiGEU2qG6G7ktzfhyjPRtMxtvGluMcFmeuEFoQYEMHIzAtvWAIH67v5uL1k5AKHS6Xn4cA/pub?w=680&h=255)

å¤„ç†æ—¶åºé—®é¢˜çš„ä¼ ç»Ÿæ–¹æ³•æ˜¯ä½¿ç”¨RNNæ¨¡å‹ï¼ŒRNNèƒ½å¤Ÿä¿å­˜çŠ¶æ€ï¼Œå®ƒå°†è¾“å…¥åˆ†ä¸ºå¤šæ­¥ï¼Œä¾é æ¯æ­¥è¾“å…¥å’Œä¸Šä¸€æ­¥çš„çŠ¶æ€æ›´æ–°å½“å‰çš„çŠ¶æ€ï¼ˆå’Œè¾“å‡ºï¼‰ï¼Œé€šè¿‡é‡å¤è¿™ç§æ­¥éª¤åœ¨è¯»å…¥æ‰€æœ‰åºåˆ—å…ƒç´ åå¾—åˆ°åºåˆ—ç¼–ç ã€‚ç”±äºRNNæœ‰å­˜å‚¨æœºåˆ¶å¹¶ä¸”ä¸é™åˆ¶åºåˆ—çš„é•¿åº¦ï¼Œä»æ¨¡å‹ç»“æ„ä¸Šæ¥è¯´æ¯”è¾ƒé€‚åˆåºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚ä½†æ˜¯é—®é¢˜æœ‰ä¸‰ç‚¹
	  - é•¿åºåˆ—çš„è®­ç»ƒå¾ˆå›°éš¾ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨é•¿åºåˆ—çš„è®­ç»ƒä¸­å®¹æ˜“å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ï¼Œè™½ç„¶LSTMå¯ä»¥æ”¹å–„è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åœ¨è¾ƒé•¿åºåˆ—çš„è®­ç»ƒä¸­ä»ç„¶æ— æ³•å®Œå…¨é¿å…ã€‚
	  - åªèƒ½é¡ºåºæ‰§è¡Œï¼Œæ— æ³•é€šè¿‡å¹¶è¡ŒåŠ é€Ÿè®­ç»ƒ
	  - å›ºå®šçš„å­˜å‚¨ç©ºé—´åœ¨å¤„ç†è¶…é•¿åºåˆ—å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Ÿ

ä¸ºäº†è§£å†³RNNé•¿åºåˆ—è®­ç»ƒé—®é¢˜ï¼Œé™¤äº†ä¸æ–­æ”¹è¿›åŸç”ŸRNNä¹‹å¤–ï¼Œäººä»¬è¿˜å°è¯•å€ŸåŠ©äºCNNã€‚è¿™æ˜¯ç”±äºCNNæœ‰èƒ½åŠ›å¤„ç†ä¸€æ®µè¾“å…¥åºåˆ—è€Œä¸æ˜¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ï¼Œè™½ç„¶å•ä¸ªå·ç§¯æ ¸å°ºå¯¸æœ‰é™ï¼Œå¯ä»¥é€šè¿‡å †å å¤šå±‚å·ç§¯æ“ä½œçš„æ–¹å¼é€æ­¥æ”¾å¤§è§†åŸŸ ã€‚ä½†è¿™æ ·åšä¼šä¸å¯é¿å…çš„å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼ˆå·ç§¯æ“ä½œä¸­çš„ä¸Šé‡‡æ ·upsamplingè¿‡ç¨‹ï¼‰ï¼ŒåŒæ—¶å¢åŠ äº†æ¨¡å‹çš„å¤æ‚åº¦ã€‚

ä¸Šè¿°ä¸¤ç§æ¨¡å‹å¯¹äºé•¿åºåˆ—çš„å¤„ç†éƒ½æœ‰ç¼ºé™·ï¼ŒRNNéœ€è¦ä¸€æ­¥ä¸€æ­¥çš„å¤„ç†è¾“å…¥åºåˆ—ï¼ŒCNNåšå‡ºäº†ä¸€äº›æ”¹è¿›ä½†å¹¶ä¸å½»åº•ã€‚ä»æ ¹æœ¬ä¸Šçš„è§£å†³é•¿åºåˆ—å¤„ç†é—®é¢˜éœ€è¦èƒ½ä¸€æ¬¡æ€§çš„å¤„ç†å…¨éƒ¨è¾“å…¥ï¼ˆæ— è®ºåºåˆ—æœ‰å¤šé•¿ï¼‰ï¼Œå¹¶ä¸”èƒ½æ ¹æ®è¿™äº›è¾“å…¥ä¿¡æ¯åˆ†æåºåˆ—å…ƒç´ ä¹‹é—´çš„å…³è”å…³ç³»ã€‚äººä»¬ä»è‡ªå·±å¿«é€Ÿæµè§ˆçš„æ–¹å¼è·å¾—äº†å¯å‘ï¼Œå½“äººä»¬éœ€è¦å¿«é€Ÿæµè§ˆçš„æ—¶å€™ä¸ä¼šæŒ‰è¾“å…¥çš„é¡ºåºä¾æ¬¡é˜…è¯»ï¼Œè€Œä¼šç›´æ¥è·³åˆ°éœ€è¦å…³æ³¨çš„çš„éƒ¨åˆ†ï¼Œè¿™ç§æ ¹æ®éœ€è¦åœ¨ä¸åŒä½ç½®è·³è·ƒçš„é˜…è¯»æ–¹å¼å’Œæ³¨æ„åŠ›ç›¸å…³ï¼Œå› æ­¤è¿™ç§æ–°çš„åºåˆ—å¤„ç†æ–¹å¼è¢«å‘½åä¸ºæ³¨æ„åŠ›æœºåˆ¶ã€‚

## æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰
åŸºäºç»„æˆæ•´ä½“çš„å„ä¸ªå…ƒç´ åœ¨æ•´ä½“ä¸­å‘æŒ¥çš„ä½œç”¨ä¸ç›¸åŒè¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ€æƒ³æ˜¯æ ¹æ®ä»»åŠ¡ç›®æ ‡ä½¿ç”¨ä¸åŒçš„æƒé‡ç»„åˆå„ä¸ªåºåˆ—å…ƒç´ æ¥æè¿°æ•´ä½“ã€‚~~ä»æ•°å­¦è¿ç®—æ¥è®²ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯å¯¹ç»„æˆæ•´ä½“çš„æ‰€æœ‰å…ƒç´ åŠ æƒæ±‚å’Œçš„è¿‡ç¨‹ã€‚æ¯ä¸ªå…ƒç´ çš„æƒå€¼ç”±ä»»åŠ¡ç›®æ ‡æ¥ç¡®å®šï¼Œåœ¨æœºå™¨ç¿»è¯‘ï¼ˆä¸€ç§å¸¸è§çš„seq2seqä»»åŠ¡ï¼‰ä¸­ä¸€ç§å¸¸è§çš„æƒå€¼è¡¡é‡æ–¹æ³•æ˜¯è®¡ç®—åºåˆ—å…ƒç´ ï¼ˆå•è¯ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚~~

~~æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦ç”¨äºseq2seqä»»åŠ¡ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³å°±æ˜¯å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ä»¥ä¸€å®šçš„è§„åˆ™åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸åŒäºRNNä¸­å…ˆé€šè¿‡ä¾æ¬¡åˆ†æè¾“å…¥å…ƒç´ æ¥é€æ­¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorçš„æ–¹å¼ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯¹è¿™äº›è¾“å…¥å…ƒç´ è¿›è¡ŒåŠ æƒå¹³å‡çš„æ–¹å¼æ¥ä¸€æ­¥åŠ å…¥æ‰€æœ‰å…ƒç´ ä¿¡æ¯æ¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯èƒ½å¤Ÿä¸€æ­¥åˆ°ä½æ•æ‰åˆ°å…¨å±€çš„è”ç³»(åºåˆ—å…ƒç´ ç›´æ¥è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ),ä¸ä»…å¤§å¤§åŠ é€Ÿï¼ˆå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼‰äº†context vectorçš„ç”Ÿæˆï¼Œè€Œä¸”é¿å…äº†RNNçš„é•¿åºåˆ—è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚~~
ä»å®ç°ä¸Šæ¥è®²ï¼Œæ³¨æ„åŠ›è¿ç®—è¡¨ç°ä¸ºåŠ æƒæ±‚å’Œè¿ç®—ï¼ŒåŠ æ•°æ˜¯åºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œæƒå€¼è®¡ç®—æ–¹æ³•æ ¹æ®ä»»åŠ¡ç›®æ ‡è€Œä¸åŒï¼ˆåœ¨æœºå™¨ç¿»è¯‘çš„åœºæ™¯ä¸­ä½¿ç”¨ç›¸ä¼¼åº¦æ¥ä½œä¸ºæƒå€¼ï¼‰ã€‚å¦‚æœ$X$è¡¨ç¤ºè¾“å…¥åºåˆ—é›†åˆ$X=\{x_1, x_2, ... x_n\}$ï¼Œå¯ä»¥å°†æ³¨æ„åŠ›è¿ç®—å½¢å¼åŒ–çš„è¡¨ç¤ºä¸º
$$Attention(X, y)=\sum_{i=1}w_ix_i$$
å…¶ä¸­$w_i$è¡¨ç¤º$x_i$çš„æƒå€¼ï¼ˆé€šå¸¸è¡¨ç°ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå³$\sum_1^n w_i=1$ï¼‰ï¼Œç”±$x_i, y$é€šè¿‡ä¸€å®šçš„è¿ç®—$f$å¾—åˆ°ã€‚$f$æ ¹æ®ä»»åŠ¡çš„ä¸åŒè€Œä¸åŒï¼Œåœ¨æœºå™¨ç¿»è¯‘çš„åœºæ™¯ä¸­ä½¿ç”¨ç›¸ä¼¼åº¦å‡½æ•°è¡¨ç¤ºã€‚
$$w_i=f(x_i, y)$$
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¯¹
$$y_2=w_{21}x_1+w_{22}x_2+w_{23}x_3+w_{24}x_4$$

![enter image description here](http://www.peterbloem.nl/files/transformers/self-attention.svg)
$$Att(X, Y) =$$

ä»è¿ç®—çš„ç»“æœä¸Šçœ‹ï¼Œç”±äº$y$åŒ…å«äº†åºåˆ—$X$æ‰€æœ‰å…ƒç´ çš„ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠæ³¨æ„åŠ›è¿ç®—ç†è§£ä¸º**å…ƒç´ åœ¨æŸä¸€ä¸ªåºåˆ—ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„é‡æ–°å®šä¹‰**ã€‚è¿™æ˜¯ä¸€ç§å¯¹äºæ—¶åºä»»åŠ¡éå¸¸å¥½çš„å±æ€§ï¼ŒRNNç”±äºèƒ½å¤Ÿä¿å­˜è¾“å…¥åºåˆ—çš„çŠ¶æ€è€Œè¢«å¹¿æ³›åº”ç”¨äºæ—¶åºä»»åŠ¡ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶ä¸ä½†ä¹Ÿ
attentionæœ€æ ¸å¿ƒçš„ç‰¹ç‚¹ï¼Œä¹Ÿæ˜¯attentionèƒ½å¤Ÿå–ä»£RNNçš„åŸºç¡€ã€‚

æƒå€¼$w_{ij}$è¡¨ç¤º$x_j$åœ¨å¯¹äº$y_i$çš„è®¡ç®—ä¸­å‘æŒ¥çš„æƒé‡ï¼Œç”±äºæ‰€æœ‰$x$éƒ½å‚ä¸$y_i$çš„è®¡ç®—ï¼Œæ‰€ä»¥ä½¿ç”¨softmaxæ¥ä¿è¯æ‰€æœ‰æƒå€¼çš„å’Œç­‰äº1ã€‚
$$w_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}exp(e_{ik})}$$
è¿™é‡Œçš„$e_{ij}$è¡¨ç¤º$x_j$å’Œ$y_i$çš„ç›¸å…³æ€§ï¼Œå¯¹äºæœºå™¨ç¿»è¯‘ä»»åŠ¡æ¥è¯´ï¼Œé€šå¸¸ç”¨çŸ¢é‡ç›¸ä¼¼æ€§æ¥è¡¨è¿°å…ƒç´ çš„ç›¸å…³æ€§ï¼Œé€‚é‡ç›¸ä¼¼æ€§çš„è®¡ç®—æ–¹æ³•æœ‰å¾ˆå¤šï¼Œå…¶ä¸­æœ€å¸¸ç”¨çš„å°±æ˜¯ç‚¹ç§¯è¿ç®—ï¼ˆdot productï¼‰

$$e_{ij}=x_j\cdot y_i=|x_j||y_i|cos\theta$$ 
$\theta$è¡¨ç¤ºä¸¤ä¸ªå‘é‡$a,b$ä¹‹é—´çš„å¤¹è§’ï¼Œå¦‚æœ$a,b$è¶Šç›¸ä¼¼åˆ™å¤¹è§’$\theta$è¶Šå°ï¼Œ$cos\theta$åˆ™è¶Šæ¥è¿‘1

![enter image description here](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSO0ZVpogoaP-ipyQF0Xhir4wSrgGJBdeU_5wDrea6UD9sF7icIYg)
-   **æœ€å**ï¼Œä»ç‰©ç†æ„ä¹‰ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**ç›¸ä¼¼æ€§åº¦é‡**ã€‚
$$e_{ij}=Sim(h_i,x_j)$$

> **try to understand why K and V are different in transformer first!!!**
> Attention has a more generalized the form: XXXXXX
> 
> Goal is to learn $W_k, W_q, W_v$ so that 
-   **å…¶æ¬¡**ï¼Œä»å½¢å¼ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**é”®å€¼æŸ¥è¯¢**
å¯¹äºè¿›è¡Œç›¸ä¼¼æ€§è®¡ç®—å’Œâ€”â€”ä¸åŒçš„æƒ…å†µï¼ŒAttentionå¯ä»¥æ›´ä¸€èˆ¬çš„è¡¨ç¤ºä¸º
$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(Sim(Q,K))V$$
ä¸Šå¼è¡¨ç¤ºå¯¹äºæŸ¥è¯¢$q$å’Œé”®å€¼å¯¹$K,V$Given a query  **q**  and a set of key-value pairs  **(K, V)**, attention can be generalised to compute a weighted sum of the values dependent on the query and the corresponding keys.  
The query determines which values to focus on; we can say that the query â€˜attendsâ€™ to the values.

![enter image description here](https://ldzhangyx.github.io/2018/10/14/self-attention/1.jpg)
æ³¨æ„åŠ›æœºåˆ¶æœ€æ—©ä½¿ç”¨åœ¨åŸºäº[RNNçš„æœºå™¨ç¿»è¯‘æ¨¡å‹](https://arxiv.org/pdf/1409.0473.pdf)ä¸­ï¼Œä¸åŒäºä»¥å¾€ä½¿ç”¨å›ºå®šçš„context vectorï¼Œ æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿè®©è§£ç å™¨æ¯æ¬¡è§£ç çš„æ—¶å€™å…³æ³¨æ›´ç›¸å…³çš„è¾“å…¥å…ƒç´ ï¼ˆç”ŸæˆåŠ¨æ€çš„context vectorï¼‰ä»è€Œæé«˜ç¿»è¯‘çš„å‡†ç¡®åº¦ã€‚

$$c_i=\sum_{j=1}\alpha_{ij}h_j$$
$$\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}exp(e_{ik})}$$
$$e_{ij}=alignment(h_i,x_j)$$

![enter image description here](https://oscimg.oschina.net/oscnet/5bdc25e12070e665409112ee13ac9e76603.jpg)

## Transformeræ¨¡å‹

Transformeræ¥è‡ªGoogle Brainå›¢é˜Ÿ2017å¹´çš„æ–‡ç« Attention is all you needã€‚æ­£å¦‚è®ºæ–‡çš„é¢˜ç›®æ‰€è¿°ï¼Œæ•´ä¸ªç½‘ç»œç»“æ„å®Œå…¨æ˜¯ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆï¼Œç”±äºæ²¡æœ‰ä½¿ç”¨RNNå’ŒCNNï¼Œé¿å…äº†æ— æ³•å¹¶è¡Œè®¡ç®—å’Œé•¿è·ç¦»ä¾èµ–ç­‰ä¼ ç»Ÿæ–¹æ³•æ— æ³•å…‹æœçš„é—®é¢˜ï¼Œç”¨æ›´å°‘çš„è®¡ç®—èµ„æºï¼Œå–å¾—äº†æ›´å¥½çš„ç»“æœï¼Œåˆ·æ–°äº†å¤šé¡¹æœºå™¨ç¿»è¯‘ä»»åŠ¡çš„è®°å½•ã€‚
æ•´ä½“æ¶æ„ä¸Šçœ‹ï¼Œtransformerä»å±äºEncoder-Decoderæ¶æ„ï¼Œé€šè¿‡encoderå°†è¾“å…¥åºåˆ—è½¬æ¢æˆå†…éƒ¨è¡¨ç¤ºï¼Œå†é€šè¿‡ä¸åŒdecoderå®ç°ä¸åŒçš„é¢„æµ‹åŠŸèƒ½ã€‚ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œç¼–ç å™¨ä¸»è¦ç”±ä¸¤ç§ç»„ä»¶æ„æˆï¼š
![enter image description here](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-01-32.png)


![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSBNAHsyf_HP3_CkV1cygicnt0LhGxWcvw2PofecPP9TYJj41bghsAXTM6l6OSonSMvAjjgFInVDxC4/pub?w=961&h=590)


### ä¸ºä»€ä¹ˆAttention is all you need?
Transformerè®ºæ–‡çš„æ ‡é¢˜è¯´åªéœ€è¦attentionæ„å‘³ç€attentionå¯ä»¥å®Œæˆä»¥å‰éœ€è¦RNNæ‰èƒ½åšçš„å·¥ä½œã€‚ç”±äºRNNæœ‰å­˜å‚¨çš„èƒ½åŠ›ï¼Œå› æ­¤å¯ä»¥åœ¨ç¼–ç é˜¶æ®µé€šè¿‡ä¸æ–­çš„å¤„ç†å’Œç§¯ç´¯ä¸€ä¸ªä¸ªçš„è¾“å…¥å…ƒç´ ä»è€Œæœ€ç»ˆè·å¾—è¿™ä¸ªè¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆcontext vectorï¼‰ï¼ŒåŒæ ·åœ¨è§£ç é˜¶æ®µæ ¹æ®context vectoräº§ç”Ÿè¾“å‡ºã€‚åœ¨transformeræ¨¡å‹ä¸­è®¾è®¡äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ç”Ÿæˆconext vector
> Attentionæ˜¯transformerçš„æ ¸å¿ƒï¼Œå®ƒä¸ä»…ä½œç”¨åœ¨encoderåˆ°decoderçš„è½¬æ¢ä¸­ï¼Œè¿˜è¢«ç”¨åœ¨ç¼–ç å™¨ï¼ˆencoderï¼‰å’Œè§£ç å™¨ï¼ˆdecoderï¼‰å†…éƒ¨ï¼Œè¿™ç§åœ¨ç¼–ç è§£ç å™¨å†…éƒ¨ä½¿ç”¨çš„attentionè¢«ç§°ä¸ºè‡ªæ³¨æ„åŠ›self-attentionã€‚è‡ªæ³¨æ„åŠ›ç”¨äºæ›¿ä»£RNNæ¥åšencoding

#### è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰
> æ—¶åºé—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯NLPé—®é¢˜ï¼‰ä¸­çš„åºåˆ—å…ƒç´ è¡¨ç¤ºçš„å«ä¹‰é€šå¸¸ä¸æ­¢è¯¥å•ä¸ªå…ƒç´ çš„çš„å­—é¢æ„ä¹‰ï¼Œè€Œæ˜¯ä¸æ•´ä¸ªåºåˆ—ä¸Šä¸‹æ–‡æœ‰å…³ç³»ï¼Œå› æ­¤åœ¨encodingè¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘æ•´ä¸ªåºåˆ—æ¥å†³å®šå…¶ä¸­æ¯ä¸ªå…ƒç´ çš„æ„ä¹‰ã€‚self-attentionæœºåˆ¶å°±æ˜¯åŸºäºè¿™ç§ç”±å…¨å±€ç¡®å®šå±€éƒ¨çš„æ€æƒ³ï¼Œç®€å•æ¥è¯´å®ƒä½¿ç”¨æ•´ä¸ªåºåˆ—æ‰€æœ‰å…ƒç´ çš„**åŠ æƒ**å¹³å‡æ¥ç¡®å®šæ¯ä¸€ä¸ªå…ƒç´ åœ¨æ‰€å¤„åºåˆ—ï¼ˆä¸Šä¸‹æ–‡ï¼‰ä¸­çš„å«ä¹‰ã€‚

åœ¨encoder-decoderæ¨¡å‹ä¸­encoderè´Ÿè´£å°†è¾“å…¥è½¬åŒ–ä¸ºè¾“å…¥åºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ï¼Œä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨RNNé€šè¿‡ä¸€æ­¥æ­¥çš„å åŠ åˆ†æè¿‡çš„è¾“å…¥æ¥å¾—åˆ°æ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆå›ºå®šé•¿åº¦ï¼‰ï¼ŒTransformeræ¨¡å‹ä¸­ä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰æœºåˆ¶æ¥å®ç°encodingï¼Œä¹‹æ‰€ä»¥ç§°ä½œè‡ªæ³¨æ„åŠ›æ˜¯å› ä¸ºè¿™æ˜¯åœ¨è¾“å…¥åºåˆ—å†…éƒ¨è¿›è¡Œçš„attentionæ“ä½œï¼Œç”±äºattentionæ“ä½œå°±æ˜¯å¯¹å…ƒç´ è¿›è¡Œé‡æ–°å®šä¹‰ä½¿å…¶åŒ…å«åºåˆ—ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨è¾“å…¥åºåˆ—å…ƒç´ è¿›è¡Œattentionçš„æ“ä½œç»“æœå°±æ˜¯ä½¿è¯¥å…ƒç´ åŒ…å«è¾“å…¥åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤ç»è¿‡self attentionè¿ç®—çš„æ•´ä¸ªè¾“å…¥åºåˆ—çš„ç»“æœå°±æ˜¯å’Œä¸€ä¸ªè¾“å…¥åºåˆ—å¤§å°ä¸€è‡´çš„context vectorã€‚æ˜¾ç„¶ï¼Œself attentionä¸éœ€è¦æƒ³RNNé‚£æ ·ä¸€æ­¥æ­¥çš„å‡ºå…¥è¾“å…¥ï¼Œè€Œæ˜¯å¯ä»¥åŒæ—¶å¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œattentionè¿ç®—ï¼Œä»ä¸‹å›¾å¯ä»¥å‘ç°ï¼ŒRNNéœ€è¦åœ¨ä¾æ¬¡å¤„ç†å…ƒç´ x1, x2å’Œx3ä¹‹åæ‰èƒ½å¾—åˆ°æ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œattentionåˆ™å¯ä»¥åŒæ—¶å¤„ç†x1ï¼Œx2ï¼Œx3è€Œå¾—åˆ°åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQZ5I4YZtpZOU8xnxqqJ2WVd7o9eeo0sHQa119cWm4qR85KanMs7-Z1DV1EfKxJLQrZaVglHLUJGPF2/pub?w=856&h=225)


æ€»ç»“æ¥è¯´ï¼ŒAttentionæ¯”è¾ƒRNNæœ‰ä¸€ä¸‹ä¸‰ç‚¹ä¼˜åŠ¿
- å¯¹äºNLPçš„ä»»åŠ¡åœºæ™¯ï¼Œattentionçš„è®¡ç®—å¤æ‚åº¦æ›´ä½ï¼ˆdim>lengthï¼‰

||FLOPs|
|--|--|
| attention | $O(length^2 \cdot dim)$ |
| RNN | $O(length \cdot dim^2)$ |
| CNN | $O(length \cdot dim^2 \cdot kernelwidth)$ |
ç”±äºé€šå¸¸dimè¦å¤§äºlengthï¼Œæ‰€ä»¥self-attentionçš„è¿ç®—é‡ä¼šå°‘äºRNNå’ŒCNNï¼Œ

- åœ¨å¹¶è¡Œæ–¹é¢ï¼Œå¤šå¤´attentionå’ŒCNNä¸€æ ·ä¸ä¾èµ–äºå‰ä¸€æ—¶åˆ»çš„è®¡ç®—ï¼Œå¯ä»¥å¾ˆå¥½çš„å¹¶è¡Œï¼Œä¼˜äºRNNã€‚
- åœ¨é•¿è·ç¦»ä¾èµ–ä¸Šï¼Œç”±äºself-attentionæ˜¯æ¯ä¸ªè¯å’Œæ‰€æœ‰è¯éƒ½è¦è®¡ç®—attentionï¼Œæ‰€ä»¥ä¸ç®¡ä»–ä»¬ä¸­é—´æœ‰å¤šé•¿è·ç¦»ï¼Œæœ€å¤§çš„è·¯å¾„é•¿åº¦ä¹Ÿéƒ½åªæ˜¯1ã€‚å¯ä»¥æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚RNNåˆ™å­˜åœ¨æ¢¯åº¦å¼¥æ•£æˆ–è€…æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚
#### Attention mask
Attentionè¿™ç§æ–°çš„ç»“æ„ä½¿å¾—ä»–çš„è®­ç»ƒæ–¹å¼ä¹Ÿå’ŒRNNä¸åŒï¼Œè¿™æ˜¯ç”±äºAttentionå¯ä»¥ç›´æ¥çœ‹åˆ°æ‰€æœ‰çš„å…ƒç´ ï¼Œå› æ­¤éœ€è¦maskæ¥é˜²æ­¢â€”â€”â€”â€”â€”â€”ï¼Œ å…·ä½“æ¥çœ‹
- ç¼–ç å™¨self attentionï¼Œä¸éœ€è¦mask
- ç¼–ç å™¨-è§£ç å™¨attentionï¼Œéœ€è¦å¯¹paddingè¿›è¡Œmask
- è§£ç å™¨self attentionï¼Œéœ€è¦å¯¹å½“å‰ä½ç½®ä¹‹åçš„æ‰€æœ‰å…ƒç´ masking

#### Scaled Dot-Product Attention (SDPA)
Transformerå¯¹æ ‡å‡†çš„attentionåšäº†ä¸€ä¸ªå°å°è°ƒæ•´ï¼šåŠ å…¥ç‰¹å¾ç¼©æ”¾ï¼ˆfeature scalingï¼‰ã€‚è¿™æ ·åšä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢softmaxè¿ç®—å°†å€¼è¾ƒå¤§çš„keyè¿‡åº¦æ”¾å¤§ï¼Œå¯¼è‡´å…¶ä»–keyçš„ä¿¡æ¯å¾ˆéš¾åŠ å…¥åˆ°attentionç»“æœä¸­ã€‚
$$\mathrm{SDPA}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$
ç‰¹å¾ç¼©æ”¾ä½“ç°åœ¨å¯¹$Q$å’Œ$K$è®¡ç®—ç‚¹ç§¯$QK^T$ä»¥åï¼Œå¢åŠ äº†ä¸€æ­¥é™¤ä»¥$\sqrt{d_k}$è¿ç®—ã€‚
ä¸‹å›¾æ˜¯ä¸Šå¼çš„å›¾åƒåŒ–è¡¨ç¤ºï¼Œå…¶ä¸­Scaleå°±æ˜¯ç‰¹å¾ç¼©æ”¾çš„æ“ä½œã€‚

>å…¶ä¸­çš„æƒå€¼æ¥è‡ªè¯¥å…ƒç´ ä¸å…¶ä»–å…ƒç´ çš„ç›¸ä¼¼åº¦ï¼Œè¿™æ˜¯åŸºäºè¿™æ ·çš„å‡è®¾-ç›¸ä¼¼åº¦è¶Šé«˜çš„å…ƒç´ å¯¹ç¡®å®šè¯¥å…ƒç´ åœ¨æ•´ä¸ªåºåˆ—ä¸­çš„å«ä¹‰çš„è´¡çŒ®åº¦è¶Šå¤§ï¼Œç”±äºåºåˆ—å…ƒç´ ä»¥å‘é‡è¡¨ç¤ºï¼ˆword4vecï¼‰ï¼Œåœ¨transformerä¸­ä½¿ç”¨ç‚¹ç§¯è¿ç®—æ¥ç¡®å®šç›¸ä¼¼åº¦ï¼Œå…¶ç»“æœæ˜¯ä¸€ä¸ªæ•°å€¼ã€‚å½¢å¼åŒ–çš„å®šä¹‰ä¸º
$W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$

![enter image description here](https://miro.medium.com/max/676/1*nCznYOY-QtWIm8Y4jyk2Kw.png)


####  encoder-decoder attention
In terms of encoder-decoder, the **query** is usually the hidden state of the _decoder_. Whereas **key**, is the hidden state of the _encoder_, and the corresponding **value** is normalized weight, representing how much attention a _key_ gets. Output is calculated as a wighted sum â€“ here the dot product of _query_ and _key_ is used to get a _value_.
![enter image description here](http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png)

### ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰
ä¸RNNå’ŒCNNä¸åŒï¼Œåœ¨Attentionä¸­æ²¡æœ‰è¯åºçš„æ¦‚å¿µï¼ˆå¦‚ç¬¬ä¸€ä¸ªè¯ï¼Œç¬¬äºŒä¸ªè¯ç­‰ï¼‰ï¼Œ è¾“å…¥åºåˆ—çš„æ‰€æœ‰å•è¯éƒ½ä»¥æ²¡æœ‰ç‰¹æ®Šé¡ºåºæˆ–ä½ç½®çš„æ–¹å¼è¾“å…¥ç½‘ç»œï¼Œå› æ­¤æ¨¡å‹ä¸çŸ¥é“å•è¯çš„é¡ºåºã€‚ å› æ­¤ï¼Œéœ€è¦å°†ä¸ä½ç½®ç›¸å…³çš„ä¿¡å·æ·»åŠ åˆ°æ¯ä¸ªè¯ä¸­ï¼Œä»¥å¸®åŠ©æ¨¡å‹ç†è§£è¯çš„é¡ºåºã€‚
ä½ç½®ç¼–ç æ˜¯å•è¯å€¼åŠå…¶åœ¨å¥å­ä¸­ä½ç½®çš„é‡æ–°è¡¨ç¤ºï¼ˆå‡å®šå¼€å¤´å’Œç»“å°¾æˆ–ä¸­é—´çš„å¼€å¤´å’Œå¼€å¤´ä¸ç›¸åŒï¼‰ã€‚è€ƒè™‘åˆ°å¥å­çš„é•¿åº¦å¯ä»¥æ˜¯ä»»æ„é•¿åº¦ï¼Œåªè®¨è®ºè¯çš„ç»å¯¹ä½ç½®æ˜¯ä¸å…¨é¢çš„ï¼ˆåŒä¸€ä¸ªè¯ï¼Œåœ¨ç”±3ä¸ªè¯ç»„æˆçš„å¥å­ä¸­çš„ç¬¬ä¸‰ä¸ªä½ç½®å’Œ30ä¸ªè¯ç»„æˆçš„å¥å­ä¸­çš„ç¬¬ä¸‰ä¸ªä½ç½®æ‰€è¡¨è¾¾çš„æ„æ€å¾ˆå¯èƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼‰ã€‚
åœ¨Transformeræ¨¡å‹ä¸­åˆ©ç”¨äº†ä¸åŒé¢‘ç‡çš„å‘¨æœŸå‡½æ•°æ¥è¿›è¡Œä½ç½®ç¼–ç ï¼Œè¿™ç§ä½ç½®ç¼–ç æœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š
- ç”±äºsin/coså‡½æ•°çš„å‘¨æœŸæ€§å®ƒèƒ½å¤Ÿè¿›è¡Œä»»æ„é•¿åº¦åºåˆ—çš„ä½ç½®ç¼–ç 
- ä½¿ç”¨å¤šä¸ªä¸åŒé¢‘ç‡æ¥ä¿è¯ä¸ä¼šç”±äºå‘¨æœŸæ€§å¯¼è‡´ä¸åŒä½ç½®çš„ç¼–ç ç›¸åŒ
- ç¬¬äºŒæ˜¯ç”±äºsin/coså‡½æ•°çš„å€¼æ€»æ˜¯åœ¨-1åˆ°1ä¹‹é—´ï¼Œè¿™ç§ç¼–ç æœ¬èº«ä¹Ÿæœ‰æ­£åˆ™åŒ–ï¼ˆnormalizationï¼‰çš„ä½œç”¨ï¼Œè¿™æœ‰åˆ©äºç¥ç»ç½‘ç»œçš„å­¦ä¹ ã€‚

å¦‚æœç”¨$pos$è¡¨ç¤ºä½ç½®ï¼Œ$i$è¡¨ç¤ºå…ƒç´ ç¼–ç çš„ç»´åº¦ï¼Œ$d_{model}$è¡¨ç¤ºæ¨¡å‹çš„ç»´åº¦ï¼Œä½ç½®ç¼–ç $PE$å¯ä»¥è¡¨ç¤ºä¸º
$$PE_{{pos,2i}}=sin(pos/10000^{2i/d_{model}}) $$
$$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$

![enter image description here](http://vandergoten.ai/img/attention_is_all_you_need/positional_embedding.png)
è®¡ç®—äº§ç”Ÿçš„ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¸å…ƒç´ å…·æœ‰ç›¸åŒç»´åº¦çš„å‘é‡ï¼Œä½¿ç”¨ç›¸åŠ çš„æ–¹å¼å°†ä½ç½®ä¿¡æ¯å åŠ è¿›å…ƒç´ ä¸­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º
![enter image description here](https://wikidocs.net/images/page/31379/transformer6_final.PNG)
ä¸ºä½•é‡‡ç”¨ç›¸åŠ çš„æ–¹å¼ï¼Ÿ
> ç›´è§‰æ˜¯ï¼Œåœ¨é«˜ç»´ä¸­éšæœºé€‰æ‹©çš„å‘é‡å‡ ä¹æ€»æ˜¯è¿‘ä¼¼æ­£äº¤çš„ã€‚æ²¡æœ‰ç†ç”±è®¤ä¸ºå•è¯å‘é‡å’Œä½ç½®ç¼–ç å‘é‡ä¹‹é—´æœ‰ä»»ä½•å…³è”ã€‚å¦‚æœå•è¯åµŒå…¥å½¢æˆä¸€ä¸ªè¾ƒå°ç»´çš„å­ç©ºé—´ï¼Œè€Œä½ç½®ç¼–ç å½¢æˆå¦ä¸€ä¸ªè¾ƒå°ç»´çš„å­ç©ºé—´ï¼Œåˆ™ä¸¤ä¸ªå­ç©ºé—´æœ¬èº«å¯èƒ½è¿‘ä¼¼æ­£äº¤ï¼Œå› æ­¤å¤§æ¦‚å¯ä»¥å¯¹è¿™äº›å­ç©ºé—´è¿›è¡Œå˜æ¢ï¼Œå°½ç®¡è¿›è¡Œäº†çŸ¢é‡ç›¸åŠ ï¼Œä½†ä¸¤ä¸ªå­ç©ºé—´ä»å¯ä»¥é€šè¿‡ä¸€äº›å•ä¸ªå­¦ä¹ çš„å˜æ¢è€Œå½¼æ­¤ç‹¬ç«‹åœ°è¿›è¡Œæ“ä½œã€‚å› æ­¤ï¼Œä¸²è”å¹¶ä¸ä¼šå¢åŠ å¤ªå¤šï¼Œä½†ä¼šå¤§å¤§å¢åŠ å­¦ä¹ å‚æ•°æ–¹é¢çš„æˆæœ¬ã€‚

ä¸ºä»€ä¹ˆè¦åŒæ—¶ä½¿ç”¨sinå’Œcosï¼Œè€Œä¸åªä½¿ç”¨å…¶ä¸­çš„ä¸€ä¸ªï¼Ÿ
ä¸‹å›¾å¯è§
![enter image description here](https://i.stack.imgur.com/5QQmq.gif)

![enter image description here](https://i.stack.imgur.com/W0b0c.gif)


### å¤šå¤´æ³¨æ„åŠ›ï¼ˆ Multiple Headed Attention, MHA)

Transformerä»…ä»…ä½¿ç”¨attentionè¿›è¡Œè¾“å…¥encodingï¼Œç”±äºattentionæœ¬è´¨ä¸Šåªæ˜¯å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå¹³å‡è¿ç®—ï¼Œè¿™å¯¼è‡´ç‰¹å¾æå–èƒ½åŠ›ä¸è¶³(æ¯”è¾ƒconvolutionåšçº¿æ€§å˜æ¢ï¼Œè€Œattentionåªæ˜¯åšäº†åŠ æƒå¹³å‡)ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ä½œè€…æå‡ºäº†å¤šå¤´æ³¨æ„åŠ›ï¼ˆï¼‰çš„æ–¹æ³•ã€‚å¤šå¤´æ³¨æ„åŠ›çš„åŸºæœ¬æ€æƒ³é€šè¿‡å¤šæ¬¡åˆå§‹åŒ–è¿‡ç¨‹å¢åŠ æ¨¡å‹æå–ä¸åŒç‰¹å¾çš„æœºä¼šï¼Œå‡è®¾ä¸‹å›¾ä¸­é€šè¿‡ä¸‰æ¬¡åˆå§‹åŒ–åˆ†åˆ«å¾—åˆ°äº†ä¸‰ç§ç‰¹å¾ï¼šçº¢è‰²è¡¨ç¤ºåŠ¨ä½œï¼Œç»¿è‰²è¡¨åšåŠ¨ä½œæ–½åŠ è€…ï¼Œè“è‰²è¡¨ç¤ºåŠ¨ä½œæ‰¿å—ç€ï¼Œå¯ä»¥çœ‹åˆ°åœ¨å¯¹â€œè¸¢â€œè¿›è¡Œäº†ä¸‰æ¬¡self attentionè¿ç®—ï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ç§ç‰¹å¾ã€‚åœ¨å¯¹äºåŠ¨ä½œä¿¡æ¯çš„self attentionä¸­ï¼Œ"æˆ‘â€œå’Œâ€çƒâ€œçš„æƒå€¼ï¼ˆç°è‰²ç»†çº¿è¡¨ç¤ºï¼‰æ¯”â€œè¸¢â€çš„æƒå€¼ï¼ˆçº¢è‰²ç²—çº¿ï¼‰è¦å°å¾ˆå¤šï¼›åŒæ ·ï¼Œå¯¹åŠ¨ä½œæ–½åŠ è€…çš„self attentionä¸­ï¼Œâ€œæˆ‘â€ï¼ˆç»¿è‰²ç²—çº¿ï¼‰åˆ™æ˜¯ä¸»è¦è´¡çŒ®è€…ã€‚åœ¨å°†ä¸‰æ¬¡self attentionçš„ç»“æœç›¸åŠ åï¼Œå¾—åˆ°çš„æ–°çš„â€œè¸¢â€çš„ç¼–ç ä¸­å°±åŒ…å«äº†ä¸‰ç§ç‰¹å¾çš„ä¿¡æ¯ã€‚ç°å®ä¸­ä¸å¯èƒ½æ¯æ¬¡éšæœºåˆå§‹åŒ–éƒ½èƒ½å¸¦æ¥æœ‰æ•ˆçš„ç‰¹å¾ï¼Œç†è®ºä¸Šéšæœºåˆå§‹åŒ–æµ‹æ¬¡æ•°è¶Šå¤šå°±è¶Šæœ‰å¯èƒ½å‘ç°æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä¸è¿‡éšä¹‹å¢é•¿çš„æ˜¯è®­ç»ƒå‚æ•°çš„å¢åŠ ï¼Œè¿™æ„å‘³ç€è®­ç»ƒéš¾åº¦çš„æé«˜ï¼Œå› æ­¤éœ€è¦å¹³è¡¡ï¼Œå†Transformeræ¨¡å‹ä¸­è¿™ä¸ªå€¼æ˜¯8ã€‚

![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vT4_Vn34rr1zN4OhXIo7oCGkzXDF__Y3CIVnZ_12fjqLHtKoRSJaVIyoR7ndQHtRlfNUmgecF5mucNg/pub?w=538&h=363)
å…·ä½“æ–¹æ³•æ˜¯å¯¹åŒä¸€ä¸ªå…ƒç´ è¿›è¡Œå¤šæ¬¡attentionè¿ç®—ï¼Œ æ¯æ¬¡attentionéƒ½ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–å‚æ•°$W$ï¼Œæœ€ååœ¨å°†å¤šæ¬¡attentionçš„ç»“æœç›¸åŠ ã€‚
åœ¨transformerä¸­å¯¹æ¯ä¸€ä¸ªå…ƒç´ $x_i$ï¼Œè¿›è¡Œ$h$æ¬¡(ï¼Œå¦‚word2vec,gloveï¼Œåçš„åºåˆ—å…ƒç´ )åˆå§‹åŒ–
$$head_i =\mathrm{SDPA}(QW^Q_i, KW_i^K, VW_i^V)$$
$$\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(head_i, ..., head_h)W^O$$

- å¯¹äºç¼–ç å™¨MHAï¼Œ$Q, K, V$éƒ½æ˜¯è¾“å…¥å…ƒç´ ç¼–ç $x_i$
- å¯¹äºè§£ç å™¨MHAï¼Œ$Q, K, V$éƒ½æ˜¯å·²ç”Ÿæˆçš„è¾“å‡ºå…ƒç´ ç¼–ç $y_i$
- å¯¹äºç¼–ç å™¨-è§£ç å™¨MHAï¼Œ $Q$æ˜¯è¾“å‡ºå…ƒç´ ç¼–ç $y_i$, $K,V$æ˜¯context vectorä¸­çš„å…ƒç´ $c_i$

![enter image description here](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png)

### ç¼–ç /è§£ç å±‚
transformeræ¨¡å‹ä¸­å°†å¤šå¤´æ³¨æ„åŠ›HMAè®¡ç®—åçš„ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œï¼Œè¿™é‡ŒæŒ‰ä½ä¸»è¦æ˜¯æŒ‡æ¯ä¸ªä½ç½®çš„å…ƒç´ å„è‡ªè¾“å…¥å‰é¦ˆç½‘ç»œé‡Œè¿›è¡Œè®¡ç®—ï¼Œç½‘ç»œé€šå¸¸ä¸º2å±‚ï¼Œä¸­é—´å±‚ç»´åº¦ç¨å¤§ï¼Œæœ€åä¸€å±‚çš„ç»´åº¦å’Œå…ƒç´ ç¼–ç çš„ç»´åº¦ç›¸åŒã€‚è¿™ä¸ªè®¾è®¡çš„ç›®çš„å…¶å®å’ŒHMAçš„è®¾è®¡ç±»ä¼¼ï¼Œç”±äºattentionåœ¨ç‰¹å¾åˆæˆèƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦å€ŸåŠ©å…¨è¿æ¥ç½‘ç»œçš„éçº¿æ€§è®¡ç®—æ¥å¢åŠ ç‰¹å¾åˆæˆçš„èƒ½åŠ›ã€‚
éœ€è¦æŒ‡å‡ºçš„æ˜¯è§£ç å±‚..._____________________________________
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vTFCzc5frUSM_IkIZ9W7XE92dfKzjh9M05OqTd8FDz3mZpPBTfO0cIVQ-Uk5ZItYZGzi119CYHUaGJk/pub?w=312&h=379)![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQPYuIriXvfFSANLnztpXorpe-MH71EMWvf0sO5EBwx1JZci48LUp6hM52ICNQ6-cga70MZe7UH6QAJ/pub?w=349&h=698)
> Like the name indicates, this is a regular feedforward network applied to _each_ time step of the Multi Head attention outputs. The network has three layers with a non-linearity like ReLU for the hidden layer. You might be wondering why do we need a feedforward network after attention; after all isnâ€™t attention all we need ğŸ˜ˆ ? I suspect it is needed to improve model expressiveness. As we saw earlier the multi head attention partitioned the inputs and applied attention independently. There was only a linear projection to the outputs, i.e. the partitions were combined only linearly. The _Positionwise Feedforward_ network thus brings in some non-linear â€˜mixingâ€™ if we call it that. In fact for the sequence tagging task we use convolutions instead of fully connected layers. A filter of width 3 allows interactions to happen with adjacent time steps to improve performance.

### Transformerå…¨è²Œ
åœ¨ä»‹ç»äº†Transformerçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ä¹‹åï¼Œæˆ‘ä»¬å†æ¥å®Œæ•´çœ‹ä¸€ä¸‹Transformeræ¨¡å‹ã€‚æ•´ä½“ä¸Šæ¥çœ‹ï¼ŒTransformeræ¨¡å‹å±äºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè§£ç å™¨éœ€è¦æ ¹æ®åºåˆ—ç¼–ç sequence embeddingï¼ˆç”±ç¼–ç å™¨ç”Ÿæˆï¼‰å’Œä¸Šä¸€æ­¥çš„è§£ç å™¨è¾“å‡ºæ¥äº§ç”Ÿä¸‹ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤å±äºè‡ªå›å½’(auto regressor)æ¨¡å‹ã€‚
![enter image description here](https://camo.githubusercontent.com/4b80977ac0757d1d18eb7be4d0238e92673bfaba/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f7472616e73666f726d65722e706e67)
ç¼–ç å™¨ç”±è‹¥å¹²ä¸ªï¼ˆNï¼‰ç›¸åŒçš„ç¼–ç å±‚å †å å½¢æˆï¼Œæ¯ä¸ªç¼–ç å±‚ä¸»è¦ç”±ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›HMAå’Œä¸€ä¸ªæŒ‰ä½å‰é¦ˆç½‘ç»œæ„æˆï¼Œä¸»è¦ä½œç”¨æ˜¯å°†åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯èå…¥æ¯ä¸ªå…ƒç´ å¹¶è¿›è¡Œç‰¹å¾åˆæˆã€‚åŸå§‹çš„è¾“å…¥ç¼–ç é¦–å…ˆç»è¿‡ä½ç½®ç¼–ç å™¨åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œåœ¨é€šè¿‡å¤šä¸ªç¼–ç å±‚ç”ŸæˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼Œå¤æ‚ç‰¹å¾ä¿¡æ¯çš„åºåˆ—ç¼–ç ï¼ˆcontext vector/sequence embeddingï¼‰ã€‚
è§£ç å™¨åŒæ ·æœ‰å¤šä¸ªï¼ˆNï¼‰è§£ç å±‚å †å è€Œæˆã€‚æ¯ä¸ªè§£ç å±‚éœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼Œç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ä¸Šä¸€æ­¥çš„è§£ç å™¨è¾“å‡ºï¼ˆç¬¬ä¸€ä¸ªè§£ç å™¨è¾“å‡ºç”±ä¸€ä¸ªå›ºå®šçš„æ ‡è¯†ç¼–ç å……å½“ï¼‰ï¼Œè¿™ä¸ªè¾“å…¥é¦–å…ˆè¦é€šè¿‡ä½ç½®ç¼–ç å™¨åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œç„¶åé€šè¿‡è§£ç å™¨çš„å¸¦é®ç½©çš„è‡ªæ³¨æ„åŠ›MHAï¼ˆå›¾ä¸­Masked Multi-Head Attentionï¼‰åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°å·²è¾“å‡ºå…ƒç´ ï¼Œä¹‹ååŠ å…¥ç¬¬äºŒä¸ªè¾“å…¥å³åºåˆ—ç¼–ç ï¼Œé€šè¿‡è¿›è¡Œç¼–ç å™¨-è§£ç å™¨MHAåŠ å…¥åºåˆ—ç¼–ç sequence embeddingä¸­çš„æ¥è‡ªç¼–ç å™¨çš„ç‰¹å¾ä¿¡æ¯ï¼Œæœ€ååœ¨ç»è¿‡æŒ‰ä½å‰é¦ˆç½‘ç»œåˆæˆå¤æ‚ç‰¹å¾ã€‚ç»è¿‡å¤šä¸ªè§£ç å±‚å¤„ç†ååœ¨é€šè¿‡å…¨è¿æ¥è¿ç®—æ˜ å°„åˆ°ç›®æ ‡è¯å…¸ç©ºé—´ï¼Œæœ€åé€šè¿‡softmaxé€‰æ‹©å¯èƒ½æ€§æœ€å¤§çš„å…ƒç´ ä½œä¸ºè¾“å‡ºã€‚
å·¥ä½œæµç¨‹ï¼š
1. è¾“å…¥å…ƒç´ è¿›è¡Œä½ç½®ç¼–ç 
2. ä½ç½®ç¼–ç ä¸è¾“å…¥å…ƒç´ æŒ‰ä½ç›¸åŠ 
3. åœ¨ç¼–ç å±‚
	3.1 é¦–å…ˆè¿›è¡Œè¾“å…¥å…ƒç´ è‡ªæ³¨æ„åŠ›ï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰è®¡ç®—ï¼Œ
	3.2 å†å°†ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œ
4. é‡å¤å¤šæ¬¡ç¼–ç å±‚ç»“ç®—ï¼Œç»“æŸç¼–ç é˜¶æ®µï¼Œå¾—åˆ°context vector
5. å¼€å§‹è§£ç é˜¶æ®µï¼Œé¦–å…ˆå¯¹è¾“å‡ºå…ƒç´ è¿›è¡Œä½ç½®ç¼–ç ï¼ˆç¬¬ä¸€ä¸ªè¾“å‡ºä¸ºå¼€å§‹æ ‡è®°ï¼‰
6. è¾“å…¥å…ƒç´ ä¸å…¶ä½ç½®ç¼–ç æŒ‰ä½ç›¸åŠ 
7. åœ¨è§£ç å±‚
	7.1 é¦–å…ˆè¿›è¡Œè¾“å‡ºå…ƒç´ ï¼ˆå½“å‰å·²è¾“å‡ºï¼‰çš„å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—
	7.2 è¿›è¡Œç¼–ç ï¼ˆcontext vectorï¼‰-è§£ç ï¼ˆ7.1ç»“æœï¼‰æ³¨æ„åŠ›è®¡ç®—
	7.3 å¯¹7.2ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œ
8. é‡å¤å¤šæ¬¡è§£ç å±‚è®¡ç®—
9. é€šè¿‡å…¨è¿æ¥ç½‘ç»œè½¬åŒ–ä¸ºç›®æ ‡è¯å…¸å®½åº¦å‘é‡
10. ä½¿ç”¨softmaxç¡®å®šè¾“å‡ºå…ƒç´ ï¼ˆå¯èƒ½æ€§æœ€å¤§ï¼‰
11.  å°†å½“å‰è¾“å‡ºå…ƒç´ è¾“å…¥6å¼€å§‹ä¸‹ä¸€ä¸ªè¾“å‡ºå…ƒç´ çš„è®¡ç®—ï¼Œç›´åˆ°è¾“å‡ºä¸ºç»“æŸæ ‡è®°ç¬¦

æ€»ç»“ä¸€ä¸‹ï¼Œattentionæ˜¯transformerçš„æ ¸å¿ƒï¼Œå®ƒå…·æœ‰è®¡ç®—æ•ˆç‡é«˜ï¼ˆå°¤å…¶å¯¹äºé•¿åºåˆ—ï¼‰ï¼Œå¯å¹¶è¡Œï¼Œå®¹æ˜“è®­ç»ƒç­‰ä¼˜åŠ¿ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿå¸¦äº†ä¸€äº›æ–°é—®é¢˜ï¼šæ¯”å¦‚æ— åºå’Œç‰¹å¾åˆæˆèƒ½åŠ›ä¸‹é™ã€‚Transformeré’ˆå¯¹è¿™äº›æ–°é—®é¢˜åˆ†åˆ«æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œå¦‚ä½¿ç”¨ä½ç½®ç¼–ç ç”Ÿæˆä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›å’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œå¢å¼ºç‰¹å¾åˆæˆèƒ½åŠ›ã€‚

## Transformerä¼˜åŒ–æŠ€å·§
ç”±äºTransformerå±äºæ¯”è¾ƒå¤æ‚çš„æ·±åº¦æ¨¡å‹ï¼Œå› æ­¤è¦é€šè¿‡ä½¿ç”¨ä¸€äº›ä¼˜åŒ–æŠ€å·§æ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚Transformerä¸­è¿ç”¨åˆ°çš„ä¼˜åŒ–æŠ€æœ¯æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬é€‰æ‹©å…¶ä¸­æ¯”è¾ƒé‡è¦æˆ–è€…æ˜¯æœ‰è¶£çš„æ¥è¿›è¡Œç®€å•ä»‹ç»
### 1. æ®‹å·®é“¾æ¥(residual connection)
ç½‘ç»œè¶Šæ·±ï¼Œè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œæ‰€ä»¥åœ¨éœ€è¦è¡¨è¾¾å¤æ‚ç‰¹å¾ï¼ˆå¦‚NLPï¼Œå›¾åƒï¼‰çš„åœºæ™¯ä¸­ä½¿ç”¨çš„ç¥ç»ç½‘ç»œæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ·±ï¼Œä½†æ˜¯æ·±å±‚ç½‘ç»œå¸¦æ¥äº†ä¸¤ä¸ªé—®é¢˜ï¼š1. æ¢¯åº¦å¼¥æ•£ã€çˆ†ç‚¸ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥è®­ç»ƒ 2. ç½‘ç»œé€€åŒ–degradationï¼Œå½“ç½‘ç»œæ·±åº¦åˆ°è¾¾ä¸€å®šåï¼Œæ€§èƒ½ä¸ä½†ä¸ä¼šéšç€æ·±åº¦çš„å¢åŠ ï¼Œåè€Œä¼šç”±æ€§èƒ½ä¸‹é™ã€‚
![enter image description here](https://www.google.com/url?sa=i&source=images&cd=&ved=2ahUKEwjAjajGrMblAhXB26QKHZfDBS0QjRx6BAgBEAQ&url=https://www.researchgate.net/figure/A-cell-from-the-Residual-Network-architecture-The-identity-connection-helps-to-reduce_fig4_326786331&psig=AOvVaw1UDvQHXM-esMFq1rcNP7FV&ust=1572606118049027)
æ®‹å·®é“¾æ¥ç”¨ä¸€ä¸ªç®€å•çš„åŠæ³•å·§å¦™çš„è§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å°†ä¸¤ä¸ªä¸ç›¸é‚»ç½‘ç»œå±‚ç›´æ¥è¿æ¥ï¼ˆçŸ­æ¥ï¼‰ã€‚è¿™æ ·æ¢¯åº¦gradientå¯ä»¥è·¨è¶Šä¸­é—´å±‚ç›´æ¥ä¼ é€’ï¼Œé¿å…ç»è¿‡ä¸­é—´å±‚æ—¶æ¢¯åº¦è¢«å¤šæ¬¡ç¼©æ”¾å¯¼è‡´æ¢¯åº¦å¼¥æ•£ï¼ˆçˆ†ç‚¸ï¼‰çš„é—®é¢˜ï¼›å¦ä¸€æ–¹é¢ï¼Œå®éªŒè¯æ˜å½“ä½¿ç”¨RELUä½œä¸ºæ¿€æ´»å‡½æ•°æ—¶ï¼Œæ®‹å·®è¿æ¥ä¹Ÿä»¥æœ‰æ•ˆé˜²æ­¢ç½‘ç»œé€€åŒ–ã€‚åŸå› ã€‚ã€‚ã€‚
åœ¨transformerä¸­çš„æ¯ä¸€ä¸ªç¼–ç å±‚ï¼ˆè§£ç å±‚ï¼‰éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥æ¥åˆ†åˆ«çŸ­æ¥å¤šå¤´æ³¨æ„åŠ›å’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œï¼Œè¿™æ ·åšä¸€æ¥è§£å†³äº†æ¢¯åº¦é—®é¢˜ï¼ŒåŒæ—¶è¿˜èƒ½å¸®åŠ©ä½ç½®ä¿¡æ¯é¡ºåˆ©ä¼ é€’åˆ°é«˜å±‚å»
### 2. Layer normalization
  Normalizationæ˜¯åœ¨æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„ä¸€ç§æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼Œä¸ºäº†æ›´æœ‰æ•ˆçš„è¿è¡Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œéœ€è¦å°†åŸå§‹æ•°æ®â€œç™½åŒ–â€Whiteningï¼Œä¹Ÿå°±æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­å¸¸å¸¸æåˆ°çš„ä½¿æ•°æ®â€œç‹¬ç«‹ï¼ŒåŒåˆ†å¸ƒâ€ã€‚
   ç›®å‰åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ˜¯BNï¼Œå®ƒæ˜¯å¯¹ä¸åŒè®­ç»ƒæ•°æ®çš„åŒä¸€ç»´åº¦è¿›è¡Œnormalizationï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆç¼“è§£æ·±åº¦æ¨¡å‹è®­ç»ƒä¸­çš„*æ¢¯åº¦çˆ†ç‚¸ã€å¼¥æ•£çš„é—®é¢˜*ã€‚è€Œåœ¨transformeré‡‡ç”¨äº†ç›¸å¯¹å†·é—¨çš„LNï¼Œä¸»è¦åŸå› æ˜¯BNå¾ˆéš¾åº”ç”¨åœ¨è®­ç»ƒæ•°æ®é•¿åº¦ä¸åŒçš„seq2seqä»»åŠ¡ä¸Šï¼Œè€Œè¿™æ­£æ˜¯LNçš„ä¼˜åŠ¿æ‰€åœ¨ï¼Œç”±äºLNæ˜¯ä½œç”¨åœ¨å•ä¸ªè®­ç»ƒæ•°æ®çš„ä¸åŒç»´åº¦ä¸Šï¼Œå› æ­¤å®ƒèƒ½å¤Ÿåœ¨ä¸€æ¡æ•°æ®ä¸Šè¿›è¡Œnormalization
  
### 3. æ ‡ç­¾å¹³æ»‘å½’ä¸€åŒ–label smoothing regularization
é€šå¸¸æˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæ¥è®¡ç®—é¢„æµ‹è¯¯å·®æ—¶ä½¿ç”¨ç‹¬çƒ­ï¼ˆone-hotï¼‰ç¼–ç è¡¨ç¤ºçœŸå®å€¼ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¸ºäº†å‡å°è¯¯å·®ä¼šå°½é‡ä½¿é¢„æµ‹ç»“æœæ¥è¿‘one-hotç¼–ç ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç½‘ç»œä¼šé©±ä½¿è‡ªèº«å¾€æ­£ç¡®æ ‡ç­¾å’Œé”™è¯¯æ ‡ç­¾å·®å€¼å¤§çš„æ–¹å‘å­¦ä¹ ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸è¶³ä»¥è¡¨å¾æ‰€ä»¥çš„æ ·æœ¬ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹ç»“æœçš„ç½®ä¿¡åº¦è¿‡é«˜ä¼šå¯¼è‡´ç½‘ç»œè¿‡æ‹Ÿåˆã€‚
æ ‡ç­¾å¹³æ»‘å½’ä¸€åŒ–é€šè¿‡"è½¯åŒ–"ä¼ ç»Ÿçš„ç‹¬çƒ­ç¼–ç ï¼Œä½¿å¾—è®­ç»ƒæ—¶èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶è¿‡æ‹Ÿåˆç°è±¡ã€‚å®ƒçš„å®ç°éå¸¸ç®€å•ï¼Œé€šè¿‡ä¸€ä¸ªè¶…å‚æ•°$\epsilon \in(0,1)$å°†åŸæ¥çš„0ï¼Œ1åˆ†å¸ƒå˜æˆ$\epsilon, 1-\epsilon$åˆ†å¸ƒï¼ˆå¯¹äºäºŒå€¼åˆ†ç±»é—®é¢˜ï¼‰ï¼Œè¿™æ ·å°±ç¼©çŸ­äº†çœŸå‡å€¼ä¹‹é—´çš„è·ç¦»ï¼Œæœ€ç»ˆèµ·åˆ°æŠ‘åˆ¶è¿‡æ‹Ÿåˆçš„æ•ˆæœã€‚
### 4. å­¦ä¹ ç‡çƒ­èº«Learning rate warm up
 è®­ç»ƒåˆæœŸç”±äºç¦»ç›®æ ‡è¾ƒè¿œï¼Œä¸€èˆ¬éœ€è¦é€‰æ‹©å¤§çš„å­¦ä¹ ç‡ï¼Œä½†å¦‚æœè®­ç»ƒæ•°æ®é›†å…·æœ‰é«˜åº¦çš„å·®å¼‚æ€§åˆ™ä½¿ç”¨è¿‡å¤§çš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´ä¸ç¨³å®šæ€§ã€‚è¿™æ˜¯ç”±äºå¦‚æœåˆå§‹åŒ–åçš„æ•°æ®æ°å¥½åªåŒ…å«ä¸€éƒ¨åˆ†ç‰¹å¾ï¼Œåˆ™æ¨¡å‹çš„åˆå§‹è®­ç»ƒå¯èƒ½ä¼šä¸¥é‡åå‘äºè¿™äº›ç‰¹å¾ï¼Œè¿™ä¼šå¢åŠ æ¨¡å‹å­¦ä¹ å…¶ä»–ç‰¹å¾çš„éš¾åº¦ã€‚
 æ‰€ä»¥å¯ä»¥åšä¸€ä¸ªå­¦ä¹ ç‡çƒ­èº«é˜¶æ®µï¼Œåœ¨å¼€å§‹çš„æ—¶å€™å…ˆä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„å­¦ä¹ ç‡ï¼Œç„¶åå½“è®­ç»ƒè¿‡ç¨‹ç¨³å®šçš„æ—¶å€™å†æŠŠå­¦ä¹ ç‡è°ƒå›å»ã€‚åœ¨é¢„çƒ­æœŸé—´ï¼Œå­¦ä¹ ç‡å‘ˆçº¿æ€§å¢åŠ ã€‚å¦‚æœç›®æ ‡å­¦ä¹ ç‡æ˜¯$p$ï¼Œé¢„çƒ­æœŸæ˜¯$n$ï¼Œåˆ™ç¬¬ä¸€æ‰¹è¿­ä»£å°†$p/n$ç”¨ä½œå­¦ä¹ ç‡ï¼›ç¬¬äºŒä¸ªä½¿ç”¨$2*p/n$ï¼Œä¾æ­¤ç±»æ¨ï¼šè¿­ä»£$i$ä½¿ç”¨$i*p/n$ï¼Œç›´åˆ°æˆ‘ä»¬åœ¨è¿­ä»£$n$è¾¾åˆ°å­¦ä¹ ç‡$p$ã€‚

## Transformerçš„æ”¹è¿›å’Œå‘å±•
Transformerå–å¾—å·¨å¤§æˆåŠŸå¼•èµ·å…³æ³¨ï¼Œå­¦æœ¯å’Œäº§ä¸šç•Œéƒ½åœ¨å°è¯•åœ¨å®ç°å’Œç†è®ºå±‚é¢å¯¹ä»–è¿›è¡Œæ”¹è¿›
### Transformer-XL
è™½ç„¶ç†è®ºä¸ŠTransformerå¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥ï¼Œä½†åœ¨å®é™…çš„è¿ç”¨ä¸­èµ„æºæ˜¯æœ‰é™çš„ï¼Œå› æ­¤Transformersç›®å‰ä½¿ç”¨å›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡æ¥å®ç°ï¼Œå³å°†ä¸€ä¸ªé•¿çš„æ–‡æœ¬åºåˆ—æˆªæ–­ä¸ºå‡ ç™¾ä¸ªå­—ç¬¦çš„å›ºå®šé•¿åº¦ç‰‡æ®µï¼Œç„¶ååˆ†åˆ«å¤„ç†æ¯ä¸ªç‰‡æ®µã€‚è¿™ç§æ“ä½œä¼šä½¿ç›¸é‚»å—ç‰‡æ®µä¹‹é—´çš„ä¸Šä¸‹æ–‡ä¸¢å¤±  ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€‚Transformer-XLåŸºäºä»¥ä¸‹ä¸¤ç§å…³é”®æŠ€æœ¯è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š
	- ç‰‡æ®µçº§é€’å½’æœºåˆ¶(segment-level recurrence mechanism) 
	ä¸»è¦è§£å†³ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–é—®é¢˜ï¼Œä½¿ä¸Šä¸‹æ–‡ä¿¡æ¯ç°åœ¨å¯ä»¥è·¨ç‰‡æ®µè¾¹ç•ŒæµåŠ¨ã€‚æ€è·¯æ˜¯å°†ä¸Šä¸€ç‰‡æ®µsegmentçš„memoryä¼ åˆ°ä¸‹ä¸€ç‰‡æ®µçš„åŒæ ·ä½ç½®
	![enter image description here](https://miro.medium.com/max/2152/1*Y3rxi7H06Ir-q_W2Q2zSIg.png)
	- ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆ(relative positional encoding scheme)ã€‚
	ç”±äºtransformerä¸Šçš„ä½ç½®ç¼–ç æ–¹æ¡ˆä¼šå¯¼è‡´ä¸åŒå—çš„å…ƒç´ å…·æœ‰ç›¸åŒçš„ä½ç½®ç¼–ç ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®ç¼–ç ï¼Œå®ƒæ˜¯æ¯ä¸ªattentionæ¨¡å—çš„ä¸€éƒ¨åˆ†ï¼ŒåŸºäºå…ƒç´ ä¹‹é—´çš„ç›¸å¯¹è·ç¦»è€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚

### å¹¶è¡ŒåŒ–
Despite not having any explicit recurrency, implicitly the model is built as an autoregressive one. It implies that in order to generate an output (both while training or during inference), the model needs to compute previous outputs, which is extremely costly, for the whole net has to be run for every output. Thatâ€™s the main idea to overcome in a recent paper by researchers at [_Salesforce Research_](https://einstein.ai/research/non-autoregressive-neural-machine-translation) and the University of Hong Kong, who tried to make the whole process parallelizable[23](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html#fn:23). Their proposal is to compute _fertilities_ for every input word in the sequence, and use it instead of previous outputs in order to compute the current output. This is summarized in the figure below.
å°½ç®¡æ²¡æœ‰ä»»ä½•æ˜¾å¼é€’å½’ï¼Œä½†æ˜¯éšå¼åœ°å°†æ¨¡å‹æ„å»ºä¸ºè‡ªå›å½’æ¨¡å‹ã€‚ è¿™æ„å‘³ç€ä¸ºäº†ç”Ÿæˆè¾“å‡ºï¼ˆåœ¨è®­ç»ƒæ—¶æˆ–åœ¨æ¨ç†æœŸé—´ï¼‰ï¼Œè¯¥æ¨¡å‹éœ€è¦è®¡ç®—å…ˆå‰çš„è¾“å‡ºï¼Œè¿™éå¸¸æ˜‚è´µï¼Œå› ä¸ºå¿…é¡»ä¸ºæ¯ä¸ªè¾“å‡ºè¿è¡Œæ•´ä¸ªç½‘ç»œã€‚ è¿™æ˜¯Salesforce Researchå’Œé¦™æ¸¯å¤§å­¦çš„ç ”ç©¶äººå‘˜åœ¨æœ€è¿‘çš„ä¸€ç¯‡è®ºæ–‡ä¸­è¦å…‹æœçš„ä¸»è¦æ€æƒ³ï¼Œä»–ä»¬è¯•å›¾ä½¿æ•´ä¸ªè¿‡ç¨‹å¯å¹¶è¡ŒåŒ–ã€‚ ä»–ä»¬çš„å»ºè®®æ˜¯ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªè¾“å…¥å•è¯è®¡ç®—è‚¥åŠ›ï¼Œå¹¶ä½¿ç”¨å®ƒä»£æ›¿å…ˆå‰çš„è¾“å‡ºä»¥è®¡ç®—å½“å‰è¾“å‡ºã€‚ ä¸‹å›¾å¯¹æ­¤è¿›è¡Œäº†æ€»ç»“ã€‚
![enter image description here](https://ricardokleinklein.github.io/images/transformer/fertilities.png)
## æ€»ç»“
Transformerä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå®ƒåœ¨NLPé¢†åŸŸå–å¾—çªç ´æ€§æˆç»©æ˜¯ç”±äºå®ƒé’ˆå¯¹æœºå™¨ç¿»è¯‘é¢†åŸŸåšäº†é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œæ¯”å¦‚positional enbemddingï¼Œ self attentionï¼Œ multihead attentionï¼Œå¹¶ç»“åˆäº†å¤šç§ç›¸å…³çš„ä¼˜åŒ–æŠ€å·§ï¼Œå¦‚residual connectionï¼Œlayer normalizationç­‰ã€‚
å› æ­¤ï¼Œå¯¹äºä»»ä½•ä»»åŠ¡ï¼Œéƒ½éœ€è¦é’ˆå¯¹ä»»åŠ¡ç›®æ ‡è¿›è¡Œç›¸å¯¹åº”è®¾è®¡ï¼Œå¹¶ä¸”è¦è¿›è¡Œä¼˜åŒ–æ‰èƒ½å……åˆ†å‘æŒ¥æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
ä¸€ä¸ªå¥½çš„æ¨¡å‹ä¸ä¼šä»å¤©è€Œé™ï¼Œè€Œæ˜¯éœ€è¦ä¸æ–­åœ°åˆ†æè§‰æ¥é—®é¢˜æ‰èƒ½é€æ¸å®Œå–„ï¼Œé€šè¿‡å¯¹Transformerçš„å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æŒæ¡å¯¹å·²æœ‰æ¨¡å‹è¿›è¡Œæ”¹è¿›çš„åŸºæœ¬æ€è·¯ï¼Œ1. æ‰¾åˆ°ç—›ç‚¹å¹¶é’ˆå¯¹ä¸»è¦é—®é¢˜è¿›è¡Œè®¾è®¡ï¼›2. å»ºç«‹æ ¸å¿ƒæ¨¡å‹åè¦å¯¹éšä¹‹äº§ç”Ÿçš„æ–°é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆï¼›3.é€šè¿‡å®éªŒè¿›è¡ŒéªŒè¯ï¼Œè¿˜æœ‰åˆ©ç”¨å·²æœ‰çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚

## Resources
[Attention is all you need review]([https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html))
[The transformer - Attention is all you need]([https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY))
[Building the Mighty Transformer for Sequence Tagging in PyTorch](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8))
[Walkthrough: The Transformer Architecture](https://www.lesswrong.com/posts/qscAeYE67GoSffDDA/walkthrough-the-transformer-architecture-part-1-2)
[The Transformer: Attention Is All You Need](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)
[How to code The Transformer in PyTorch](https://blog.floydhub.com/the-transformer-in-pytorch/)
[https://www.d2l.ai/chapter_attention-mechanism/transformer.html](https://www.d2l.ai/chapter_attention-mechanism/transformer.html)
[What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
[Paper Dissected: â€œAttention is All You Needâ€ Explained](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
[https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html](https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html)
[https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network](https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network)
[Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)
[The Transformer â€“ Attention is all you need.](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
[Create The Transformer With Tensorflow 2.0](https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/)
[æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://blog.csdn.net/songbinxu/article/details/80739447)
[nlpä¸­çš„Attentionæ³¨æ„åŠ›æœºåˆ¶+Transformerè¯¦è§£](https://zhuanlan.zhihu.com/p/53682800)
[Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc)
[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
[https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis](https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis)
[Seq2seq pay Attention to Self Attention: Part 2](https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d)
[Details Need More Attention: Transformer æ²¡æœ‰è¢«æåˆ°çš„ç»†èŠ‚](https://zhuanlan.zhihu.com/p/79987949)
[TRANSFORMERS FROM SCRATCH](http://www.peterbloem.nl/blog/transformers)
[Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding)
[When Does Label Smoothing Help?](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326)
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTUzNDg1MDEwLDE5MTUzNTAzNjgsLTEyOT
A0MzkzNjEsNjQyOTQyMjIsLTE1MzEzMjIyMDQsMjExNjcwNzY4
Myw4NDUzMjcwNzEsMjEyMjQ4ODM4MiwxNTcwMzIxMTI4LC0yMT
Q2NTg0NDQ0LDIzODgxODI3MywtMTA2NjEwNTk0NCwtMTEzOTQ4
Mzk3OCwtMTI0ODA5NzMwOSwtMTc3OTE4NzU1MiwtNTk2NjA1OD
Q4LDExNzQ4NDczNTgsMzM2Nzg3OTE3LDE4MTIyNTAzOTksLTY5
ODI4ODQxN119
-->