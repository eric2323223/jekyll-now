# Transformer-è®¾è®¡å’Œæ„å»ºé«˜æ•ˆçš„æ—¶åºæ¨¡å‹
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸï¼ŒRNNä¸€ç›´æ˜¯è¢«æœ€å¹¿æ³›ä½¿ç”¨çš„æ·±åº¦æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿‘å¹´æ¥CNNä¹Ÿé€æ¸è¢«ç”¨äºè¿›è¡Œã€‚ã€‚ã€‚ç„¶è€Œè¿™ä¸¤ç±»æ¨¡å‹éƒ½æœ‰ä¸€äº›éš¾ä»¥å…‹æœçš„é—®é¢˜ï¼ŒTransformerå°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜çš„æ–°å‹æ¨¡å‹ï¼Œå¹¶å–å¾—äº†éå¸¸å¥½çš„æ•ˆæœï¼Œå¤§æœ‰å–ä»£RNNåœ¨NLPé¢†åŸŸçš„ç»Ÿæ²»åœ°ä½çš„è¶‹åŠ¿ï¼Œæœ¬æ–‡æˆ‘ä»¬å°±æ¥ä¸€æ­¥æ­¥çš„åˆ†æå’Œç†è§£è¿™ä¸ªä¼˜ç§€çš„seq2seqæ¨¡å‹ã€‚

## åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼ˆseq2seqï¼‰
seq2seqé—®é¢˜æ˜¯ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ï¼‰è§£å†³çš„ä¸€ç±»å¸¸è§é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œè¯­æ€åˆ†æï¼Œæ‘˜è¦ç”Ÿæˆç­‰è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ï¼ˆNLPï¼‰ï¼Œè¿˜åŒ…æ‹¬_______ã€‚ è¿™ç±»é—®é¢˜çš„æœ€å¤§ç‰¹ç‚¹æ˜¯è¾“å…¥ï¼ˆæˆ–è¾“å‡ºï¼‰ä»¥åºåˆ—çš„å½¢å¼å‡ºç°ï¼Œåºåˆ—çš„é•¿åº¦å¯å˜ï¼Œä»»åŠ¡é€šå¸¸è¦æ±‚åˆ†ææ•´ä¸ªåºåˆ—æ‰èƒ½äº§ç”Ÿè¾“å‡ºâ€”â€”â€”â€”â€”â€”â€”â€”ã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰å¤„ç†seq2seqä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆencoder-decoderï¼‰æ¶æ„ï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºæ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ï¼Œè§£ç å™¨åˆ™å¯¹è¿™ä¸ªå†…éƒ¨è¡¨ç¤ºè¿›è¡Œè§£é‡Šã€‚
![enter image description here](https://img-blog.csdn.net/20180627114128329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hwdWxmYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)ä¼ ç»Ÿä¸Šæœ‰ä¸¤ç±»æ¨¡å‹ï¼š
- RNN
å¤„ç†seq2seqé—®é¢˜çš„ä¼ ç»Ÿæ–¹æ³•æ˜¯ä½¿ç”¨RNNæ¨¡å‹ï¼ŒRNNèƒ½å¤Ÿä¿å­˜çŠ¶æ€ï¼Œå®ƒå°†è¾“å…¥åˆ†ä¸ºå¤šæ­¥ï¼Œä¾é æ¯æ­¥è¾“å…¥å’Œä¸Šä¸€æ­¥çš„çŠ¶æ€æ›´æ–°å½“å‰çš„çŠ¶æ€ï¼ˆå’Œè¾“å‡ºï¼‰ï¼Œé€šè¿‡é‡å¤è¿™ç§æ­¥éª¤åœ¨è¯»å…¥æ‰€æœ‰åºåˆ—å…ƒç´ åå¾—åˆ°æ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ã€‚![enter image description here](https://miro.medium.com/max/2658/1*Ismhi-muID5ooWf3ZIQFFg.png)
ä»æ¨¡å‹ç»“æ„ä¸Šæ¥è¯´ç‰¹åˆ«é€‚åˆåºåˆ—åˆ°åºåˆ—é—®é¢˜ã€‚é—®é¢˜æœ‰ä¸‰ç‚¹
1. é•¿åºåˆ—çš„è®­ç»ƒå¾ˆå›°éš¾ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨é•¿åºåˆ—çš„è®­ç»ƒä¸­å®¹æ˜“å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ï¼Œè™½ç„¶LSTMå¯ä»¥æ”¹å–„è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åœ¨è¾ƒé•¿åºåˆ—çš„è®­ç»ƒä¸­ä»ç„¶æ— æ³•å®Œå…¨é¿å…ã€‚
2. åªèƒ½é¡ºåºæ‰§è¡Œï¼Œè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢
3. å›ºå®šçš„å­˜å‚¨ä¸é€‚åˆé•¿åºåˆ—
- CNN
CNNå¯ä»¥åŒæ—¶å¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œä½†æ˜¯ç”±äºå·ç§¯è¿ç®—çš„è§†åŸŸæœ‰é™ï¼Œä¸€æ¬¡å·ç§¯æ“ä½œåªèƒ½å¤„ç†æœ‰é™çš„å…ƒç´ ï¼Œå¯¹äºè¾ƒé•¿çš„åºåˆ—æ— æ³•å¤„ç†ã€‚è§£å†³åŠæ³•æ˜¯é€šè¿‡å †å å¤šå±‚å·ç§¯æ“ä½œæ¥é€æ¸å¢åŠ è§†åŸŸï¼Œä½†è¿™æ ·ä¼šä¸å¯é¿å…çš„å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ä¸”ä»æ²¡æœ‰å®Œå…¨è§£å†³é•¿åºåˆ—è¾“å…¥çš„å¤„ç†é—®é¢˜ï¼Œâ€”â€”â€”â€”â€”â€”â€”â€”è€Œä¸”å¢åŠ äº†æ¨¡å‹çš„å¤æ‚åº¦ï¼Œä½¿è¿ç®—å˜æ…¢ï¼Œè¿™å’Œåˆè¡·ä¸ç¬¦ã€‚

æ€»ç»“ä¸€ä¸‹ï¼Œä¸Šè¿°ä¸¤ç§æ¨¡å‹å¯¹äºé•¿åºåˆ—çš„å¤„ç†éƒ½æœ‰ç¼ºé™·ã€‚RNNéœ€è¦ä¸€æ­¥ä¸€æ­¥çš„å¤„ç†è¾“å…¥åºåˆ—ï¼ŒCNNåšå‡ºäº†ä¸€äº›æ”¹è¿›ä½†å¹¶ä¸å½»åº•ã€‚ä»æ ¹æœ¬ä¸Šçš„è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦èƒ½ä¸€æ¬¡æ€§çš„å¤„ç†å…¨éƒ¨è¾“å…¥ï¼ˆæ— è®ºåºåˆ—æœ‰å¤šé•¿ï¼‰ï¼Œå¹¶ä¸”èƒ½æ ¹æ®è¿™äº›è¾“å…¥ä¿¡æ¯åˆ†æåºåˆ—å…ƒç´ ä¹‹é—´çš„å…³è”å…³ç³»ã€‚äººä»¬ä»è‡ªå·±å¿«é€Ÿæµè§ˆçš„æ–¹å¼è·å¾—äº†å¯å‘ï¼Œå½“äººä»¬éœ€è¦å¿«é€Ÿæµè§ˆçš„æ—¶å€™ä¸ä¼šæŒ‰è¾“å…¥çš„é¡ºåºé€æ­¥é˜…è¯»ï¼Œè€Œä¼šç›´æ¥è·³åˆ°éœ€è¦å…³æ³¨çš„çš„éƒ¨åˆ†ï¼Œè¿™ç§æ ¹æ®éœ€è¦åœ¨ä¸åŒä½ç½®è·³è·ƒçš„é˜…è¯»æ–¹å¼å’Œæ³¨æ„åŠ›ç›¸å…³ï¼Œå› æ­¤è¿™ç§æ–°çš„åºåˆ—å¤„ç†æ–¹å¼è¢«å‘½åä¸ºæ³¨æ„åŠ›æœºåˆ¶
![enter image description here](https://www.visionears.nl/images/babyproduct.jpg)
Attentionæœºåˆ¶æ¥è‡ªäºäººç±»è§†è§‰æ³¨æ„åŠ›æœºåˆ¶ã€‚äººä»¬è§†è§‰åœ¨æ„ŸçŸ¥ä¸œè¥¿çš„æ—¶å€™ä¸€èˆ¬ä¸ä¼šæ˜¯ä¸€ä¸ªåœºæ™¯ä»åˆ°å¤´çœ‹åˆ°å°¾æ¯æ¬¡å…¨éƒ¨éƒ½çœ‹ï¼Œè€Œå¾€å¾€æ˜¯æ ¹æ®éœ€æ±‚è§‚å¯Ÿæ³¨æ„ç‰¹å®šçš„ä¸€éƒ¨åˆ†ã€‚è€Œä¸”å½“äººä»¬å‘ç°ä¸€ä¸ªåœºæ™¯ç»å¸¸åœ¨æŸéƒ¨åˆ†å‡ºç°è‡ªå·±æƒ³è§‚å¯Ÿçš„ä¸œè¥¿æ—¶ï¼Œäººä»¬ä¼šè¿›è¡Œå­¦ä¹ åœ¨å°†æ¥å†å‡ºç°ç±»ä¼¼åœºæ™¯æ—¶æŠŠæ³¨æ„åŠ›æ”¾åˆ°è¯¥éƒ¨åˆ†ä¸Šã€‚
> Attention is a method for aggregating a set of vectors  vivi  into just one vector, often via a lookup vector  uu. Usually,  vivi  is either the inputs to the model or the hidden states of previous time-steps, or the hidden states one level down (in the case of stacked LSTMs).
> 
> The result is often called the context vector  cc, since it contains
> the  _context_  relevant to the current time-step.
> 
> This additional context vector  cc  is then fed into the RNN/LSTM as
> well (it can be simply concatenated with the original input).
> Therefore, the context can be used to help with prediction.
> 
> The simplest way to do this is to compute probability vector 
> p=softmax(VTu)p=softmax(VTu)  and  c=âˆ‘ipivic=âˆ‘ipiviwhere  VV  is the
> concatenation of all previous  vivi. A common lookup vector  uu  is
> the current hidden state  htht.
> 
> There are many variations on this, and you can make things as
> complicated as you want. For example, instead using  vTiuviTu  as the
> logits, one may choose  f(vi,u)f(vi,u)  instead, where  ff  is an
> arbitrary neural network.
> 
> A common attention mechanism for sequence-to-sequence models uses 
> p=softmax(qTtanh(W1vi+W2ht))p=softmax(qTtanhâ¡(W1vi+W2ht)), where  vv 
> are the hidden states of the encoder, and  htht  is the current hidden
> state of the decoder.  qq  and both  WWs are parameters.
> 
> Some papers which show off different variations on the attention idea:
> 
> [Pointer Networks](https://arxiv.org/abs/1506.03134)  use attention to
> reference inputs in order to solve combinatorial optimization
> problems.
> 
> [Recurrent Entity Networks](https://arxiv.org/abs/1612.03969) 
> maintain separate memory states for different entities
> (people/objects) while reading text, and update the correct memory
> state usingf ttention attention.
> 
> [Transformer](https://arxiv.org/pdf/1706.03762.pdf)  models also make
> extensive use of attention. Their formulation of attention is slightly
> more general and also involves key vectors  kiki: the attention
> weights  pp  are actually computed between the keys and the lookup,
> and the context is then constructed with the  vivi.

## æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰
åŸºäºç»„æˆæ•´ä½“çš„å„ä¸ªå…ƒç´ åœ¨æ•´ä½“ä¸­å‘æŒ¥çš„ä½œç”¨ä¸ç›¸åŒè¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ€æƒ³æ˜¯é€šå¯¹ä½¿ç”¨ä¸åŒçš„æƒé‡ç»„åˆå„ä¸ªåºåˆ—å…ƒç´ æ¥æè¿°æ•´ä½“ï¼Œ~~è¿™å°±å¥½åƒæˆ‘ä»¬åœ¨å¿«é€Ÿè§‚å¯Ÿäººç‰©çš„ç…§ç‰‡æ—¶ä¼šæŠŠæ³¨æ„åŠ›æ›´å¤šçš„æ”¾åœ¨äººç‰©çš„é¢éƒ¨è€Œå‡ ä¹ä¸ä¼šç•™æ„èƒŒæ™¯ä¸­çš„æŸä¸€æ£µå°è‰~~ã€‚ä»æ•°å­¦è¿ç®—æ¥è®²ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯å¯¹ç»„æˆæ•´ä½“çš„å…ƒç´ åŠ æƒæ±‚å’Œçš„è¿‡ç¨‹ã€‚æƒå€¼çš„è®¡ç®—æ–¹æ³•ç”±ä»»åŠ¡ç›®æ ‡æ¥ç¡®å®šï¼Œè¿™å°±å¥½åƒã€‚ã€‚ã€‚å¯¹ã€‚ã€‚ã€‚çš„å…³æ³¨ç¨‹åº¦ä¸ä¸€è‡´æ˜¯ä¸€ä¸ªé“ç†ã€‚åœ¨æœºå™¨ç¿»è¯‘ï¼ˆä¸€ç§å¸¸è§çš„seq2seqä»»åŠ¡ï¼‰ä¸­ä¸€ç§å¸¸è§çš„æƒå€¼è¡¡é‡æ–¹æ³•æ˜¯è®¡ç®—åºåˆ—å…ƒç´ ï¼ˆå•è¯ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
æ³¨æ„åŠ›æœºåˆ¶æœ€æ—©ä½¿ç”¨åœ¨åŸºäº[RNNçš„æœºå™¨ç¿»è¯‘æ¨¡å‹](https://arxiv.org/pdf/1409.0473.pdf)ä¸­ï¼Œä¸åŒäºä»¥å¾€ä½¿ç”¨å›ºå®šçš„context vectorï¼Œ attentionèƒ½å¤Ÿè®©è§£ç å™¨æ¯æ¬¡è§£ç çš„æ—¶å€™å…³æ³¨æ›´ç›¸å…³çš„è¾“å…¥å…ƒç´ ï¼ˆç”ŸæˆåŠ¨æ€çš„context vectorï¼‰ä»è€Œæé«˜ç¿»è¯‘çš„å‡†ç¡®åº¦ã€‚

$$c_i=\sum_{j=1}\alpha_{ij}h_j$$
$$\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}exp(e_{ik})}$$
$$e_{ij}=alignment(h_i,x_j)$$

![enter image description here](https://oscimg.oschina.net/oscnet/5bdc25e12070e665409112ee13ac9e76603.jpg)

æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦ç”¨äºseq2seqä»»åŠ¡ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³å°±æ˜¯å¯¹åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ä»¥ä¸€å®šçš„è§„åˆ™åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸åŒäºRNNä¸­å…ˆé€šè¿‡ä¾æ¬¡åˆ†æè¾“å…¥å…ƒç´ æ¥é€æ­¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorçš„æ–¹å¼ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯¹è¿™äº›è¾“å…¥å…ƒç´ è¿›è¡ŒåŠ æƒå¹³å‡çš„æ–¹å¼æ¥ä¸€æ­¥åŠ å…¥æ‰€æœ‰å…ƒç´ ä¿¡æ¯æ¥ç”Ÿæˆä¸Šä¸‹æ–‡context vectorã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯èƒ½å¤Ÿä¸€æ­¥åˆ°ä½æ•æ‰åˆ°å…¨å±€çš„è”ç³»(åºåˆ—å…ƒç´ ç›´æ¥è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ),ä¸ä»…å¤§å¤§åŠ é€Ÿï¼ˆå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼‰äº†context vectorçš„ç”Ÿæˆï¼Œè€Œä¸”é¿å…äº†RNNçš„é•¿åºåˆ—è®­ç»ƒå›°éš¾çš„é—®é¢˜ã€‚
ä»å®ç°ä¸Šæ¥è®²ï¼Œattentionæ“ä½œå¯ä»¥ç†è§£ä¸ºåŠ æƒæ±‚å’Œçš„è¿ç®—ï¼ŒåŠ æ•°æ˜¯åºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œæƒå€¼è®¡ç®—æ–¹æ³•æ ¹æ®ä»»åŠ¡ç›®æ ‡è€Œä¸åŒï¼ˆåœ¨æœºå™¨ç¿»è¯‘çš„åœºæ™¯ä¸­ä½¿ç”¨ç›¸ä¼¼åº¦æ¥ä½œä¸ºæƒå€¼ï¼‰ã€‚ç”¨$\alpha$è¡¨ç¤ºæƒå€¼ï¼ˆé€šå¸¸è¡¨ç°ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå³$\sum \alpha=1$ï¼‰ï¼Œ$h$è¡¨ç¤ºåºåˆ—å…ƒç´ ï¼Œå¯ä»¥å°†attentionå½¢å¼åŒ–çš„è¡¨ç¤ºä¸º
$$y_2=w_{21}x_1+w_{22}x_2+w_{23}x_3+w_{24}x_4$$
$$y_i=\sum_{j=1}w_{ij}x_j$$
![enter image description here](http://www.peterbloem.nl/files/transformers/self-attention.svg)
ä»è¿™ä¸ªå®šä¹‰å¯ä»¥çœ‹å‡ºattentionçš„ç»“æœ$c$å°±æ˜¯åºåˆ—ä¸­æ‰€æœ‰å…ƒç´ æŒ‰ä¸€å®šçš„æ¯”ä¾‹å…³ç³»ç›¸åŠ å¾—åˆ°çš„ï¼Œç”±äº$c$å…·å¤‡äº†åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠattentionç†è§£ä¸º**å…ƒç´ åœ¨æŸä¸€ä¸ªåºåˆ—ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„é‡æ–°å®šä¹‰**ã€‚è¿™æ˜¯attentionæœ€æ ¸å¿ƒçš„ç‰¹ç‚¹ï¼Œä¹Ÿæ˜¯attentionèƒ½å¤Ÿå–ä»£RNNçš„åŸºç¡€ã€‚
æƒå€¼$w_{ij}$è¡¨ç¤º$x_j$åœ¨å¯¹äº$y_i$çš„è®¡ç®—ä¸­å‘æŒ¥çš„æƒé‡ï¼Œç”±äºæ‰€æœ‰$x$éƒ½å‚ä¸$y_i$çš„è®¡ç®—ï¼Œæ‰€ä»¥ä½¿ç”¨softmaxæ¥ä¿è¯æ‰€æœ‰æƒå€¼çš„å’Œç­‰äº1ã€‚
$$w_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}exp(e_{ik})}$$
è¿™é‡Œçš„$e_{ij}$è¡¨ç¤º$x_j$å’Œ$y_i$çš„ç›¸å…³æ€§ï¼Œå¯¹äºæœºå™¨ç¿»è¯‘ä»»åŠ¡æ¥è¯´ï¼Œé€šå¸¸ç”¨çŸ¢é‡ç›¸ä¼¼æ€§æ¥è¡¨è¿°å…ƒç´ çš„ç›¸å…³æ€§ï¼Œé€‚é‡ç›¸ä¼¼æ€§çš„è®¡ç®—æ–¹æ³•æœ‰å¾ˆå¤šï¼Œå…¶ä¸­æœ€å¸¸ç”¨çš„å°±æ˜¯ç‚¹ç§¯è¿ç®—ï¼ˆdot productï¼‰

$$e_{ij}=x_j\cdot y_i=|x_j||y_i|cos\theta$$ 
$\theta$è¡¨ç¤ºä¸¤ä¸ªå‘é‡$a,b$ä¹‹é—´çš„å¤¹è§’ï¼Œå¦‚æœ$a,b$è¶Šç›¸ä¼¼åˆ™å¤¹è§’$\theta$è¶Šå°ï¼Œ$cos\theta$åˆ™è¶Šæ¥è¿‘1

![enter image description here](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSO0ZVpogoaP-ipyQF0Xhir4wSrgGJBdeU_5wDrea6UD9sF7icIYg)
-   **æœ€å**ï¼Œä»ç‰©ç†æ„ä¹‰ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**ç›¸ä¼¼æ€§åº¦é‡**ã€‚
$$e_{ij}=Sim(h_i,x_j)$$

> **try to understand why K and V are different in transformer first!!!**
> Attention has a more generalized the form: XXXXXX
> 
> Goal is to learn $W_k, W_q, W_v$ so that 
-   **å…¶æ¬¡**ï¼Œä»å½¢å¼ä¸ŠAttentionå¯ä»¥ç†è§£ä¸º**é”®å€¼æŸ¥è¯¢**
å¯¹äºè¿›è¡Œç›¸ä¼¼æ€§è®¡ç®—å’Œâ€”â€”ä¸åŒçš„æƒ…å†µï¼ŒAttentionå¯ä»¥æ›´ä¸€èˆ¬çš„è¡¨ç¤ºä¸º
$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(Sim(Q,K))V$$
ä¸Šå¼è¡¨ç¤ºå¯¹äºæŸ¥è¯¢$q$å’Œé”®å€¼å¯¹$K,V$Given a query  **q**  and a set of key-value pairs  **(K, V)**, attention can be generalised to compute a weighted sum of the values dependent on the query and the corresponding keys.  
The query determines which values to focus on; we can say that the query â€˜attendsâ€™ to the values.

![enter image description here](https://ldzhangyx.github.io/2018/10/14/self-attention/1.jpg)


## Transformeræ¨¡å‹

Transformeræ¥è‡ªGoogle Brainå›¢é˜Ÿ2017å¹´çš„æ–‡ç« Attention is all you needã€‚æ­£å¦‚è®ºæ–‡çš„é¢˜ç›®æ‰€è¯´çš„ï¼Œæ•´ä¸ªç½‘ç»œç»“æ„å®Œå…¨æ˜¯ç”±Attentionæœºåˆ¶ç»„æˆã€‚ç”±äºæ²¡æœ‰ä½¿ç”¨RNNå’ŒCNNï¼Œé¿å…äº†æ— æ³•å¹¶è¡Œè®¡ç®—å’Œé•¿è·ç¦»ä¾èµ–ç­‰ä¼ ç»Ÿæ–¹æ³•æ— æ³•å…‹æœçš„é—®é¢˜ï¼Œç”¨æ›´å°‘çš„è®¡ç®—èµ„æºï¼Œå–å¾—äº†æ¯”è¿‡å»çš„ç»“æ„æ›´å¥½çš„ç»“æœï¼Œåœ¨æœºå™¨ç¿»è¯‘ä¸­å–å¾—äº†BLEUå€¼å¾—æ–°é«˜ã€‚
æ•´ä½“æ¶æ„ä¸Šçœ‹ï¼Œtransformerä»å±äºEncoder-Decoderæ¶æ„ï¼Œé€šè¿‡encoderå°†è¾“å…¥åºåˆ—è½¬æ¢æˆå†…éƒ¨è¡¨ç¤ºï¼Œå†é€šè¿‡ä¸åŒdecoderå®ç°ä¸åŒçš„é¢„æµ‹åŠŸèƒ½ã€‚ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œç¼–ç å™¨ä¸»è¦ç”±ä¸¤ç§ç»„ä»¶æ„æˆï¼š
![enter image description here](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-01-32.png)


![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSBNAHsyf_HP3_CkV1cygicnt0LhGxWcvw2PofecPP9TYJj41bghsAXTM6l6OSonSMvAjjgFInVDxC4/pub?w=961&h=590)


### ä¸ºä»€ä¹ˆAttention is all you need?
Transformerè®ºæ–‡çš„æ ‡é¢˜è¯´åªéœ€è¦attentionæ„å‘³ç€attentionå¯ä»¥å®Œæˆä»¥å‰éœ€è¦RNNæ‰èƒ½åšçš„å·¥ä½œã€‚ç”±äºRNNæœ‰å­˜å‚¨çš„èƒ½åŠ›ï¼Œå› æ­¤å¯ä»¥åœ¨ç¼–ç é˜¶æ®µé€šè¿‡ä¸æ–­çš„å¤„ç†å’Œç§¯ç´¯ä¸€ä¸ªä¸ªçš„è¾“å…¥å…ƒç´ ä»è€Œæœ€ç»ˆè·å¾—è¿™ä¸ªè¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆcontext vectorï¼‰ï¼ŒåŒæ ·åœ¨è§£ç é˜¶æ®µæ ¹æ®context vectoräº§ç”Ÿè¾“å‡ºã€‚åœ¨transformeræ¨¡å‹ä¸­è®¾è®¡äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ç”Ÿæˆconext vector
> Attentionæ˜¯transformerçš„æ ¸å¿ƒï¼Œå®ƒä¸ä»…ä½œç”¨åœ¨encoderåˆ°decoderçš„è½¬æ¢ä¸­ï¼Œè¿˜è¢«ç”¨åœ¨ç¼–ç å™¨ï¼ˆencoderï¼‰å’Œè§£ç å™¨ï¼ˆdecoderï¼‰å†…éƒ¨ï¼Œè¿™ç§åœ¨ç¼–ç è§£ç å™¨å†…éƒ¨ä½¿ç”¨çš„attentionè¢«ç§°ä¸ºè‡ªæ³¨æ„åŠ›self-attentionã€‚è‡ªæ³¨æ„åŠ›ç”¨äºæ›¿ä»£RNNæ¥åšencoding

#### è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰
> æ—¶åºé—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯NLPé—®é¢˜ï¼‰ä¸­çš„åºåˆ—å…ƒç´ è¡¨ç¤ºçš„å«ä¹‰é€šå¸¸ä¸æ­¢è¯¥å•ä¸ªå…ƒç´ çš„çš„å­—é¢æ„ä¹‰ï¼Œè€Œæ˜¯ä¸æ•´ä¸ªåºåˆ—ä¸Šä¸‹æ–‡æœ‰å…³ç³»ï¼Œå› æ­¤åœ¨encodingè¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘æ•´ä¸ªåºåˆ—æ¥å†³å®šå…¶ä¸­æ¯ä¸ªå…ƒç´ çš„æ„ä¹‰ã€‚self-attentionæœºåˆ¶å°±æ˜¯åŸºäºè¿™ç§ç”±å…¨å±€ç¡®å®šå±€éƒ¨çš„æ€æƒ³ï¼Œç®€å•æ¥è¯´å®ƒä½¿ç”¨æ•´ä¸ªåºåˆ—æ‰€æœ‰å…ƒç´ çš„**åŠ æƒ**å¹³å‡æ¥ç¡®å®šæ¯ä¸€ä¸ªå…ƒç´ åœ¨æ‰€å¤„åºåˆ—ï¼ˆä¸Šä¸‹æ–‡ï¼‰ä¸­çš„å«ä¹‰ã€‚

åœ¨encoder-decoderæ¨¡å‹ä¸­encoderè´Ÿè´£å°†è¾“å…¥è½¬åŒ–ä¸ºè¾“å…¥åºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆcontext vectorï¼‰ï¼Œä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨RNNé€šè¿‡ä¸€æ­¥æ­¥çš„å åŠ åˆ†æè¿‡çš„è¾“å…¥æ¥å¾—åˆ°æ•´ä¸ªåºåˆ—çš„å†…éƒ¨è¡¨ç¤ºï¼ˆå›ºå®šé•¿åº¦ï¼‰ï¼ŒTransformeræ¨¡å‹ä¸­ä½¿ç”¨è‡ªæ³¨æ„åŠ›ï¼ˆself attentionï¼‰æœºåˆ¶æ¥å®ç°encodingï¼Œä¹‹æ‰€ä»¥ç§°ä½œè‡ªæ³¨æ„åŠ›æ˜¯å› ä¸ºè¿™æ˜¯åœ¨è¾“å…¥åºåˆ—å†…éƒ¨è¿›è¡Œçš„attentionæ“ä½œï¼Œç”±äºattentionæ“ä½œå°±æ˜¯å¯¹å…ƒç´ è¿›è¡Œé‡æ–°å®šä¹‰ä½¿å…¶åŒ…å«åºåˆ—ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨è¾“å…¥åºåˆ—å…ƒç´ è¿›è¡Œattentionçš„æ“ä½œç»“æœå°±æ˜¯ä½¿è¯¥å…ƒç´ åŒ…å«è¾“å…¥åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤ç»è¿‡self attentionè¿ç®—çš„æ•´ä¸ªè¾“å…¥åºåˆ—çš„ç»“æœå°±æ˜¯å’Œä¸€ä¸ªè¾“å…¥åºåˆ—å¤§å°ä¸€è‡´çš„context vectorã€‚æ˜¾ç„¶ï¼Œself attentionä¸éœ€è¦æƒ³RNNé‚£æ ·ä¸€æ­¥æ­¥çš„å‡ºå…¥è¾“å…¥ï¼Œè€Œæ˜¯å¯ä»¥åŒæ—¶å¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œattentionè¿ç®—ï¼Œä»ä¸‹å›¾å¯ä»¥å‘ç°ï¼ŒRNNéœ€è¦åœ¨ä¾æ¬¡å¤„ç†å…ƒç´ x1, x2å’Œx3ä¹‹åæ‰èƒ½å¾—åˆ°æ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œattentionåˆ™å¯ä»¥åŒæ—¶å¤„ç†x1ï¼Œx2ï¼Œx3è€Œå¾—åˆ°åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQZ5I4YZtpZOU8xnxqqJ2WVd7o9eeo0sHQa119cWm4qR85KanMs7-Z1DV1EfKxJLQrZaVglHLUJGPF2/pub?w=856&h=225)


æ€»ç»“æ¥è¯´ï¼ŒAttentionæ¯”è¾ƒRNNæœ‰ä¸€ä¸‹ä¸‰ç‚¹ä¼˜åŠ¿
- å¯¹äºNLPçš„ä»»åŠ¡åœºæ™¯ï¼Œattentionçš„è®¡ç®—å¤æ‚åº¦æ›´ä½ï¼ˆdim>lengthï¼‰

||FLOPs|
|--|--|
| attention | $O(length^2 \cdot dim)$ |
| RNN | $O(length \cdot dim^2)$ |
| CNN | $O(length \cdot dim^2 \cdot kernelwidth)$ |
ç”±äºé€šå¸¸dimè¦å¤§äºlengthï¼Œæ‰€ä»¥self-attentionçš„è¿ç®—é‡ä¼šå°‘äºRNNå’ŒCNNï¼Œ

- åœ¨å¹¶è¡Œæ–¹é¢ï¼Œå¤šå¤´attentionå’ŒCNNä¸€æ ·ä¸ä¾èµ–äºå‰ä¸€æ—¶åˆ»çš„è®¡ç®—ï¼Œå¯ä»¥å¾ˆå¥½çš„å¹¶è¡Œï¼Œä¼˜äºRNNã€‚
- åœ¨é•¿è·ç¦»ä¾èµ–ä¸Šï¼Œç”±äºself-attentionæ˜¯æ¯ä¸ªè¯å’Œæ‰€æœ‰è¯éƒ½è¦è®¡ç®—attentionï¼Œæ‰€ä»¥ä¸ç®¡ä»–ä»¬ä¸­é—´æœ‰å¤šé•¿è·ç¦»ï¼Œæœ€å¤§çš„è·¯å¾„é•¿åº¦ä¹Ÿéƒ½åªæ˜¯1ã€‚å¯ä»¥æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚RNNåˆ™å­˜åœ¨æ¢¯åº¦å¼¥æ•£æˆ–è€…æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚
#### Attention mask
Attentionè¿™ç§æ–°çš„ç»“æ„ä½¿å¾—ä»–çš„è®­ç»ƒæ–¹å¼ä¹Ÿå’ŒRNNä¸åŒï¼Œè¿™æ˜¯ç”±äºAttentionå¯ä»¥ç›´æ¥çœ‹åˆ°æ‰€æœ‰çš„å…ƒç´ ï¼Œå› æ­¤éœ€è¦maskæ¥é˜²æ­¢â€”â€”â€”â€”â€”â€”ï¼Œ å…·ä½“æ¥çœ‹
- ç¼–ç å™¨self attentionï¼Œä¸éœ€è¦mask
- ç¼–ç å™¨-è§£ç å™¨attentionï¼Œéœ€è¦å¯¹paddingè¿›è¡Œmask
- è§£ç å™¨self attentionï¼Œéœ€è¦å¯¹å½“å‰ä½ç½®ä¹‹åçš„æ‰€æœ‰å…ƒç´ masking

#### Scaled Dot-Product Attention (SDPA)
Transformerå¯¹æ ‡å‡†çš„attentionåšäº†ä¸€ä¸ªå°å°è°ƒæ•´ï¼šåŠ å…¥ç‰¹å¾ç¼©æ”¾ï¼ˆfeature scalingï¼‰ã€‚è¿™æ ·åšä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢softmaxè¿ç®—å°†å€¼è¾ƒå¤§çš„keyè¿‡åº¦æ”¾å¤§ï¼Œå¯¼è‡´å…¶ä»–keyçš„ä¿¡æ¯å¾ˆéš¾åŠ å…¥åˆ°attentionç»“æœä¸­ã€‚
$$\mathrm{SDPA}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$
ç‰¹å¾ç¼©æ”¾ä½“ç°åœ¨å¯¹$Q$å’Œ$K$è®¡ç®—ç‚¹ç§¯$QK^T$ä»¥åï¼Œå¢åŠ äº†ä¸€æ­¥é™¤ä»¥$\sqrt{d_k}$è¿ç®—ã€‚
ä¸‹å›¾æ˜¯ä¸Šå¼çš„å›¾åƒåŒ–è¡¨ç¤ºï¼Œå…¶ä¸­Scaleå°±æ˜¯ç‰¹å¾ç¼©æ”¾çš„æ“ä½œã€‚

>å…¶ä¸­çš„æƒå€¼æ¥è‡ªè¯¥å…ƒç´ ä¸å…¶ä»–å…ƒç´ çš„ç›¸ä¼¼åº¦ï¼Œè¿™æ˜¯åŸºäºè¿™æ ·çš„å‡è®¾-ç›¸ä¼¼åº¦è¶Šé«˜çš„å…ƒç´ å¯¹ç¡®å®šè¯¥å…ƒç´ åœ¨æ•´ä¸ªåºåˆ—ä¸­çš„å«ä¹‰çš„è´¡çŒ®åº¦è¶Šå¤§ï¼Œç”±äºåºåˆ—å…ƒç´ ä»¥å‘é‡è¡¨ç¤ºï¼ˆword4vecï¼‰ï¼Œåœ¨transformerä¸­ä½¿ç”¨ç‚¹ç§¯è¿ç®—æ¥ç¡®å®šç›¸ä¼¼åº¦ï¼Œå…¶ç»“æœæ˜¯ä¸€ä¸ªæ•°å€¼ã€‚å½¢å¼åŒ–çš„å®šä¹‰ä¸º
$W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$

![enter image description here](https://miro.medium.com/max/676/1*nCznYOY-QtWIm8Y4jyk2Kw.png)


####  encoder-decoder attention
In terms of encoder-decoder, the **query** is usually the hidden state of the _decoder_. Whereas **key**, is the hidden state of the _encoder_, and the corresponding **value** is normalized weight, representing how much attention a _key_ gets. Output is calculated as a wighted sum â€“ here the dot product of _query_ and _key_ is used to get a _value_.

### ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰
ä¸RNNå’ŒCNNä¸åŒï¼Œåœ¨Attentionä¸­æ²¡æœ‰è¯åºçš„æ¦‚å¿µï¼ˆå¦‚ç¬¬ä¸€ä¸ªè¯ï¼Œç¬¬äºŒä¸ªè¯ç­‰ï¼‰ï¼Œ è¾“å…¥åºåˆ—çš„æ‰€æœ‰å•è¯éƒ½ä»¥æ²¡æœ‰ç‰¹æ®Šé¡ºåºæˆ–ä½ç½®çš„æ–¹å¼è¾“å…¥ç½‘ç»œï¼Œå› æ­¤æ¨¡å‹ä¸çŸ¥é“å•è¯çš„é¡ºåºã€‚ å› æ­¤ï¼Œéœ€è¦å°†ä¸ä½ç½®ç›¸å…³çš„ä¿¡å·æ·»åŠ åˆ°æ¯ä¸ªè¯ä¸­ï¼Œä»¥å¸®åŠ©æ¨¡å‹ç†è§£è¯çš„é¡ºåºã€‚
ä½ç½®ç¼–ç æ˜¯å•è¯å€¼åŠå…¶åœ¨å¥å­ä¸­ä½ç½®çš„é‡æ–°è¡¨ç¤ºï¼ˆå‡å®šå¼€å¤´å’Œç»“å°¾æˆ–ä¸­é—´çš„å¼€å¤´å’Œå¼€å¤´ä¸ç›¸åŒï¼‰ã€‚è€ƒè™‘åˆ°å¥å­çš„é•¿åº¦å¯ä»¥æ˜¯ä»»æ„é•¿åº¦ï¼Œåªè®¨è®ºè¯çš„ç»å¯¹ä½ç½®æ˜¯ä¸å…¨é¢çš„ï¼ˆåŒä¸€ä¸ªè¯ï¼Œåœ¨ç”±3ä¸ªè¯ç»„æˆçš„å¥å­ä¸­çš„ç¬¬ä¸‰ä¸ªä½ç½®å’Œ30ä¸ªè¯ç»„æˆçš„å¥å­ä¸­çš„ç¬¬ä¸‰ä¸ªä½ç½®æ‰€è¡¨è¾¾çš„æ„æ€å¾ˆå¯èƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼‰ã€‚
åœ¨Transformeræ¨¡å‹ä¸­åˆ©ç”¨äº†ä¸åŒé¢‘ç‡çš„å‘¨æœŸå‡½æ•°æ¥è¿›è¡Œä½ç½®ç¼–ç ï¼Œè¿™ç§ä½ç½®ç¼–ç æœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š
- ç”±äºsin/coså‡½æ•°çš„å‘¨æœŸæ€§å®ƒèƒ½å¤Ÿè¿›è¡Œä»»æ„é•¿åº¦åºåˆ—çš„ä½ç½®ç¼–ç 
- ä½¿ç”¨å¤šä¸ªä¸åŒé¢‘ç‡æ¥ä¿è¯ä¸ä¼šç”±äºå‘¨æœŸæ€§å¯¼è‡´ä¸åŒä½ç½®çš„ç¼–ç ç›¸åŒ
- ç¬¬äºŒæ˜¯ç”±äºsin/coså‡½æ•°çš„å€¼æ€»æ˜¯åœ¨-1åˆ°1ä¹‹é—´ï¼Œè¿™ç§ç¼–ç æœ¬èº«ä¹Ÿæœ‰æ­£åˆ™åŒ–ï¼ˆnormalizationï¼‰çš„ä½œç”¨ï¼Œè¿™æœ‰åˆ©äºç¥ç»ç½‘ç»œçš„å­¦ä¹ ã€‚

å¦‚æœç”¨$pos$è¡¨ç¤ºä½ç½®ï¼Œ$i$è¡¨ç¤ºå…ƒç´ ç¼–ç çš„ç»´åº¦ï¼Œ$d_{model}$è¡¨ç¤ºæ¨¡å‹çš„ç»´åº¦ï¼Œä½ç½®ç¼–ç $PE$å¯ä»¥è¡¨ç¤ºä¸º
$$PE_{{pos,2i}}=sin(pos/10000^{2i/d_{model}}) $$
$$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$

![enter image description here](http://vandergoten.ai/img/attention_is_all_you_need/positional_embedding.png)
è®¡ç®—äº§ç”Ÿçš„ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªä¸å…ƒç´ å…·æœ‰ç›¸åŒç»´åº¦çš„å‘é‡ï¼Œä½¿ç”¨ç›¸åŠ çš„æ–¹å¼å°†ä½ç½®ä¿¡æ¯å åŠ è¿›å…ƒç´ ä¸­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º
![enter image description here](https://wikidocs.net/images/page/31379/transformer6_final.PNG)
ä¸ºä½•é‡‡ç”¨ç›¸åŠ çš„æ–¹å¼ï¼Ÿ
> ç›´è§‰æ˜¯ï¼Œåœ¨é«˜ç»´ä¸­éšæœºé€‰æ‹©çš„å‘é‡å‡ ä¹æ€»æ˜¯è¿‘ä¼¼æ­£äº¤çš„ã€‚æ²¡æœ‰ç†ç”±è®¤ä¸ºå•è¯å‘é‡å’Œä½ç½®ç¼–ç å‘é‡ä¹‹é—´æœ‰ä»»ä½•å…³è”ã€‚å¦‚æœå•è¯åµŒå…¥å½¢æˆä¸€ä¸ªè¾ƒå°ç»´çš„å­ç©ºé—´ï¼Œè€Œä½ç½®ç¼–ç å½¢æˆå¦ä¸€ä¸ªè¾ƒå°ç»´çš„å­ç©ºé—´ï¼Œåˆ™ä¸¤ä¸ªå­ç©ºé—´æœ¬èº«å¯èƒ½è¿‘ä¼¼æ­£äº¤ï¼Œå› æ­¤å¤§æ¦‚å¯ä»¥å¯¹è¿™äº›å­ç©ºé—´è¿›è¡Œå˜æ¢ï¼Œå°½ç®¡è¿›è¡Œäº†çŸ¢é‡ç›¸åŠ ï¼Œä½†ä¸¤ä¸ªå­ç©ºé—´ä»å¯ä»¥é€šè¿‡ä¸€äº›å•ä¸ªå­¦ä¹ çš„å˜æ¢è€Œå½¼æ­¤ç‹¬ç«‹åœ°è¿›è¡Œæ“ä½œã€‚å› æ­¤ï¼Œä¸²è”å¹¶ä¸ä¼šå¢åŠ å¤ªå¤šï¼Œä½†ä¼šå¤§å¤§å¢åŠ å­¦ä¹ å‚æ•°æ–¹é¢çš„æˆæœ¬ã€‚

ä¸ºä»€ä¹ˆè¦åŒæ—¶ä½¿ç”¨sinå’Œcosï¼Œè€Œä¸åªä½¿ç”¨å…¶ä¸­çš„ä¸€ä¸ªï¼Ÿ
ä¸‹å›¾å¯è§
![enter image description here](https://i.stack.imgur.com/5QQmq.gif)

![enter image description here](https://i.stack.imgur.com/W0b0c.gif)


### å¤šå¤´æ³¨æ„åŠ›ï¼ˆ Multiple Headed Attention, MHA)

Transformerä»…ä»…ä½¿ç”¨attentionè¿›è¡Œè¾“å…¥encodingï¼Œç”±äºattentionæœ¬è´¨ä¸Šåªæ˜¯å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå¹³å‡è¿ç®—ï¼Œè¿™å¯¼è‡´ç‰¹å¾æå–èƒ½åŠ›ä¸è¶³(æ¯”è¾ƒconvolutionåšçº¿æ€§å˜æ¢ï¼Œè€Œattentionåªæ˜¯åšäº†åŠ æƒå¹³å‡)ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ä½œè€…æå‡ºäº†å¤šå¤´æ³¨æ„åŠ›ï¼ˆï¼‰çš„æ–¹æ³•ã€‚å¤šå¤´æ³¨æ„åŠ›çš„åŸºæœ¬æ€æƒ³é€šè¿‡å¤šæ¬¡åˆå§‹åŒ–è¿‡ç¨‹å¢åŠ æ¨¡å‹æå–ä¸åŒç‰¹å¾çš„æœºä¼šï¼Œå‡è®¾ä¸‹å›¾ä¸­é€šè¿‡ä¸‰æ¬¡åˆå§‹åŒ–åˆ†åˆ«å¾—åˆ°äº†ä¸‰ç§ç‰¹å¾ï¼šçº¢è‰²è¡¨ç¤ºåŠ¨ä½œï¼Œç»¿è‰²è¡¨åšåŠ¨ä½œæ–½åŠ è€…ï¼Œè“è‰²è¡¨ç¤ºåŠ¨ä½œæ‰¿å—ç€ï¼Œå¯ä»¥çœ‹åˆ°åœ¨å¯¹â€œè¸¢â€œè¿›è¡Œäº†ä¸‰æ¬¡self attentionè¿ç®—ï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ç§ç‰¹å¾ã€‚åœ¨å¯¹äºåŠ¨ä½œä¿¡æ¯çš„self attentionä¸­ï¼Œ"æˆ‘â€œå’Œâ€çƒâ€œçš„æƒå€¼ï¼ˆç°è‰²ç»†çº¿è¡¨ç¤ºï¼‰æ¯”â€œè¸¢â€çš„æƒå€¼ï¼ˆçº¢è‰²ç²—çº¿ï¼‰è¦å°å¾ˆå¤šï¼›åŒæ ·ï¼Œå¯¹åŠ¨ä½œæ–½åŠ è€…çš„self attentionä¸­ï¼Œâ€œæˆ‘â€ï¼ˆç»¿è‰²ç²—çº¿ï¼‰åˆ™æ˜¯ä¸»è¦è´¡çŒ®è€…ã€‚åœ¨å°†ä¸‰æ¬¡self attentionçš„ç»“æœç›¸åŠ åï¼Œå¾—åˆ°çš„æ–°çš„â€œè¸¢â€çš„ç¼–ç ä¸­å°±åŒ…å«äº†ä¸‰ç§ç‰¹å¾çš„ä¿¡æ¯ã€‚ç°å®ä¸­ä¸å¯èƒ½æ¯æ¬¡éšæœºåˆå§‹åŒ–éƒ½èƒ½å¸¦æ¥æœ‰æ•ˆçš„ç‰¹å¾ï¼Œç†è®ºä¸Šéšæœºåˆå§‹åŒ–æµ‹æ¬¡æ•°è¶Šå¤šå°±è¶Šæœ‰å¯èƒ½å‘ç°æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä¸è¿‡éšä¹‹å¢é•¿çš„æ˜¯è®­ç»ƒå‚æ•°çš„å¢åŠ ï¼Œè¿™æ„å‘³ç€è®­ç»ƒéš¾åº¦çš„æé«˜ï¼Œå› æ­¤éœ€è¦å¹³è¡¡ï¼Œå†Transformeræ¨¡å‹ä¸­è¿™ä¸ªå€¼æ˜¯8ã€‚

![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vT4_Vn34rr1zN4OhXIo7oCGkzXDF__Y3CIVnZ_12fjqLHtKoRSJaVIyoR7ndQHtRlfNUmgecF5mucNg/pub?w=538&h=363)
å…·ä½“æ–¹æ³•æ˜¯å¯¹åŒä¸€ä¸ªå…ƒç´ è¿›è¡Œå¤šæ¬¡attentionè¿ç®—ï¼Œ æ¯æ¬¡attentionéƒ½ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–å‚æ•°$W$ï¼Œæœ€ååœ¨å°†å¤šæ¬¡attentionçš„ç»“æœç›¸åŠ ã€‚
åœ¨transformerä¸­å¯¹æ¯ä¸€ä¸ªå…ƒç´ $x_i$ï¼Œè¿›è¡Œ$h$æ¬¡(ï¼Œå¦‚word2vec,gloveï¼Œåçš„åºåˆ—å…ƒç´ )åˆå§‹åŒ–
$$head_i =\mathrm{SDPA}(QW^Q_i, KW_i^K, VW_i^V)$$
$$\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(head_i, ..., head_h)W^O$$

- å¯¹äºç¼–ç å™¨MHAï¼Œ$Q, K, V$éƒ½æ˜¯è¾“å…¥å…ƒç´ ç¼–ç $x_i$
- å¯¹äºè§£ç å™¨MHAï¼Œ$Q, K, V$éƒ½æ˜¯å·²ç”Ÿæˆçš„è¾“å‡ºå…ƒç´ ç¼–ç $y_i$
- å¯¹äºç¼–ç å™¨-è§£ç å™¨MHAï¼Œ $Q$æ˜¯è¾“å‡ºå…ƒç´ ç¼–ç $y_i$, $K,V$æ˜¯context vectorä¸­çš„å…ƒç´ $c_i$

![enter image description here](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png)

### ç¼–ç /è§£ç å±‚
transformeræ¨¡å‹ä¸­å°†å¤šå¤´æ³¨æ„åŠ›HMAè®¡ç®—åçš„ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œï¼Œè¿™é‡ŒæŒ‰ä½ä¸»è¦æ˜¯æŒ‡æ¯ä¸ªä½ç½®çš„å…ƒç´ å„è‡ªè¾“å…¥å‰é¦ˆç½‘ç»œé‡Œè¿›è¡Œè®¡ç®—ï¼Œç½‘ç»œé€šå¸¸ä¸º2å±‚ï¼Œä¸­é—´å±‚ç»´åº¦ç¨å¤§ï¼Œæœ€åä¸€å±‚çš„ç»´åº¦å’Œå…ƒç´ ç¼–ç çš„ç»´åº¦ç›¸åŒã€‚è¿™ä¸ªè®¾è®¡çš„ç›®çš„å…¶å®å’ŒHMAçš„è®¾è®¡ç±»ä¼¼ï¼Œç”±äºattentionåœ¨ç‰¹å¾åˆæˆèƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦å€ŸåŠ©å…¨è¿æ¥ç½‘ç»œçš„éçº¿æ€§è®¡ç®—æ¥å¢åŠ ç‰¹å¾åˆæˆçš„èƒ½åŠ›ã€‚
éœ€è¦æŒ‡å‡ºçš„æ˜¯è§£ç å±‚..._____________________________________
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vTFCzc5frUSM_IkIZ9W7XE92dfKzjh9M05OqTd8FDz3mZpPBTfO0cIVQ-Uk5ZItYZGzi119CYHUaGJk/pub?w=312&h=379)![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vQPYuIriXvfFSANLnztpXorpe-MH71EMWvf0sO5EBwx1JZci48LUp6hM52ICNQ6-cga70MZe7UH6QAJ/pub?w=349&h=698)
> Like the name indicates, this is a regular feedforward network applied to _each_ time step of the Multi Head attention outputs. The network has three layers with a non-linearity like ReLU for the hidden layer. You might be wondering why do we need a feedforward network after attention; after all isnâ€™t attention all we need ğŸ˜ˆ ? I suspect it is needed to improve model expressiveness. As we saw earlier the multi head attention partitioned the inputs and applied attention independently. There was only a linear projection to the outputs, i.e. the partitions were combined only linearly. The _Positionwise Feedforward_ network thus brings in some non-linear â€˜mixingâ€™ if we call it that. In fact for the sequence tagging task we use convolutions instead of fully connected layers. A filter of width 3 allows interactions to happen with adjacent time steps to improve performance.

### Transformerå…¨è²Œ
åœ¨ä»‹ç»äº†Transformerçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ä¹‹åï¼Œæˆ‘ä»¬å†æ¥å®Œæ•´çœ‹ä¸€ä¸‹Transformeræ¨¡å‹ã€‚æ•´ä½“ä¸Šæ¥çœ‹ï¼ŒTransformeræ¨¡å‹å±äºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè§£ç å™¨éœ€è¦æ ¹æ®åºåˆ—ç¼–ç sequence embeddingï¼ˆç”±ç¼–ç å™¨ç”Ÿæˆï¼‰å’Œä¸Šä¸€æ­¥çš„è§£ç å™¨è¾“å‡ºæ¥äº§ç”Ÿä¸‹ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤å±äºè‡ªå›å½’(auto regressor)æ¨¡å‹ã€‚
![enter image description here](https://camo.githubusercontent.com/4b80977ac0757d1d18eb7be4d0238e92673bfaba/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f7472616e73666f726d65722e706e67)
ç¼–ç å™¨ç”±è‹¥å¹²ä¸ªï¼ˆNï¼‰ç›¸åŒçš„ç¼–ç å±‚å †å å½¢æˆï¼Œæ¯ä¸ªç¼–ç å±‚ä¸»è¦ç”±ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›HMAå’Œä¸€ä¸ªæŒ‰ä½å‰é¦ˆç½‘ç»œæ„æˆï¼Œä¸»è¦ä½œç”¨æ˜¯å°†åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯èå…¥æ¯ä¸ªå…ƒç´ å¹¶è¿›è¡Œç‰¹å¾åˆæˆã€‚åŸå§‹çš„è¾“å…¥ç¼–ç é¦–å…ˆç»è¿‡ä½ç½®ç¼–ç å™¨åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œåœ¨é€šè¿‡å¤šä¸ªç¼–ç å±‚ç”ŸæˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼Œå¤æ‚ç‰¹å¾ä¿¡æ¯çš„åºåˆ—ç¼–ç ï¼ˆcontext vector/sequence embeddingï¼‰ã€‚
è§£ç å™¨åŒæ ·æœ‰å¤šä¸ªï¼ˆNï¼‰è§£ç å±‚å †å è€Œæˆã€‚æ¯ä¸ªè§£ç å±‚éœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼Œç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ä¸Šä¸€æ­¥çš„è§£ç å™¨è¾“å‡ºï¼ˆç¬¬ä¸€ä¸ªè§£ç å™¨è¾“å‡ºç”±ä¸€ä¸ªå›ºå®šçš„æ ‡è¯†ç¼–ç å……å½“ï¼‰ï¼Œè¿™ä¸ªè¾“å…¥é¦–å…ˆè¦é€šè¿‡ä½ç½®ç¼–ç å™¨åŠ å…¥ä½ç½®ä¿¡æ¯ï¼Œç„¶åé€šè¿‡è§£ç å™¨çš„å¸¦é®ç½©çš„è‡ªæ³¨æ„åŠ›MHAï¼ˆå›¾ä¸­Masked Multi-Head Attentionï¼‰åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°å·²è¾“å‡ºå…ƒç´ ï¼Œä¹‹ååŠ å…¥ç¬¬äºŒä¸ªè¾“å…¥å³åºåˆ—ç¼–ç ï¼Œé€šè¿‡è¿›è¡Œç¼–ç å™¨-è§£ç å™¨MHAåŠ å…¥åºåˆ—ç¼–ç sequence embeddingä¸­çš„æ¥è‡ªç¼–ç å™¨çš„ç‰¹å¾ä¿¡æ¯ï¼Œæœ€ååœ¨ç»è¿‡æŒ‰ä½å‰é¦ˆç½‘ç»œåˆæˆå¤æ‚ç‰¹å¾ã€‚ç»è¿‡å¤šä¸ªè§£ç å±‚å¤„ç†ååœ¨é€šè¿‡å…¨è¿æ¥è¿ç®—æ˜ å°„åˆ°ç›®æ ‡è¯å…¸ç©ºé—´ï¼Œæœ€åé€šè¿‡softmaxé€‰æ‹©å¯èƒ½æ€§æœ€å¤§çš„å…ƒç´ ä½œä¸ºè¾“å‡ºã€‚
å·¥ä½œæµç¨‹ï¼š
1. è¾“å…¥å…ƒç´ è¿›è¡Œä½ç½®ç¼–ç 
2. ä½ç½®ç¼–ç ä¸è¾“å…¥å…ƒç´ æŒ‰ä½ç›¸åŠ 
3. åœ¨ç¼–ç å±‚
	3.1 é¦–å…ˆè¿›è¡Œè¾“å…¥å…ƒç´ è‡ªæ³¨æ„åŠ›ï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰è®¡ç®—ï¼Œ
	3.2 å†å°†ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œ
4. é‡å¤å¤šæ¬¡ç¼–ç å±‚ç»“ç®—ï¼Œç»“æŸç¼–ç é˜¶æ®µï¼Œå¾—åˆ°context vector
5. å¼€å§‹è§£ç é˜¶æ®µï¼Œé¦–å…ˆå¯¹è¾“å‡ºå…ƒç´ è¿›è¡Œä½ç½®ç¼–ç ï¼ˆç¬¬ä¸€ä¸ªè¾“å‡ºä¸ºå¼€å§‹æ ‡è®°ï¼‰
6. è¾“å…¥å…ƒç´ ä¸å…¶ä½ç½®ç¼–ç æŒ‰ä½ç›¸åŠ 
7. åœ¨è§£ç å±‚
	7.1 é¦–å…ˆè¿›è¡Œè¾“å‡ºå…ƒç´ ï¼ˆå½“å‰å·²è¾“å‡ºï¼‰çš„å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—
	7.2 è¿›è¡Œç¼–ç ï¼ˆcontext vectorï¼‰-è§£ç ï¼ˆ7.1ç»“æœï¼‰æ³¨æ„åŠ›è®¡ç®—
	7.3 å¯¹7.2ç»“æœè¾“å…¥æŒ‰ä½å‰é¦ˆç½‘ç»œ
8. é‡å¤å¤šæ¬¡è§£ç å±‚è®¡ç®—
9. é€šè¿‡å…¨è¿æ¥ç½‘ç»œè½¬åŒ–ä¸ºç›®æ ‡è¯å…¸å®½åº¦å‘é‡
10. ä½¿ç”¨softmaxç¡®å®šè¾“å‡ºå…ƒç´ ï¼ˆå¯èƒ½æ€§æœ€å¤§ï¼‰
11.  å°†å½“å‰è¾“å‡ºå…ƒç´ è¾“å…¥6å¼€å§‹ä¸‹ä¸€ä¸ªè¾“å‡ºå…ƒç´ çš„è®¡ç®—ï¼Œç›´åˆ°è¾“å‡ºä¸ºç»“æŸæ ‡è®°ç¬¦

æ€»ç»“ä¸€ä¸‹ï¼Œattentionæ˜¯transformerçš„æ ¸å¿ƒï¼Œå®ƒå…·æœ‰è®¡ç®—æ•ˆç‡é«˜ï¼ˆå°¤å…¶å¯¹äºé•¿åºåˆ—ï¼‰ï¼Œå¯å¹¶è¡Œï¼Œå®¹æ˜“è®­ç»ƒç­‰ä¼˜åŠ¿ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿå¸¦äº†ä¸€äº›æ–°é—®é¢˜ï¼šæ¯”å¦‚æ— åºå’Œç‰¹å¾åˆæˆèƒ½åŠ›ä¸‹é™ã€‚Transformeré’ˆå¯¹è¿™äº›æ–°é—®é¢˜åˆ†åˆ«æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œå¦‚ä½¿ç”¨ä½ç½®ç¼–ç ç”Ÿæˆä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›å’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œå¢å¼ºç‰¹å¾åˆæˆèƒ½åŠ›ã€‚

## Transformerä¼˜åŒ–æŠ€å·§
ç”±äºTransformerå±äºæ¯”è¾ƒå¤æ‚çš„æ¨¡å‹ï¼Œå› æ­¤è¦é€šè¿‡ä½¿ç”¨ä¸€äº›ä¼˜åŒ–æŠ€å·§æ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚ç”±äºTransformerä¸­è¿ç”¨åˆ°çš„ä¼˜åŒ–æŠ€æœ¯æ¯”è¾ƒå¤šï¼Œæˆ‘ä»¬é€‰æ‹©å…¶ä¸­æ¯”è¾ƒé‡è¦æˆ–è€…æ˜¯æœ‰è¶£çš„æ¥è¿›è¡Œç®€å•ä»‹ç»
1. æ®‹å·®é“¾æ¥(residual connection)
æ®‹å·®é“¾æ¥å¯ä»¥ç®—å¾—ä¸Šæ·±åº¦å­¦ä¹ ä¸­çš„ç¥å™¨ï¼Œç‰¹åˆ«é€‚åˆç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®­ç»ƒã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯
åœ¨Transformerä¸­
   - Help gradient propagated back through stacked decoders and encoders
   - Residuals carry positional information to higher layers, among other information.

2. Layer normalization
  Normalizationæ˜¯åœ¨æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„ä¸€ç§æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼Œä¸ºäº†æ›´æœ‰æ•ˆçš„è¿è¡Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œéœ€è¦å°†åŸå§‹æ•°æ®â€œç™½åŒ–â€Whiteningï¼Œä¹Ÿå°±æ˜¯åœ¨ç»Ÿè®¡å­¦ä¸­å¸¸å¸¸æåˆ°çš„ä½¿æ•°æ®â€œç‹¬ç«‹ï¼ŒåŒåˆ†å¸ƒâ€ï¼š
	  - ç‹¬ç«‹	ç‰¹å¾ä¹‹é—´ç›¸å…³ç³»è¦ä½
	  - åŒåˆ†å¸ƒ	æ‰€æœ‰ç‰¹å¾åº”è¯¥å…·æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®
  
   ç›®å‰åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ˜¯BNï¼Œå®ƒæ˜¯å¯¹ä¸åŒè®­ç»ƒæ•°æ®çš„åŒä¸€ç»´åº¦è¿›è¡Œnormalizationï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆç¼“è§£æ·±åº¦æ¨¡å‹è®­ç»ƒä¸­çš„æ¢¯åº¦çˆ†ç‚¸ã€å¼¥æ•£çš„é—®é¢˜ã€‚è€Œåœ¨transformeré‡‡ç”¨äº†ç›¸å¯¹å†·é—¨çš„LNï¼Œä¸»è¦åŸå› æ˜¯BNå¾ˆéš¾åº”ç”¨åœ¨è®­ç»ƒæ•°æ®é•¿åº¦ä¸åŒçš„seq2seqä»»åŠ¡ä¸Šï¼Œè€Œè¿™æ­£æ˜¯LNçš„ä¼˜åŠ¿æ‰€åœ¨ï¼Œç”±äºLNæ˜¯ä½œç”¨åœ¨å•ä¸ªè®­ç»ƒæ•°æ®çš„ä¸åŒç»´åº¦ä¸Šï¼Œå› æ­¤å®ƒèƒ½å¤Ÿåœ¨ä¸€æ¡æ•°æ®ä¸Šè¿›è¡Œnormalization
  
4. æ ‡ç­¾å¹³æ»‘å½’ä¸€åŒ–label smoothing regularization
é€šå¸¸æˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæ¥è®¡ç®—é¢„æµ‹è¯¯å·®æ—¶ä½¿ç”¨ç‹¬çƒ­ï¼ˆone-hotï¼‰ç¼–ç è¡¨ç¤ºçœŸå®å€¼ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¸ºäº†å‡å°è¯¯å·®ä¼šå°½é‡æ˜¯é¢„æµ‹ç»“æœæ¥è¿‘one-hotç¼–ç ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç½‘ç»œä¼šé©±ä½¿è‡ªèº«å¾€æ­£ç¡®æ ‡ç­¾å’Œé”™è¯¯æ ‡ç­¾å·®å€¼å¤§çš„æ–¹å‘å­¦ä¹ ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸è¶³ä»¥è¡¨å¾æ‰€ä»¥çš„æ ·æœ¬ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè¿™å°±ä¼šå¯¼è‡´ç½‘ç»œè¿‡æ‹Ÿåˆã€‚
æ ‡ç­¾å¹³æ»‘å½’ä¸€åŒ–é€šè¿‡"è½¯åŒ–"ä¼ ç»Ÿçš„ç‹¬çƒ­one-hotç±»å‹ç¼–ç ï¼Œä½¿å¾—åœ¨è®¡ç®—è¯¯å·®å€¼æ—¶èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶è¿‡æ‹Ÿåˆç°è±¡ã€‚å®ƒçš„å®ç°éå¸¸ç®€å•ï¼Œå¯¹äºäºŒå€¼åˆ†ç±»é—®é¢˜é€šè¿‡ä¸€ä¸ªè¶…å‚æ•°$\epsilon$å°†åŸæ¥çš„0ï¼Œ1åˆ†å¸ƒå˜æˆ$\epsilon, 1-\epsilon$åˆ†å¸ƒï¼Œè¿™æ ·å°±å¯ä»¥ç¼©çŸ­çœŸå‡å€¼ä¹‹é—´çš„è·ç¦»ï¼Œæœ€ç»ˆèµ·åˆ°æŠ‘åˆ¶è¿‡æ‹Ÿåˆçš„æ•ˆæœã€‚
5. warn-up learning rate
> If your data set is highly differentiated, you can suffer from a sort of "early over-fitting". If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all. Warm-up is a way to reduce the primacy effect of the early training examples. Without it, you may need to run a few extra epochs to get the convergence desired, as the model un-trains those early superstitions.
> Many models afford this as a command-line option. The learning rate is increased linearly over the warm-up period. If the target learning rate is  `p`  and the warm-up period is  `n`, then the first batch iteration uses  `1*p/n`  for its learning rate; the second uses  `2*p/n`, and so on: iteration  `i`  uses  `i*p/n`, until we hit the nominal rate at iteration  `n`.
> This means that the first iteration gets only 1/n of the primacy effect. This does a reasonable job of balancing that influence.
> Note that the ramp-up is commonly on the order of one epoch -- but is occasionally longer for particularly skewed data, or shorter for more homogeneous distributions. You may want to adjust, depending on how functionally extreme your batches can become when the shuffling algorithm is applied to the training set.


## Transformerçš„æ”¹è¿›å’Œå‘å±•
> ### Transformer çš„å±€é™æ€§
> Transformer æ— ç–‘æ˜¯å¯¹åŸºäºé€’å½’ç¥ç»ç½‘ç»œçš„ seq2seq æ¨¡å‹çš„å·¨å¤§æ”¹è¿›ã€‚ä½†å®ƒä¹Ÿæœ‰è‡ªèº«çš„å±€é™æ€§ï¼š
> -   æ³¨æ„åŠ›åªèƒ½å¤„ç†å›ºå®šé•¿åº¦çš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚åœ¨è¾“å…¥ç³»ç»Ÿä¹‹å‰ï¼Œæ–‡æœ¬å¿…é¡»è¢«åˆ†å‰²æˆä¸€å®šæ•°é‡çš„æ®µæˆ–å—ã€‚
> -   è¿™ç§æ–‡æœ¬å—ä¼šå¯¼è‡´**ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªå¥å­ä»ä¸­é—´åˆ†éš”ï¼Œé‚£ä¹ˆå¤§é‡çš„ä¸Šä¸‹æ–‡å°±ä¼šä¸¢å¤±ã€‚
> æ¢è¨€ä¹‹ï¼Œåœ¨ä¸è€ƒè™‘å¥å­æˆ–ä»»ä½•å…¶ä»–è¯­ä¹‰è¾¹ç•Œçš„æƒ…å†µä¸‹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†éš”ã€‚
 é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•å¤„ç†è¿™äº›éå¸¸é‡è¦çš„é—®é¢˜å‘¢ï¼Ÿè¿™å°±æ˜¯ä½¿ç”¨è¿‡ Transformer çš„äººä»¬æå‡ºçš„é—®é¢˜ã€‚ç”±æ­¤å‚¬ç”Ÿäº† Transformer-XLã€‚
åœ¨è¿™ç§æ¶æ„ä¸­ï¼Œåœ¨å…ˆå‰æ®µä¸­è·å¾—çš„éšçŠ¶æ€è¢«é‡ç”¨ä¸ºå½“å‰æ®µçš„ä¿¡æ¯å‘˜ã€‚å®ƒæ”¯æŒå¯¹é•¿æœŸä¾èµ–å»ºæ¨¡ï¼Œå› ä¸ºä¿¡æ¯å¯ä»¥ä»ä¸€ä¸ªæ®µæµå‘ä¸‹ä¸€ä¸ªæ®µã€‚

- Transformer-XL

Transformer æ¶æ„å¯ä»¥å­¦ä¹ é•¿æœŸä¾èµ–ã€‚ä½†æ˜¯ï¼Œç”±äºä½¿ç”¨å›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼ˆè¾“å…¥æ–‡æœ¬æ®µï¼‰ï¼Œå®ƒä»¬æ— æ³•æ‰©å±•åˆ°ç‰¹å®šçš„çº§åˆ«ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºç‚¹ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼š[ã€ŠTransformer-XLï¼šè¶…å‡ºå›ºå®šé•¿åº¦ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›è¯­è¨€æ¨¡å‹ã€‹](https://arxiv.org/pdf/1901.02860.pdf)ï¼ˆTransformer-XL: Attentive Language Models Beyond a Fixed-Length Contextï¼‰

- å¹¶è¡ŒåŒ–
Despite not having any explicit recurrency, implicitly the model is built as an autoregressive one. It implies that in order to generate an output (both while training or during inference), the model needs to compute previous outputs, which is extremely costly, for the whole net has to be run for every output. Thatâ€™s the main idea to overcome in a recent paper by researchers at [_Salesforce Research_](https://einstein.ai/research/non-autoregressive-neural-machine-translation) and the University of Hong Kong, who tried to make the whole process parallelizable[23](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html#fn:23). Their proposal is to compute _fertilities_ for every input word in the sequence, and use it instead of previous outputs in order to compute the current output. This is summarized in the figure below.
å°½ç®¡æ²¡æœ‰ä»»ä½•æ˜¾å¼é€’å½’ï¼Œä½†æ˜¯éšå¼åœ°å°†æ¨¡å‹æ„å»ºä¸ºè‡ªå›å½’æ¨¡å‹ã€‚ è¿™æ„å‘³ç€ä¸ºäº†ç”Ÿæˆè¾“å‡ºï¼ˆåœ¨è®­ç»ƒæ—¶æˆ–åœ¨æ¨ç†æœŸé—´ï¼‰ï¼Œè¯¥æ¨¡å‹éœ€è¦è®¡ç®—å…ˆå‰çš„è¾“å‡ºï¼Œè¿™éå¸¸æ˜‚è´µï¼Œå› ä¸ºå¿…é¡»ä¸ºæ¯ä¸ªè¾“å‡ºè¿è¡Œæ•´ä¸ªç½‘ç»œã€‚ è¿™æ˜¯Salesforce Researchå’Œé¦™æ¸¯å¤§å­¦çš„ç ”ç©¶äººå‘˜åœ¨æœ€è¿‘çš„ä¸€ç¯‡è®ºæ–‡ä¸­è¦å…‹æœçš„ä¸»è¦æ€æƒ³ï¼Œä»–ä»¬è¯•å›¾ä½¿æ•´ä¸ªè¿‡ç¨‹å¯å¹¶è¡ŒåŒ–23ã€‚ ä»–ä»¬çš„å»ºè®®æ˜¯ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªè¾“å…¥å•è¯è®¡ç®—è‚¥åŠ›ï¼Œå¹¶ä½¿ç”¨å®ƒä»£æ›¿å…ˆå‰çš„è¾“å‡ºä»¥è®¡ç®—å½“å‰è¾“å‡ºã€‚ ä¸‹å›¾å¯¹æ­¤è¿›è¡Œäº†æ€»ç»“ã€‚
![enter image description here](https://ricardokleinklein.github.io/images/transformer/fertilities.png)
## æ€»ç»“
Transformerä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå®ƒåœ¨NLPé¢†åŸŸå–å¾—çªç ´æ€§æˆç»©æ˜¯ç”±äºå®ƒé’ˆå¯¹æœºå™¨ç¿»è¯‘é¢†åŸŸåšäº†é’ˆå¯¹æ€§çš„è®¾è®¡ï¼Œæ¯”å¦‚positional enbemddingï¼Œ self attentionï¼Œ multihead attentionï¼Œå¹¶ç»“åˆäº†å¤šç§ç›¸å…³çš„ä¼˜åŒ–æŠ€å·§ï¼Œå¦‚residual connectionï¼Œlayer normalizationç­‰ã€‚
å› æ­¤ï¼Œå¯¹äºä»»ä½•ä»»åŠ¡ï¼Œéƒ½éœ€è¦é’ˆå¯¹ä»»åŠ¡ç›®æ ‡è¿›è¡Œç›¸å¯¹åº”è®¾è®¡ï¼Œå¹¶ä¸”è¦è¿›è¡Œä¼˜åŒ–æ‰èƒ½å……åˆ†å‘æŒ¥æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
ä¸€ä¸ªå¥½çš„æ¨¡å‹ä¸ä¼šä»å¤©è€Œé™ï¼Œè€Œæ˜¯éœ€è¦ä¸æ–­åœ°åˆ†æè§‰æ¥é—®é¢˜æ‰èƒ½é€æ¸å®Œå–„ï¼Œé€šè¿‡å¯¹Transformerçš„å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æŒæ¡å¯¹å·²æœ‰æ¨¡å‹è¿›è¡Œæ”¹è¿›çš„åŸºæœ¬æ€è·¯ï¼Œ1. æ‰¾åˆ°ç—›ç‚¹å¹¶é’ˆå¯¹ä¸»è¦é—®é¢˜è¿›è¡Œè®¾è®¡ï¼›2. å»ºç«‹æ ¸å¿ƒæ¨¡å‹åè¦å¯¹éšä¹‹äº§ç”Ÿçš„æ–°é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆï¼›3.é€šè¿‡å®éªŒè¿›è¡ŒéªŒè¯ï¼Œè¿˜æœ‰åˆ©ç”¨å·²æœ‰çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚

## Resources
[Attention is all you need review]([https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html))
[The transformer - Attention is all you need]([https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY))
[Building the Mighty Transformer for Sequence Tagging in PyTorch](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8))
[Walkthrough: The Transformer Architecture](https://www.lesswrong.com/posts/qscAeYE67GoSffDDA/walkthrough-the-transformer-architecture-part-1-2)
[The Transformer: Attention Is All You Need](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)
[How to code The Transformer in PyTorch](https://blog.floydhub.com/the-transformer-in-pytorch/)
[https://www.d2l.ai/chapter_attention-mechanism/transformer.html](https://www.d2l.ai/chapter_attention-mechanism/transformer.html)
[What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
[Paper Dissected: â€œAttention is All You Needâ€ Explained](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
[https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html](https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html)
[https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network](https://www.tensorflow.org/beta/tutorials/text/transformer#point_wise_feed_forward_network)
[Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)
[The Transformer â€“ Attention is all you need.](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
[Create The Transformer With Tensorflow 2.0](https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/)
[æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://blog.csdn.net/songbinxu/article/details/80739447)
[nlpä¸­çš„Attentionæ³¨æ„åŠ›æœºåˆ¶+Transformerè¯¦è§£](https://zhuanlan.zhihu.com/p/53682800)
[Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc)
[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
[https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis](https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/#attention-basis)
[Seq2seq pay Attention to Self Attention: Part 2](https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d)
[Details Need More Attention: Transformer æ²¡æœ‰è¢«æåˆ°çš„ç»†èŠ‚](https://zhuanlan.zhihu.com/p/79987949)
[TRANSFORMERS FROM SCRATCH](http://www.peterbloem.nl/blog/transformers)
[Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEwNTMwNzc0NzgsLTUzNTUyNDQxNCwxNj
kwMTEwOTMyLDEwNDU5MTg5MjEsMTcyODM0NzM1MSwxMTg2MDM5
MzcwLC03MDczMDQ0MzYsMTEyMTMyNjQzNywxNjAzNTE1OTI5LD
M2OTg3Njg4MCwtMTI5ODc0MTUwOCwtMTE1MzIzODMwOSwxNDMy
OTgyNzg1LDE5MTg2NDA4MzcsLTIxMDIwOTM5NjEsNzQzNDAwOD
E3LDIwMDU0NzkzMzIsMTg4Nzc0MDU4Miw5NzU2ODE0NDgsMTA5
NDc4NTkxNl19
-->