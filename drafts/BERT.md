# NLP transfer learning 

# NLPçš„è¿ç§»å­¦ä¹ -BERTç¯‡
è¿ç§»å­¦ä¹ è®©æ™®é€šäººåº”ç”¨å¤æ‚å¼ºå¤§æ¨¡å‹è§£å†³å®é™…é—®é¢˜çš„æ·å¾„ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§èƒ½åŠ›ï¼ŒBERTåœ¨NLPé¢†åŸŸçš„ä¸€ç³»åˆ—ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°é«˜ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTçš„ç»“æ„ï¼Œç‰¹æ€§ï¼Œé¢„è®­ç»ƒæ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œå¹¶è¯•å›¾è§£é‡ŠBERTæ¨¡å‹è®¾è®¡èƒŒåçš„åŸå› ã€‚æœ€åå›å½’åº”ç”¨ï¼Œä»‹ç»äº†å¦‚ä½•åˆ©ç”¨BERTé¢„è®­ç»ƒæ¨¡å‹åœ¨colabå¹³å°å¿«é€Ÿå®ç°æ™ºèƒ½é—®ç­”ã€‚
1. è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
    1.1 NLPçš„è¿ç§»å­¦ä¹ 
    1.2 è¯­è¨€æ¨¡å‹
2. BERTç®€ä»‹
3. BERTæ¨¡å‹ç»“æ„
    3.1 ç¼–ç å±‚
    3.2 Transformerç¼–ç å™¨
4. BERTçš„é¢„è®­ç»ƒ
    4.1 ä»»åŠ¡è®¾è®¡
    4.2 é¢„è®­ç»ƒæµç¨‹
    4.3 ä¼˜åŒ–
5. BERTçš„å¾®è°ƒ
    5.1 æƒ…ç»ªåˆ†æä»»åŠ¡
    5.2 åç§°å®ä½“è¯†åˆ«NERä»»åŠ¡
    5.3 é€šç”¨è¯­è¨€ç†è§£GLUEä»»åŠ¡
    5.4 é—®ç­”SQuADä»»åŠ¡
6. BERTçš„åº”ç”¨-å®ç°ä¸€ä¸ªæ™ºèƒ½é—®ç­”æœºå™¨äºº
    6.1 ç¯å¢ƒæ­å»º
    6.2 å®éªŒæµç¨‹
7. æ€»ç»“
# Training objective of self-supervised learning - From Word2Vec to Elmo to Bert to XLNet

self-supervised learning is important area because it can greatly reduce the effort of training deep model, 

ä½œä¸ºNLPè¿ç§»å­¦ä¹ çš„æˆåŠŸåº”ç”¨ï¼ŒBERTè¯æ˜äº†ã€‚ã€‚ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTæ¨¡å‹çš„ç»“æ„å’Œè®¾è®¡åŸç†ï¼Œä»¥åŠBERTçš„åº”ç”¨ã€‚
## è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
![enter image description here](https://miro.medium.com/max/3283/1*Z11P-CjNYWBofEbmGQrptA.png)
è¿ç§»å­¦ä¹ æ—¨åœ¨é€šè¿‡é‡ç”¨ ã€‚ã€‚ã€‚æ¥åŠ é€Ÿå­¦ä¹ å’Œå¢å¼ºé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¯¹äºå½“ä»Šè¶Šæ¥è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œéœ€è¦å·¨å¤§çš„äººåŠ›ç‰©åŠ›å’Œæ—¶é—´æˆæœ¬ã€‚ã€‚ã€‚ä½¿ç”¨è¿ç§»å­¦ä¹ æ˜¯éå¸¸æœ‰æ„ä¹‰çš„ã€‚é€šè¿‡å†imagenetè®­ç»ƒè§†è§‰ç‰¹å¾æå–ç½‘ç»œï¼Œæ•°æ®æ¯”è¾ƒä»å¤´è®­ç»ƒå’Œä½¿ç”¨è¿ç§»è®­ç»ƒã€‚ã€‚ã€‚
ç°å®çš„é—®é¢˜æ˜¯è·å–è¶³å¤Ÿçš„æ ‡è®°æ•°æ®éå¸¸å›°éš¾ï¼Œå› æ­¤
### NLPçš„è¿ç§»å­¦ä¹ 
æˆ‘ä»¬çŸ¥é“åœ¨CVä¸­çš„è¿ç§»å­¦ä¹ è¿‡ç¨‹æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„çš„å›¾åƒç‰¹å¾æå–æ¨¡å‹ï¼ˆå¦‚VGG19ï¼Œ ResNet50ç­‰ï¼‰ï¼Œå†ç»“åˆä¸‹æ¸¸ä»»åŠ¡éœ€è¦é€šè¿‡æ‰©å±•ç¬¬ä¸€é˜¶æ®µçš„æ¨¡å‹æ¥è¿›è¡Œfine tuningã€‚è¿›è¡Œä¸CVä»»åŠ¡ç±»ä¼¼ï¼Œåº”ç”¨è¿ç§»å­¦ä¹ è§£å†³NLPé—®é¢˜ä¹Ÿå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å‡ºå¯é‡ç”¨çš„ç‰¹å¾æå–æ¨¡å‹ï¼Œä¹Ÿå«é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºNLPä¸»è¦å…³æ³¨è¯­è¨€ï¼ˆå­—ç¬¦åºåˆ—ï¼‰çš„ç†è§£å’Œå¤„ç†ï¼Œä½œä¸ºè¯­è¨€åŸºæœ¬ç»„æˆå•ä½çš„è¯ï¼ˆwordï¼‰ä¹Ÿå°±è‡ªç„¶æˆä¸ºäº†é¢„è®­ç»ƒçš„å…³æ³¨ç‚¹ã€‚é¢„è®­ç»ƒçš„ç›®æ ‡ç»å†é€æ­¥çš„å‘å±•å˜åŒ–
#### é¢„è®­ç»ƒ
- output: embeddings
	- é™æ€è¯ç¼–ç ï¼ˆstatic word embeddingï¼‰ï¼Œæ¯”Word2Vecï¼ŒGloveç­‰ï¼Œé¡¾åæ€ä¹‰è¿™ç±»ç¼–ç èµ‹äºˆæ¯ä¸ªè¯å›ºå®šçš„ç¼–ç å€¼ï¼Œå¹¶ä¸”ç¼–ç å€¼ä½“ç°äº†è¯çš„ä»£è¡¨çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ç¼–ç å€¼çš„è¿ç®—å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœï¼Œæ¯”å¦‚è‘—åçš„ä¾‹å­
***king â€” man + woman = queen***
![enter image description here](https://miro.medium.com/max/634/1*dm9dudL37B6JG8saeR3zIw.png)

	- è¯­å¢ƒè¯ç¼–ç ï¼ˆcontextualized word embeddingï¼‰ï¼Œé™æ€è¯ç¼–ç çš„æœ€å¤§çš„é—®é¢˜åœ¨äºå®ƒåªèƒ½ä¸ªæ¯ä¸€ä¸ªè¯ä¸€ä¸ªç¼–ç å€¼ï¼Œæ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰çš„æƒ…å†µã€‚å°†â€œæˆ‘çˆ±åƒè‹¹æœâ€å’Œâ€œæˆ‘çˆ±è‹¹æœæ‰‹æœºâ€ä¸­çš„è‹¹æœèµ‹äºˆç›¸åŒçš„ç¼–ç æ˜¯ä¸åˆé€‚çš„ï¼Œæ›´åˆç†çš„æ–¹å¼æ˜¯é€šè¿‡ç»“åˆè¯å‡ºç°çš„ä¸Šä¸‹æ–‡åˆ¤æ–­è¯çš„å«ä¹‰ï¼Œæ¯”å¦‚é€šè¿‡â€œåƒâ€å’Œâ€œæ‰‹æœºâ€æ¥åˆ¤æ–­ä¸Šé¢ä¸¤å¥è¯ä¸­çš„â€œè‹¹æœâ€åˆ†åˆ«ä»£è¡¨ä¸€ç§æ°´æœå’Œä¸€ä¸ªå“ç‰Œï¼Œè¿™å°±æ˜¯è¯­å¢ƒè¯ç¼–ç çš„åŸºæœ¬æ€æƒ³ã€‚æ‰€ä»¥ä»ä½¿ç”¨è€…è§’åº¦æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹èƒ½è¿‡é€šè¿‡è¾“å…¥è¯­å¥å¾—åˆ°ï¼ˆè®¡ç®—å‡ºï¼‰è¯¥è¯­å¥çš„å«ä¹‰ï¼Œæˆ–è€…è¯¥è¯­å¥ä¸­æ¯ä¸ªè¯çš„å«ä¹‰ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šéœ€è¦çš„æ˜¯ä¸€ç§èƒ½å¤Ÿæå–è¯­ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™å’ŒCVä¸­çš„è¿ç§»å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚
		
- self-supervised learning
Imagenetå°†è¶…è¿‡ä¸€åƒå››ç™¾ä¸‡å›¾ç‰‡é€šè¿‡ä¼—åŒ…çš„æ–¹å¼è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œå°†ä»–ä»¬åˆ†æˆ2ä¸‡å¤šä¸ªä¸åŒåˆ†ç±»ï¼Œè¿™é¡¹ä»2007å¹´å¼€å§‹çš„æµ©å¤§å·¥ç¨‹ä¸ºè®¡ç®—æœºè§†è§‰å›¾å½¢ç›¸å…³çš„ç›‘ç£å¼æœºå™¨å­¦ä¹ æä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œä¸ºCVè¿ç§»å­¦ä¹ æ‰“ä¸‹äº†åŸºç¡€ã€‚åŒæ ·çš„ä¸ºäº†NLPé¢†åŸŸä¹Ÿç”±ç±»ä¼¼çš„éœ€æ±‚ï¼šä¸ºæ¯ä¸ªè¯å»ºç«‹æ­£ç¡®çš„æ ‡ç­¾æ•°æ®æ¥å¸®åŠ©è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæ ¹æ®è¯­è¨€çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰è¿™ç§è®­ç»ƒä»»åŠ¡æ¥è¿›è¡Œã€‚ã€‚ã€‚LMå±äºè‡ªç›‘ç£ï¼ˆself supervisedï¼‰è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨è¿™ç§è®­ç»ƒæ–¹æ³•ä¸éœ€è¦ä¸ºè¯­å¥è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œè€Œåªä½¿ç”¨è¯­å¥åºåˆ—æœ¬èº«å°±å¯ä»¥è¿›è¡Œè®­ç»ƒã€‚LMæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªåºåˆ—$W$ï¼ˆç”±è¯$w_i, w_2, ... w_m$ç»„æˆçš„ä¸€å¥è¯ï¼‰å‡ºç°çš„æ¦‚ç‡$$P(W)=P(w_1,w_2,w_3,...w_m)$$LMä¹Ÿå¯ä»¥ç”¨äºè®¡ç®—åœ¨ä¸€ä¸ªåºåˆ—ä¸­æŸä¸ªè¯$w_{n+1}$å‡ºç°çš„æ¦‚ç‡$$P(w_{n+1}|w_1,w_2, w_3,...w_n)$$
æ ¹æ®è¿™æ ·ä¸€ä¸ªåŸºæœ¬å‡è®¾ï¼šæ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡æ¯”ä¸æ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡å¤§
The good LM should calculate higher probabilities to â€œrealâ€ and â€œfrequently observedâ€ sentences than the ones that are wrong accordingly to natural language grammar or those that are rarely observed.
-   **Machine translation:**  translating a sentence saying about height it would probably state that  P(tall  man)>P(large  man)P(tall man)>P(large man)  as the â€˜_large_â€™ might also refer to weight or general appearance thus, not as probable as â€˜_tall_â€™
    
-   **Spelling Correction:**  Spell correcting sentence: â€œPut you name into formâ€, so that  P(name  into  form)>P(name  into  from)
ç”±æ­¤æˆ‘ä»¬é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºé¢„æµ‹å€¼$$\argmax P(w_n|w_1,w_2,w_3,...w_{n-1})$$
	- ä½¿ç”¨LMè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æŒ‰ç…§ä»å‰åˆ°åçš„é¡ºåºè¿›è¡Œé¢„æµ‹ï¼Œæ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­åä¸€ä¸ªè¯æ˜¯â€œâ€ï¼Œä¹Ÿå¯ä»¥æŒ‰ç…§ä»åå‘å‰çš„é¡ºåºï¼Œ$$\argmax P(w_i|w_n,w_{n-1},w_{n-2}, ...w_{i+1})$$æ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­å‰ä¸€ä¸ªè¯æ˜¯â€œâ€ã€‚
	
	
#### å¾®è°ƒ fine tune
ç”±äºä½¿ç”¨æµ·é‡çš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œé¢„è®­ç»ƒæ¨¡å‹é€šå¸¸å…·æœ‰ä¸€èˆ¬çš„å¸¸è¯†ï¼Œç”±æ­¤ä½œä¸ºåŸºç¡€å†è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½æ›´å¥½çš„é€‚åˆç‰¹å®šä»»åŠ¡ã€‚å¾®è°ƒå·¥ä½œå¯ä»¥ä»¥ä¸‹ä¸¤ç§å½¢å¼ï¼š
- ç›‘ç£å¼å¾®è°ƒsupervised fine tuning
ä½¿ç”¨å°‘é‡ä»»åŠ¡ç›¸å…³çš„æ ‡è®°æ•°æ®æ¥è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸çš„åšæ³•æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åé¢ç›´æ¥åŠ ä¸Šä¸Šä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆç”±å…¨è¿æ¥å’Œsoftmaxè¿ç®—æ„æˆï¼‰ä½¿æ¨¡å‹è¾“å‡ºä¸€ä¸ªé¢„æµ‹ç±»å‹ï¼Œè®¡ç®—cross entropyè¯¯å·®ä»è€Œé€šè¿‡åå‘ä¼ é€’æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
- **æ— ç›‘ç£å¼å¾®è°ƒunsupervised fine tuning**?
- zero shot learning
æ— å¾®è°ƒé€‚ç”¨äºå®¹é‡æ›´å¤§é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™ç±»æ¨¡å‹ä¸€èˆ¬åŒ…å«äº†æ›´å¤šçš„å¸¸è¯†ï¼Œæ¯”å¦‚GPT2ä½¿ç”¨äº†xxçš„é«˜è´¨é‡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€å¾®è°ƒä¹Ÿå¯èƒ½åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡é‡ç”Ÿæˆå¯æ¥å—çš„é¢„æµ‹ã€‚å¯¹äºè¿™ç±»æ¨¡å‹ï¼Œåªéœ€è¦ç»™å‡ºå°‘é‡çš„æ ·ä¾‹è®©æ¨¡å‹ç†è§£é¢„æµ‹æ„å›¾ã€‚ã€‚ã€‚

## BERTç®€ä»‹
When BERT was published it achieved [state-of-the-art] performance in 11 [natural language understanding] tasks:[[1]] [GLUE]task set (consisting of 8 tasks), [MultiNLI] [SQuAD] v1.1, SQuAD v2.0
2018, googleå‘è¡¨äº†è®ºæ–‡BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingï¼Œ 2019å¹´googleå°†BERTæ¨¡å‹åº”ç”¨åˆ°äº†æœç´¢æœåŠ¡ä¸­ï¼Œç°åœ¨å·²ç»æ”¯æŒäº†è¶…è¿‡70ç§è¯­è¨€
BERTï¼ˆBidirectional Encoder Representations from Transformerï¼‰æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒå¯ä»¥æå–è¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œ

BERTæœ€å¤§çš„åˆ›æ–°æ˜¯å°†Transformeræ¨¡å‹åº”ç”¨åˆ°äº†è¯­è¨€æ¨¡å‹ä¸­ï¼Œã€‚ã€‚ã€‚ã€‚å½±å“å’Œå†³å®šäº†BERTå¾ˆå¤šç‰¹æ®Šæ€§è´¨ã€‚åœ¨BERTä¹‹å‰ï¼Œ

- context dependent embedding
BERTæ¨¡å‹ç”Ÿæˆçš„å…ƒç´ ç¼–ç å±äºåŠ¨æ€ç¼–ç ï¼Œå®ƒèƒ½æ ¹æ®è¾“å…¥åºåˆ—ç”Ÿæˆæ¯ä¸ªåºåˆ—å…ƒç´ ï¼ˆwordï¼‰åœ¨åºåˆ—ä¸Šä¸‹æ–‡ä¸­çš„ç‰¹å¾å‘é‡
- bidirectional Language Model
è¿™æ˜¯ç”±äºå®ƒæ˜¯ä»¥Attentionæœºåˆ¶ä¸ºåŸºç¡€ã€‚æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä¸€æ¬¡çœ‹åˆ°æ‰€æœ‰çš„åºåˆ—å…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ çš„ç¼–ç çš„è®¡ç®—éƒ½åŒ…å«äº†è¯¥å…ƒç´ ä¹‹å‰å’Œä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤BERTå±äºåŒå‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”ç”±äºèƒ½å¤ŸåŒæ—¶çœ‹åˆ°å‰å‘å’Œåå‘çš„ä¿¡æ¯ï¼ŒBERTä¸åŒäºä»¥å¾€çš„åŒå‘è¯­è¨€æ¨¡å‹ï¼Œå¦‚ELMOï¼Œã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
å¹¶éæ‰€æœ‰çš„åŸºäºattentionæœºåˆ¶çš„æ¨¡å‹éƒ½æ˜¯åŒå‘è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚GPTä½¿ç”¨äº†é®ç½©çš„æ–¹å¼ä½¿æ¨¡å‹æ— æ³•çœ‹åˆ°å½“å‰å…ƒç´ ä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤å®ƒå±äºå•å‘è¯­è¨€æ¨¡å‹ã€‚

- 

## BERTæ¨¡å‹ç»“æ„
### Transformer encoder based
BERTæ¨¡å‹ä¸»è¦åŒ…å«è¿™ä¸ªéƒ¨åˆ†ï¼Œç¼–ç å±‚å’ŒTransformerç¼–ç å™¨
![enter image description here](https://miro.medium.com/max/1095/0*ViwaI3Vvbnd-CJSQ.png)

### ç¼–ç å±‚
ç¼–ç å±‚çš„ä½œç”¨æ˜¯
1. å°†è¾“å…¥è¯­å¥ï¼ˆBERT is powerfulï¼‰è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æµ®ç‚¹æ•°å‘é‡
2. åŠ å…¥ç‰¹æ®Šç¬¦å·[CLS][SEP]

    embeddings = inputs_embeds + position_embeddings + token_type_embeddings

[https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
![enter image description here](https://i.stack.imgur.com/QCcYF.png)
- è¯ç¼–ç (config.vocab_size, config.hidden_size, padding_idx=0)
[https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding](https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding)
- æ®µç¼–ç (config.type_vocab_size, config.hidden_size)
ç”±äºBERTå¯ä»¥å¤„ç†1æˆ–2æ¡è¯­å¥ï¼Œç”¨äºåŒºåˆ†ä¸åŒè¯­å¥
- ä½ç½®ç¼–ç (config.max_position_embeddings, config.hidden_size)
ä¸åŒäºTransformerçš„åŸºäºå‘¨æœŸå‡½æ•°çš„å›ºå®šä½ç½®ç¼–ç æ–¹æ³•ï¼ŒBERTé‡‡ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œbertä¸­çš„æœ€å¤§å¥å­é•¿åº¦æ˜¯512 æ‰€ä»¥Position Embedding layer æ˜¯ä¸€ä¸ªsizeä¸ºï¼ˆ512ï¼Œ768ï¼‰çš„lookup table
### Transformerç¼–ç å™¨
Transformeræ¨¡å‹æ˜¯ç”±google aiäº2017å¹´å‘å¸ƒçš„ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼Œæœ€åˆåº”ç”¨äºæœºå™¨ç¿»è¯‘ã€‚Transformerçš„æœ€å¤§ç‰¹ç‚¹æ˜¯ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰ï¼Œè§£å†³äº†ä½¿ç”¨RNNæ¨¡å‹é€ æˆçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ— æ³•å¹¶è¡Œçš„é—®é¢˜ï¼Œå¹¶ä¸”å®è·µè¯æ˜transformerä¸­æå‡ºçš„å¤šå¤´æ³¨æ„åŠ›å…·æœ‰å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œæ€§èƒ½è¶…è¶Šäº†RNN,CNNç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚
Transformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆwordï¼‰è½¬æ¢ä¸ºåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œå†ç”±è§£ç å™¨æ ¹æ®ç¼–ç åçš„ç‰¹å¾å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚BERTæ¨¡å‹ä¸­åªä½¿ç”¨äº†transformerçš„ç¼–ç å™¨ï¼Œå®ƒä¸»è¦ç”±è‹¥å¹²ä¸ªç»“æ„ç›¸åŒçš„ç¼–ç å±‚è¿æ¥è€Œæˆã€‚æ¯ä¸€ä¸ªç¼–ç å±‚ä¸»è¦æœ‰ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒå’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œç»„æˆï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒè´Ÿè´£ä¸ºæ¯ä¸ªè¾“å…¥å…ƒç´ ç”Ÿæˆç‰¹å¾å‘é‡ï¼Œå‰é¦ˆç½‘ç»œèƒ½å¤Ÿé€šè¿‡ç»„åˆå…ƒç´ ç‰¹å¾å‘é‡ç”Ÿæˆæ›´å¤æ‚çš„ç‰¹å¾å‘é‡ã€‚
### è¾“å‡º

## BERTçš„é¢„è®­ç»ƒ
### ä»»åŠ¡è®¾è®¡
BERTçš„é¢„è®­ç»ƒè¢«è®¾è®¡ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰ï¼ŒåŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼š
- MLM
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows â€” 80% are replaced with a â€œ[MASK]â€ token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):

-   If we used [MASK] 100% of the time the model wouldnâ€™t necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.
-   If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is  _never_  correct.
-   If we used [MASK] 90% of the time and kept the same word 10% of the time, then the model could just trivially copy the non-contextual embedding.

No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasnâ€™t tested with simply masking 100% of the selected tokens.
ç»†èŠ‚ä¸‰ï¼šå¯¹äºä»»åŠ¡ä¸€ï¼Œå¯¹äºåœ¨æ•°æ®ä¸­éšæœºé€‰æ‹© 15% çš„æ ‡è®°ï¼Œå…¶ä¸­80%è¢«æ¢ä½[mask]ï¼Œ10%ä¸å˜ã€10%éšæœºæ›¿æ¢å…¶ä»–å•è¯ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

**ä¸¤ä¸ªç¼ºç‚¹ï¼š**

1ã€å› ä¸ºBertç”¨äºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œ [MASK] æ ‡è®°ä¸ä¼šå‡ºç°ï¼Œå®ƒåªå‡ºç°åœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸­ã€‚è¿™å°±é€ æˆäº†é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„ä¸åŒ¹é…ï¼Œå¾®è°ƒä¸å‡ºç°[MASK]è¿™ä¸ªæ ‡è®°ï¼Œæ¨¡å‹å¥½åƒå°±æ²¡æœ‰äº†ç€åŠ›ç‚¹ã€ä¸çŸ¥ä»å“ªå…¥æ‰‹ã€‚æ‰€ä»¥åªå°†80%çš„æ›¿æ¢ä¸º[mask]ï¼Œä½†è¿™ä¹Ÿ**åªæ˜¯ç¼“è§£ã€ä¸èƒ½è§£å†³**ã€‚

2ã€ç›¸è¾ƒäºä¼ ç»Ÿè¯­è¨€æ¨¡å‹ï¼ŒBertçš„æ¯æ‰¹æ¬¡è®­ç»ƒæ•°æ®ä¸­åªæœ‰ 15% çš„æ ‡è®°è¢«é¢„æµ‹ï¼Œè¿™å¯¼è‡´æ¨¡å‹éœ€è¦æ›´å¤šçš„è®­ç»ƒæ­¥éª¤æ¥æ”¶æ•›ã€‚
- NSP
### ç‰¹æ®Šç¬¦å·
- [CLS] ç”¨äºåˆ†ç±»ä»»åŠ¡
- [SEP] ç”¨äºåˆ†å‰²è¯­å¥
- 

### optimizer
[https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866](https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866)
- size matters
- 

## BERTçš„fine tune	
- how about [CLS] and [SEP]?
![enter image description here](https://lilianweng.github.io/lil-log/assets/images/BERT-downstream-tasks.png)
- downstream tasks
	- sentence classification(sentiment classification)
	- token classification NER
	- SQuAD & unsupervised SQUAD
- å¾®è°ƒæŠ€å·§
[https://zhuanlan.zhihu.com/p/109143667](https://zhuanlan.zhihu.com/p/109143667)
	1.  **é•¿æ–‡æœ¬å¤„ç†**

		å¯¹äºé•¿æ–‡æœ¬æ–‡ä¸­åšäº†ä¸¤ç§å¤„ç†æ–¹å¼ï¼Œæˆªæ–­å’Œåˆ‡åˆ†ã€‚

		-   æˆªæ–­ï¼šä¸€èˆ¬æ¥è¯´æ–‡æœ¬ä¸­æœ€é‡è¦çš„ä¿¡æ¯æ˜¯å¼€å§‹å’Œç»“å°¾ï¼Œå› æ­¤æ–‡ä¸­å¯¹äºé•¿æ–‡æœ¬åšäº†æˆªæ–­å¤„ç†ã€‚

		> head-onlyï¼šä¿ç•™å‰510ä¸ªå­—ç¬¦  
		> tail-onlyï¼šä¿ç•™å510ä¸ªå­—ç¬¦  
		> head+tailï¼šä¿ç•™å‰128ä¸ªå’Œå382ä¸ªå­—ç¬¦
		
		- åˆ‡åˆ†: å°†æ–‡æœ¬åˆ†æˆkæ®µï¼Œæ¯æ®µçš„è¾“å…¥å’ŒBertå¸¸è§„è¾“å…¥ç›¸åŒï¼Œç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯[CLS]è¡¨ç¤ºè¿™æ®µçš„åŠ æƒä¿¡æ¯ã€‚æ–‡ä¸­ä½¿ç”¨äº†Max-pooling, Average poolingå’Œself-attentionç»“åˆè¿™äº›ç‰‡æ®µçš„è¡¨ç¤ºã€‚
		- 
	ä¸‹é¢æ˜¯å®éªŒçš„ç»“æœï¼Œhead+tailçš„è¡¨ç¤ºåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ•ˆæœéƒ½æ¯”è¾ƒå¥½ã€‚åº”è¯¥æ˜¯é•¿æ–‡æœ¬ç»“åˆäº†å¥é¦–å’Œå¥å°¾çš„ä¿¡æ¯ï¼Œè·å–çš„ä¿¡æ¯æ¯”è¾ƒå‡è¡¡ã€‚ä¸è¿‡å¥‡æ€ªçš„æ˜¯æ‹¼æ¥çš„æ–¹å¼æ•´ä½“å±…ç„¶ä¸å¦‚æˆªæ–­ï¼Œä¸ªäººçŒœæµ‹å¯èƒ½æ˜¯å°†å¥å­åˆ‡æˆå‡ æ®µä¹‹åå¢åŠ äº†æ¨¡å‹çš„ä¸ç¨³å®šæ€§ï¼Œè€Œé”™è¯¯å åŠ èµ·æ¥å¯èƒ½å°±ä¼šè¢«æ”¾å¤§ã€‚è€Œmax-poolingå’Œself-attentionä¹Ÿæ›´åŠ å¼ºè°ƒäº†æ–‡æœ¬ä¸­æ¯”è¾ƒæœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æ•´ä½“æ•ˆæœä¼˜äºaverage.
	![enter image description here](https://pic3.zhimg.com/80/v2-f932b2ed7aa4af745b512e2e0f43093e_720w.jpg)

## BERTçš„æ”¹è¿›
### task design
- spanBERT [https://zhuanlan.zhihu.com/p/75893972](https://zhuanlan.zhihu.com/p/75893972)
### distillation
### LAMPï¼Ÿnot a BERT improvement
## BERTåº”ç”¨
### environment-colab
 - User BERT base model
 - Tweak: batch size, max length
 - Mixed precision training
 - Gradient checkpoint
### huggingface transformer
### BERT as a service
### DistilBERT
### SQUAD

## æ€»ç»“













	
## Transfer learning
- what?
- why?
	- Deep model which has lots of parameters need lots of training data
	- labeled training data is very expensive
	- To save training efforts (save money and time)
	- Transfer learning has been successful in CV tasks
- how?
	- pretraining - generate embeddings (word embeddings)
	- supervised finetuning - train for downstream tasks
	
### sequential transfer learning
- pretraining
- Adaptation
- finetuning
	- supervised
	- unsupervised

## pretraining
- self-supervised learningè‡ªç›‘ç£å­¦ä¹  based on  Language Model
	- Many successful pretraining approaches are based on language modeling
	- Informally, a LM learns PÏ´(text) or PÏ´(text | some other text)
	- Doesnâ€™t require human annotation
	- Many languages have enough text to learn high capacity model
	- Versatileâ€”can learn both sentence and word representations with a variety of
objective functions
- from static embedding to dynamic embedding
	- static encoding (contextless embedding)- word2vec, glove
	- dynamic encoding (contextual embedding) - elmo, bert
- How LM help NLP transfer learning
	- feature based
	**Feature-based**æŒ‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¸­é—´ç»“æœä¹Ÿå°±æ˜¯LM embedding, å°†å…¶ä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼Œå¼•å…¥åˆ°åŸä»»åŠ¡çš„æ¨¡å‹ä¸­ã€‚é€šå¸¸feature-basedæ–¹æ³•åŒ…æ‹¬ä¸¤æ­¥ï¼š

		1.  é¦–å…ˆåœ¨å¤§çš„è¯­æ–™Aä¸Šæ— ç›‘ç£åœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒå®Œæ¯•å¾—åˆ°è¯­è¨€æ¨¡å‹
		2.  ç„¶åæ„é€ task-specific modelä¾‹å¦‚åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒtask-sepcific modelï¼Œå°†è¯­è¨€æ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œè¯­æ–™Bçš„è®­ç»ƒæ•°æ®ç»è¿‡è¯­è¨€æ¨¡å‹å¾—åˆ°LM embeddingï¼Œä½œä¸ºtask-specific modelçš„é¢å¤–ç‰¹å¾ã€‚ELMoæ˜¯è¿™æ–¹é¢çš„å…¸å‹å·¥ä½œï¼Œè¯·å‚è€ƒ[2]
	- fine-tuning
	Fine-tuningæ–¹å¼æ˜¯æŒ‡åœ¨å·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å°‘é‡çš„task-specific parameters, ä¾‹å¦‚å¯¹äºåˆ†ç±»é—®é¢˜åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸ŠåŠ ä¸€å±‚softmaxç½‘ç»œï¼Œç„¶ååœ¨æ–°çš„è¯­æ–™ä¸Šé‡æ–°è®­ç»ƒæ¥è¿›è¡Œfine-tuneã€‚ä¾‹å¦‚OpenAI GPT [3] ä¸­é‡‡ç”¨äº†è¿™æ ·çš„æ–¹æ³•ï¼Œæ¨¡å‹å¦‚ä¸‹æ‰€ç¤º

![](https://pic1.zhimg.com/80/v2-8f857288cf73acba9ddb6b3742265144_hd.jpg)

å›¾2 Transformer LM + fine-tuningæ¨¡å‹ç¤ºæ„å›¾

  
é¦–å…ˆè¯­è¨€æ¨¡å‹é‡‡ç”¨äº†Transformer Decoderçš„æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨æ–‡æœ¬é¢„æµ‹ä½œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œè®­ç»ƒå®Œæ¯•ä¹‹åï¼ŒåŠ ä¸€å±‚Linear Projectæ¥å®Œæˆåˆ†ç±»/ç›¸ä¼¼åº¦è®¡ç®—ç­‰NLPä»»åŠ¡ã€‚å› æ­¤æ€»ç»“æ¥è¯´ï¼ŒLM + Fine-Tuningçš„æ–¹æ³•å·¥ä½œåŒ…æ‹¬ä¸¤æ­¥ï¼š

1.  æ„é€ è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤§çš„è¯­æ–™Aæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹
2.  åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ å°‘é‡ç¥ç»ç½‘ç»œå±‚æ¥å®Œæˆspecific taskä¾‹å¦‚åºåˆ—æ ‡æ³¨ã€åˆ†ç±»ç­‰ï¼Œç„¶åé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­è¯­è¨€æ¨¡å‹çš„å‚æ•°å¹¶ä¸å›ºå®šï¼Œä¾ç„¶æ˜¯trainable variables.
- Encoding
	- character level
	- BPE
	- word level
- task design (training objective) for self-supervised learning
	- Language model
	- bidirectional LM
	- MLM, NSP
	- GAN (ELATRA)
- The pretrained model is too complex to use
	- distillation
	- 
## Adaptation
GPT-2è®ºè¯äº†ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Ÿå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸åŒé¢†åŸŸçš„æ–‡æœ¬ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„taskï¼Œè€Œå¦‚æœæŠŠè¿™äº›taskç»„åˆèµ·æ¥å­¦ä¹ ï¼Œé‚£ä¹ˆå°±æ˜¯multi-taskå­¦ä¹ ã€‚æ‰€ç‰¹æ®Šçš„æ˜¯è¿™äº›taskéƒ½æ˜¯åŒè´¨çš„ï¼Œå³å®ƒä»¬çš„ç›®æ ‡å‡½æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç»Ÿä¸€å­¦ä¹ ã€‚é‚£ä¹ˆå½“å¢å¤§æ•°æ®é›†åï¼Œç›¸å½“äºæ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸä¸Šè¿›è¡Œäº†å­¦ä¹ ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰äº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚
### GPT-2 ç›´æ¥åšä¸‹æ¸¸ä»»åŠ¡

é™¤äº†è¯­è¨€æ¨¡å‹ä¸Šçš„è¿›å±•ä¹‹å¤–ï¼ŒGPT-2è¿˜é¦–æ¬¡å°è¯•äº†ç›´æ¥ç”¨è¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸ç”¨åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„æŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿ

æ¯”å¦‚ï¼Œå¦‚æœæ˜¯summarizationä»»åŠ¡ï¼Œé‚£ä¹ˆå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆ‘åŠ ä¸€ä¸ªæ–°è¯TL;DR:, æ”¹è¯å‰é¢æ˜¯contextï¼Œåé¢æ˜¯æ‘˜è¦ã€‚é‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°è¿™ä¸ªè¯åï¼Œå°±èƒ½æ¨æ–­å‡ºæ¥ï¼Œæ¥ä¸‹æ¥è¦åšæŠ½æ‘˜è¦çš„å·¥ä½œäº†ã€‚

åŒç†ï¼Œå¯¹äºtranslateä»»åŠ¡ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åšæˆ french sentence = english sentenceï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°=çš„æ—¶å€™ï¼Œåº”è¯¥èƒ½æ¨æ–­å‡ºæ¥ä¸‹æ¥æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚

è™½ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒGPT-2éƒ½æ²¡æœ‰è¾¾åˆ°SOTAçš„æ•ˆæœï¼Œä½†æ˜¯æ•ˆæœä¹Ÿæ˜¯ç›¸å½“å¯è§‚çš„ã€‚è¡¨æ˜äº†é«˜å®¹é‡æ¨¡å‹åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å¯èƒ½æ€§ã€‚

## Downstream fine-tuning
- finetuning tips - ULMFit
- tools: TF-hub

### Example

[Generalized language model](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[How to build openai's GPT2](https://blog.floydhub.com/gpt2/)
[BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[NLPé¢„è®­ç»ƒæ¼”è¿› - from Word2Vec to XLNet](https://zhuanlan.zhihu.com/p/93343298)
[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼š](https://zhuanlan.zhihu.com/p/56382372)
[å²ä¸Šæœ€å…¨è¯å‘é‡è®²è§£](https://zhuanlan.zhihu.com/p/75391062)
[ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡](https://zhuanlan.zhihu.com/p/89763176)
[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
[å¦‚ä½•è¯„ä»·BERT-å›ç­”](https://www.zhihu.com/question/298203515/answer/516170825)
[NLPè§„åˆ™æ”¹å†™](https://zhuanlan.zhihu.com/p/47488095)
[BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)
[BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
[Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[ELMO](https://petrlorenc.github.io/ELMO/)
[Character embedding CNN](https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)
[The Illustrated BERT EMLO and co.](http://jalammar.github.io/illustrated-bert/)
[Transfer learning in NLP](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_177_4)
[VisualGuideToUsingBERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
[An In-Depth Tutorial to AllenNLP](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
[Transfer learning using elmo embedding](https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c)
[State of transfer learing in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)
[Generalized language model: ULMfit&openai GPT](https://www.topbots.com/generalized-language-models-ulmfit-openai-gpt/)
[Bertæ¨¡å‹åŠfine-tuning](https://zhuanlan.zhihu.com/p/46833276)
[Openai GPT2 è¯¦è§£](https://zhuanlan.zhihu.com/p/57251615)
[How to make custom AI-generated text with GPT2](https://minimaxir.com/2019/09/howto-gpt2/)
[GPT2: Understand language generation through visualization](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)
[GPTä¸ºä»€ä¹ˆä¸èƒ½åŒå‘ï¼Ÿ](https://www.zhihu.com/question/322034410/answer/794201004)
[ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
[ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Unsupervised NER with BERT](https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT)
[BERT explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[Zero shot GPT2](https://rakeshchada.github.io/Zero-Shot-GPT-2.html)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Understanding BERT Part 2: BERT Specifics](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
[Google BERT â€” Pre Training and Fine Tuning for NLP Tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[why BERT has 3 embedding layers?](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
[from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
[google BERT - pretraining and finetuing for NLP tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[NLP: Explaining Neural language model](https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/#.XniDIWgzZPY)
[Bertå¾®è°ƒæŠ€å·§å®éªŒå¤§å…¨](https://zhuanlan.zhihu.com/p/109143667)
[BERT finetuneçš„è‰ºæœ¯](https://zhuanlan.zhihu.com/p/62642374)
[Bertåœ¨NLPå„é¢†åŸŸçš„åº”ç”¨è¿›å±•](https://zhuanlan.zhihu.com/p/68446772)
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjEwNjI1NjUsLTkwNzk0Mjc5MiwtMjAwNj
M3MTg4NCw4NzQyNDcxODMsLTY4Mzk5MzE2NiwtMzcwMjkyMjM5
LDE3MjMxNDM2NzUsMTQ2NDgxNzkyLDQ0NTMwMzg1OSw2NTU5OD
Y1NzAsLTIwMTk0ODgyMjcsMTE2ODE1Nzg3NywtNDk0MjgxMDk4
LDM1MTI4NDMyLC02MTQxOTc3MjEsLTE5MjI0NjEyMSwxOTU1OD
YzMDc5LC00NzY4NzIyNDUsMTA4NDY2NzgwNSwtNjM4NDQ0ODYy
XX0=
-->