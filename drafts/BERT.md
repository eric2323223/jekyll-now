# NLP transfer learning 


# Training objective of self-supervised learning - From Word2Vec to Elmo to Bert to XLNet

self-supervised learning is important area because it can greatly reduce the effort of training deep model, 


## Transfer learning
- what?
- why?
	- Deep model which has lots of parameters need lots of training data
	- labeled training data is very expensive
	- To save training efforts (save money and time)
	- Transfer learning has been successful in CV tasks
- how?
	- pretraining - generate embeddings (word embeddings)
	- supervised finetuning - train for downstream tasks
	
### sequential transfer learning
- pretraining
- Adaptation
- finetuning
	- supervised
	- unsupervised

## pretraining
- self-supervised learningè‡ªç›‘ç£å­¦ä¹  based on  Language Model
	- Many successful pretraining approaches are based on language modeling
	- Informally, a LM learns PÏ´(text) or PÏ´(text | some other text)
	- Doesnâ€™t require human annotation
	- Many languages have enough text to learn high capacity model
	- Versatileâ€”can learn both sentence and word representations with a variety of
objective functions
- from static embedding to dynamic embedding
	- static encoding (contextless embedding)- word2vec, glove
	- dynamic encoding (contextual embedding) - elmo, bert
- How LM help NLP transfer learning
	- feature based
	**Feature-based**æŒ‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¸­é—´ç»“æœä¹Ÿå°±æ˜¯LM embedding, å°†å…¶ä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼Œå¼•å…¥åˆ°åŸä»»åŠ¡çš„æ¨¡å‹ä¸­ã€‚é€šå¸¸feature-basedæ–¹æ³•åŒ…æ‹¬ä¸¤æ­¥ï¼š

		1.  é¦–å…ˆåœ¨å¤§çš„è¯­æ–™Aä¸Šæ— ç›‘ç£åœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒå®Œæ¯•å¾—åˆ°è¯­è¨€æ¨¡å‹
		2.  ç„¶åæ„é€ task-specific modelä¾‹å¦‚åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒtask-sepcific modelï¼Œå°†è¯­è¨€æ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œè¯­æ–™Bçš„è®­ç»ƒæ•°æ®ç»è¿‡è¯­è¨€æ¨¡å‹å¾—åˆ°LM embeddingï¼Œä½œä¸ºtask-specific modelçš„é¢å¤–ç‰¹å¾ã€‚ELMoæ˜¯è¿™æ–¹é¢çš„å…¸å‹å·¥ä½œï¼Œè¯·å‚è€ƒ[2]
	- fine-tuning
	Fine-tuningæ–¹å¼æ˜¯æŒ‡åœ¨å·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å°‘é‡çš„task-specific parameters, ä¾‹å¦‚å¯¹äºåˆ†ç±»é—®é¢˜åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸ŠåŠ ä¸€å±‚softmaxç½‘ç»œï¼Œç„¶ååœ¨æ–°çš„è¯­æ–™ä¸Šé‡æ–°è®­ç»ƒæ¥è¿›è¡Œfine-tuneã€‚ä¾‹å¦‚OpenAI GPT [3] ä¸­é‡‡ç”¨äº†è¿™æ ·çš„æ–¹æ³•ï¼Œæ¨¡å‹å¦‚ä¸‹æ‰€ç¤º

![](https://pic1.zhimg.com/80/v2-8f857288cf73acba9ddb6b3742265144_hd.jpg)

å›¾2 Transformer LM + fine-tuningæ¨¡å‹ç¤ºæ„å›¾

  
é¦–å…ˆè¯­è¨€æ¨¡å‹é‡‡ç”¨äº†Transformer Decoderçš„æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨æ–‡æœ¬é¢„æµ‹ä½œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œè®­ç»ƒå®Œæ¯•ä¹‹åï¼ŒåŠ ä¸€å±‚Linear Projectæ¥å®Œæˆåˆ†ç±»/ç›¸ä¼¼åº¦è®¡ç®—ç­‰NLPä»»åŠ¡ã€‚å› æ­¤æ€»ç»“æ¥è¯´ï¼ŒLM + Fine-Tuningçš„æ–¹æ³•å·¥ä½œåŒ…æ‹¬ä¸¤æ­¥ï¼š

1.  æ„é€ è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤§çš„è¯­æ–™Aæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹
2.  åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ å°‘é‡ç¥ç»ç½‘ç»œå±‚æ¥å®Œæˆspecific taskä¾‹å¦‚åºåˆ—æ ‡æ³¨ã€åˆ†ç±»ç­‰ï¼Œç„¶åé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­è¯­è¨€æ¨¡å‹çš„å‚æ•°å¹¶ä¸å›ºå®šï¼Œä¾ç„¶æ˜¯trainable variables.
- Encoding
	- character level
	- BPE
	- word level
- task design (training objective) for self-supervised learning
	- Language model
	- bidirectional LM
	- MLM, NSP
	- GAN (ELATRA)
- The pretrained model is too complex to use
	- distillation
	- 
## Adaptation
GPT-2è®ºè¯äº†ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Ÿå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸åŒé¢†åŸŸçš„æ–‡æœ¬ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„taskï¼Œè€Œå¦‚æœæŠŠè¿™äº›taskç»„åˆèµ·æ¥å­¦ä¹ ï¼Œé‚£ä¹ˆå°±æ˜¯multi-taskå­¦ä¹ ã€‚æ‰€ç‰¹æ®Šçš„æ˜¯è¿™äº›taskéƒ½æ˜¯åŒè´¨çš„ï¼Œå³å®ƒä»¬çš„ç›®æ ‡å‡½æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç»Ÿä¸€å­¦ä¹ ã€‚é‚£ä¹ˆå½“å¢å¤§æ•°æ®é›†åï¼Œç›¸å½“äºæ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸä¸Šè¿›è¡Œäº†å­¦ä¹ ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰äº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚
### GPT-2 ç›´æ¥åšä¸‹æ¸¸ä»»åŠ¡

é™¤äº†è¯­è¨€æ¨¡å‹ä¸Šçš„è¿›å±•ä¹‹å¤–ï¼ŒGPT-2è¿˜é¦–æ¬¡å°è¯•äº†ç›´æ¥ç”¨è¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸ç”¨åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„æŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿ

æ¯”å¦‚ï¼Œå¦‚æœæ˜¯summarizationä»»åŠ¡ï¼Œé‚£ä¹ˆå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆ‘åŠ ä¸€ä¸ªæ–°è¯TL;DR:, æ”¹è¯å‰é¢æ˜¯contextï¼Œåé¢æ˜¯æ‘˜è¦ã€‚é‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°è¿™ä¸ªè¯åï¼Œå°±èƒ½æ¨æ–­å‡ºæ¥ï¼Œæ¥ä¸‹æ¥è¦åšæŠ½æ‘˜è¦çš„å·¥ä½œäº†ã€‚

åŒç†ï¼Œå¯¹äºtranslateä»»åŠ¡ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åšæˆ french sentence = english sentenceï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°=çš„æ—¶å€™ï¼Œåº”è¯¥èƒ½æ¨æ–­å‡ºæ¥ä¸‹æ¥æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚

è™½ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒGPT-2éƒ½æ²¡æœ‰è¾¾åˆ°SOTAçš„æ•ˆæœï¼Œä½†æ˜¯æ•ˆæœä¹Ÿæ˜¯ç›¸å½“å¯è§‚çš„ã€‚è¡¨æ˜äº†é«˜å®¹é‡æ¨¡å‹åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å¯èƒ½æ€§ã€‚

## Downstream fine-tuning
- finetuning tips - ULMFit
- tools: TF-hub

### Example

[Generalized language model](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[How to build openai's GPT2](https://blog.floydhub.com/gpt2/)
[BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[NLPé¢„è®­ç»ƒæ¼”è¿› - from Word2Vec to XLNet](https://zhuanlan.zhihu.com/p/93343298)
[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼š](https://zhuanlan.zhihu.com/p/56382372)
[å²ä¸Šæœ€å…¨è¯å‘é‡è®²è§£](https://zhuanlan.zhihu.com/p/75391062)
[ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡](https://zhuanlan.zhihu.com/p/89763176)
[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
[å¦‚ä½•è¯„ä»·BERT-å›ç­”](https://www.zhihu.com/question/298203515/answer/516170825)
[NLPè§„åˆ™æ”¹å†™](https://zhuanlan.zhihu.com/p/47488095)
[BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)
[BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
[Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[ELMO](https://petrlorenc.github.io/ELMO/)
[Character embedding CNN](https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)
[The Illustrated BERT EMLO and co.](http://jalammar.github.io/illustrated-bert/)
[Transfer learning in NLP](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_177_4)
[VisualGuideToUsingBERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
[An In-Depth Tutorial to AllenNLP](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
[Transfer learning using elmo embedding](https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c)
[State of transfer learing in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)
[Generalized language model: ULMfit&openai GPT](https://www.topbots.com/generalized-language-models-ulmfit-openai-gpt/)
[Bertæ¨¡å‹åŠfine-tuning](https://zhuanlan.zhihu.com/p/46833276)
[Openai GPT2 è¯¦è§£](https://zhuanlan.zhihu.com/p/57251615)
[How to make custom AI-generated text with GPT2](https://minimaxir.com/2019/09/howto-gpt2/)
[GPT2: Understand language generation through visualization](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)
[GPTä¸ºä»€ä¹ˆä¸èƒ½åŒå‘ï¼Ÿ](https://www.zhihu.com/question/322034410/answer/794201004)
[ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
[ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Unsupervised NER with BERT](https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ3Njg3MjI0NSwxMDg0NjY3ODA1LC02Mz
g0NDQ4NjIsLTc1MzU1OTI3Miw2MDMyMzY2NDIsLTgzOTczMjU2
MywxNDU4MjAxMjEyLDExMzM2MTEyMjksNzQ3NDQ3ODMyLDExMD
U5ODg0ODgsLTI0NTA1NjQxNywxNTU5OTMzOTQ2LC0xMTY2MzUw
NDc2LDk1NzMyMTQyOCwtMTI4MjQ4NTc0MywtMjE0NzA0MjA4My
wtNjY3Mzg4ODMsLTE2Njg0MzI0OTddfQ==
-->