# NLPçš„è¿ç§»å­¦ä¹ -BERTç¯‡
è¿ç§»å­¦ä¹ è®©æ™®é€šäººåº”ç”¨å¤æ‚å¼ºå¤§æ¨¡å‹è§£å†³å®é™…é—®é¢˜çš„æ·å¾„ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§èƒ½åŠ›ï¼ŒBERTåœ¨NLPé¢†åŸŸçš„ä¸€ç³»åˆ—ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°é«˜ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTçš„ç»“æ„ï¼Œç‰¹æ€§ï¼Œé¢„è®­ç»ƒæ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œå¹¶è¯•å›¾è§£é‡ŠBERTæ¨¡å‹è®¾è®¡èƒŒåçš„åŸå› ã€‚æœ€åå›å½’åº”ç”¨ï¼Œä»‹ç»äº†å¦‚ä½•åˆ©ç”¨BERTé¢„è®­ç»ƒæ¨¡å‹åœ¨colabå¹³å°å¿«é€Ÿå®ç°æ™ºèƒ½é—®ç­”ã€‚
1. è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
    1.1 NLPçš„è¿ç§»å­¦ä¹ 
    1.2 è¯­è¨€æ¨¡å‹
2. BERTç®€ä»‹
3. BERTæ¨¡å‹ç»“æ„
    3.1 ç¼–ç å±‚
    3.2 Transformerç¼–ç å™¨
4. BERTçš„é¢„è®­ç»ƒ
    4.1 ä»»åŠ¡è®¾è®¡
    4.2 é¢„è®­ç»ƒæµç¨‹
    4.3 ä¼˜åŒ–
5. BERTçš„å¾®è°ƒ
    5.1 æƒ…ç»ªåˆ†æä»»åŠ¡
    5.2 åç§°å®ä½“è¯†åˆ«NERä»»åŠ¡
    5.3 é€šç”¨è¯­è¨€ç†è§£GLUEä»»åŠ¡
    5.4 é—®ç­”SQuADä»»åŠ¡
6. æ€»ç»“


self-supervised learning is important area because it can greatly reduce the effort of training deep model, 

ä½œä¸ºNLPè¿ç§»å­¦ä¹ çš„æˆåŠŸåº”ç”¨ï¼ŒBERTè¯æ˜äº†ã€‚ã€‚ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTæ¨¡å‹çš„ç»“æ„å’Œè®¾è®¡åŸç†ï¼Œä»¥åŠBERTçš„åº”ç”¨ã€‚
## è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vR6JBirfomJ2dxM1GDEl2GUZOXZeuyqcjRr7w6-t-s2vloOyAZk8GTRP1IyVmczcmyEINONHs5DhpH0/pub?w=593&h=343)

æ·±åº¦å­¦ä¹ ç”±äºåœ¨å¤„ç†å¤æ‚ç‰¹å¾ï¼ˆå›¾åƒï¼Œå£°éŸ³ï¼Œæ–‡æœ¬ï¼‰çš„ä»»åŠ¡ä¸Šç›¸æ¯”ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•æœ‰å·¨å¤§çš„ä¼˜åŠ¿ï¼Œè·å¾—äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨å’Œå‘å±•ã€‚ä¸ºäº†ä¸æ–­å¢å¼ºé¢„æµ‹æ•ˆæœï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å‘ˆç°å‡ºè¶Šæ¥è¶Šå¤æ‚çš„è¶‹åŠ¿ã€‚æ·±åº¦å­¦ä¹ å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–éå¸¸å¼ºï¼Œè¿™æ˜¯ç”±äºå¤æ‚æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®æ‰æœ‰å¯èƒ½çš„ç†è§£æ•°æ®çš„æ½œåœ¨ï¼ˆå¤æ‚ï¼‰ç‰¹å¾ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹è¶Šå¤æ‚å°±éœ€è¦è¶Šå¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™å°±å¯¼è‡´äº†æ¯”è¾ƒå¤æ‚çš„æ·±åº¦æ¨¡å‹éœ€è¦æµ·é‡çš„æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚ç”±äºè®­ç»ƒæ•°æ®é€šå¸¸éœ€è¦äººå·¥æ ‡è®°å› æ­¤æµ·é‡è®­ç»ƒæ•°æ®çš„è·å–æˆæœ¬éå¸¸é«˜ï¼Œè¿™ä½¿å¾—è®­ç»ƒæˆ–è€…æ”¹è¿›æ·±åº¦æ¨¡å‹æˆä¸ºè€—æ—¶è€—åŠ›çš„è¿‡ç¨‹ï¼Œéå¸¸ä¸åˆ©äºæ·±åº¦æ¨¡å‹çš„æ¨å¹¿å’Œåº”ç”¨ã€‚

~~>**è®­ç»ƒæ•°æ®ä¸è¶³**æ˜¯ä¸€äº›ç‰¹æ®Šé¢†åŸŸä¸­ä¸å¯é¿å…çš„é—®é¢˜ã€‚æ•°æ®çš„æ”¶é›†æ˜¯å¤æ‚å’Œæ˜‚è´µçš„ï¼Œè¿™ä½¿å¾—æ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¸¦æ³¨é‡Šçš„æ•°æ®é›†éå¸¸å›°éš¾ã€‚ä¾‹å¦‚ï¼Œç”Ÿç‰©ä¿¡æ¯å­¦æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬ç»å¸¸æ˜¾ç¤ºä¸€ä¸ªä¸´åºŠè¯•éªŒæˆ–ä¸€ä¸ªç—›è‹¦çš„ç—…äººã€‚æ­¤å¤–ï¼Œå³ä½¿æˆ‘ä»¬ä»˜å‡ºäº†æ˜‚è´µçš„ä»£ä»·æ¥è·å–è®­ç»ƒæ•°æ®é›†ï¼Œä¹Ÿå¾ˆå®¹æ˜“è¿‡æ—¶ï¼Œä¸èƒ½æœ‰æ•ˆåœ°åº”ç”¨äºæ–°çš„ä»»åŠ¡ä¸­ã€‚~~

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œäººä»¬å°è¯•å°†æ·±åº¦å­¦ä¹ è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å…·æœ‰å…±æ€§çš„çŸ¥è¯†æå–å‡ºæ¥ç”¨äºç±»ä¼¼ç›®æ ‡çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å»ï¼Œè¿™æ ·ã€‚ã€‚å°±å¯ä»¥â€œç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šâ€è€Œä¸å¿…ä»é›¶å¼€å§‹ï¼Œä»è€ŒèŠ‚çœäº†å¤§é‡çš„èµ„æºå’Œæ—¶é—´ï¼Œè¿™å°±æ˜¯è¿ç§»å­¦ä¹ (Transfer learning)çš„åŸºæœ¬æ€æƒ³ã€‚åŸºäºè¿™ç§é‡ç”¨çš„æ€æƒ³ï¼Œè¿ç§»å­¦ä¹ å°†ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒä»»åŠ¡åˆ†æˆäº†ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒè®­ç»ƒ

è¿ç§»å­¦ä¹ æ˜¯åœ¨æºä»»åŠ¡æ¨¡å‹å’Œæ–°ä»»åŠ¡æ¨¡å‹å…·æœ‰ç›¸å…³æ€§çš„å‰æä¸‹ï¼ŒæŠŠå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°è¿ç§»åˆ°æ–°çš„æ¨¡å‹æ¥å¸®åŠ©æ–°æ¨¡å‹è®­ç»ƒï¼Œè¿™æ ·å°±å¯ä»¥åœ¨æºä»»åŠ¡æ¨¡å‹çš„åŸºç¡€ä¸Šé’ˆå¯¹æ–°ä»»åŠ¡è¿›è¡Œè°ƒæ•´å’Œæ”¹è¿›ï¼Œè€Œä¸å¿…ä»é›¶å¼€å§‹ï¼Œä»è€ŒèŠ‚çœå¤§é‡çš„æ—¶é—´å’Œé‡‘é’±ã€‚

~~>è¿ç§»å­¦ä¹ æ”¾æ¾äº†è®­ç»ƒæ•°æ®å¿…é¡»ä¸æµ‹è¯•æ•°æ®ç‹¬ç«‹ä¸”åŒåˆ†å¸ƒ(i.i.d)çš„å‡è®¾ï¼Œæ¿€åŠ±æˆ‘ä»¬åˆ©ç”¨è¿ç§»å­¦ä¹ æ¥è§£å†³è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚åœ¨è¿ç§»å­¦ä¹ ä¸­ï¼Œè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¸éœ€è¦æ˜¯i.i.dã€‚ä¸éœ€è¦å¯¹ç›®æ ‡åŸŸå†…çš„æ¨¡å‹è¿›è¡Œä»é›¶å¼€å§‹çš„è®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¯¹ç›®æ ‡åŸŸå†…è®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ—¶é—´çš„éœ€æ±‚ã€‚~~
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vStoAwye3EraSC6HH5m_S8VOsVEp3hsTtQuAVF-dEmPlFvEZqAxBHDQryl3FnVf_BZ6Csb969AGbChe/pub?w=791&h=385)
- é¢„è®­ç»ƒé˜¶æ®µ
è¿™ä¸ªé˜¶æ®µçš„è®­ç»ƒç›®æ ‡æ˜¯ç”ŸæˆåŒ…å«å¯é‡ç”¨çŸ¥è¯†çš„æ¨¡å‹-é¢„è®­ç»ƒæ¨¡å‹ã€‚é¢„è®­ç»ƒè¿™æ˜¯ä¸€ä¸ªè€—æ—¶è€—åŠ›çš„å·¨å¤§å·¥ç¨‹ï¼Œä¸ºäº†ä½¿å¾—æ›´å¤šä¸åŒçš„ä»»åŠ¡èƒ½ä»ä¸­å—ç›Šï¼Œäººä»¬è¿½æ±‚æ›´åŠ é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”±äºé€šç”¨çŸ¥è¯†çš„å¤æ‚æ€§ï¼Œé¢„è®­ç»ƒæ¨¡å‹éƒ½éå¸¸å¤æ‚ã€‚è€Œè¿™ç±»å¤æ‚æ¨¡å‹åªèƒ½é æµ·é‡æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸ªé˜¶æ®µä¼šè€—è´¹å¤§é‡çš„è®¡ç®—èµ„æºã€‚é™¤äº†æ•°æ®é‡è¦æ±‚å¤§ä¹‹å¤–ï¼Œé¢„è®­ç»ƒå¯¹æ•°æ®çš„è´¨é‡ä¹Ÿæœ‰è¾ƒé«˜è¦æ±‚ï¼Œä¾‹å¦‚åœ¨CVé¢†åŸŸæœ€æˆåŠŸçš„è¿ç§»å­¦ä¹ çš„çš„åº”ç”¨æ˜¯imagenetè®­ç»ƒæ•°æ®é›†åŠå»ºç«‹åœ¨å…¶åŸºç¡€ä¹‹ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚VGG19ï¼Œ ResNet50ã€‚é¢„è®­ç»ƒé‡‡ç”¨ç›‘ç£å¼è®­ç»ƒï¼Œå³æ¯ä¸ªimagenetæ•°æ®é›†ä¸­çš„å›¾ç‰‡éƒ½æœ‰ä¸€ä¸ªäººå·¥æ ‡æ³¨çš„æè¿°è¯¥å›¾ç‰‡æ‰€å±ç±»å‹çš„æ ‡ç­¾ã€‚Imagenetå°†è¶…è¿‡ä¸€åƒå››ç™¾ä¸‡å›¾ç‰‡é€šè¿‡ä¼—åŒ…çš„æ–¹å¼è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œå°†ä»–ä»¬åˆ†æˆ2ä¸‡å¤šä¸ªä¸åŒåˆ†ç±»ï¼Œè¿™é¡¹ä»2007å¹´å¼€å§‹çš„æµ©å¤§å·¥ç¨‹ä¸ºè®¡ç®—æœºè§†è§‰å›¾å½¢ç›¸å…³çš„é¢„è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œä¸ºCVè¿ç§»å­¦ä¹ æ‰“ä¸‹äº†åŸºç¡€ã€‚
- å¾®è°ƒé˜¶æ®µ
æ ¹æ®ä»»åŠ¡çš„éœ€è¦ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè®¾è®¡å¹¶åŠ å…¥ç›¸åº”çš„æ¨¡å‹ç»“æ„ï¼Œæ¯”å¦‚ã€‚ã€‚ã€‚ã€‚å†ä½¿ç”¨ä»»åŠ¡ç›¸å…³çš„å°‘é‡è®­ç»ƒæ•°æ®æ¥è°ƒæ•´æ¨¡å‹å‚æ•°ä½¿å…¶é€‚åº”è¯¥ä»»åŠ¡ã€‚

### NLPçš„è¿ç§»å­¦ä¹ 
NLPçš„è¿ç§»å­¦ä¹ åŒæ ·åˆ†ä¸ºé¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤æ­¥ï¼Œé¢„CVä»»åŠ¡ä¸åŒçš„æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µNLPé‡‡ç”¨äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆself supervised learningï¼‰æ–¹å¼ï¼Œè¿™æ˜¯ç”±äºNLPä¸­çš„åŸºæœ¬å…ƒç´ -wordï¼ˆæˆ–å­—ï¼‰çš„å«ä¹‰é€šå¸¸ç”±å…¶æ‰€åœ¨çš„è¯­å¥çš„ä¸Šä¸‹æ–‡æ¥å†³å®šï¼Œå…·æœ‰é«˜åº¦çš„çµæ´»æ€§ï¼Œæ— æ³•åƒCVä¸­é‚£ä¸ªç”¨ä¸€ä¸ªå›ºå®šçš„æ ‡ç­¾æ¥æ ‡è®°ã€‚æ‰€å¹¸çš„æ˜¯ä½¿ç”¨è¯­è¨€æ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°åˆ©ç”¨ç°æœ‰æ–‡æœ¬èµ„æ–™ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ çš„æ–¹å¼æ¥è¿›è¡Œé¢„è®­ç»ƒã€‚

ç”±äºNLPä¸»è¦å…³æ³¨è¯­è¨€ï¼ˆå­—ç¬¦åºåˆ—ï¼‰çš„ç†è§£å’Œå¤„ç†ï¼Œä½œä¸ºè¯­è¨€åŸºæœ¬ç»„æˆå•ä½çš„è¯ï¼ˆwordï¼‰ä¹Ÿå°±è‡ªç„¶æˆä¸ºäº†é¢„è®­ç»ƒçš„å…³æ³¨ç‚¹ã€‚é¢„è®­ç»ƒçš„ç›®æ ‡ç»å†é€æ­¥çš„å‘å±•å˜åŒ–
#### é¢„è®­ç»ƒ pre training
#####  è®­ç»ƒç›®æ ‡: ç”Ÿæˆè¯ï¼ˆå­—ï¼‰ç¼–ç  word embedding
- é™æ€è¯ç¼–ç ï¼ˆstatic word embeddingï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»æ—©æœŸçš„å›ºå®šç¼–ç æ–¹å¼ï¼Œæ¯”Word2Vecï¼ŒGloveç­‰ï¼Œé¡¾åæ€ä¹‰è¿™ç±»ç¼–ç èµ‹äºˆæ¯ä¸ªè¯å›ºå®šçš„ç¼–ç å€¼ï¼Œå¹¶ä¸”ç¼–ç å€¼ä½“ç°äº†è¯çš„ä»£è¡¨çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ç¼–ç å€¼çš„è¿ç®—å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœï¼Œæ¯”å¦‚è‘—åçš„ä¾‹å­ ***king â€” man + woman = queen***

- è¯­å¢ƒè¯ç¼–ç ï¼ˆcontextualized word embeddingï¼‰ï¼Œé™æ€è¯ç¼–ç çš„æœ€å¤§çš„é—®é¢˜åœ¨äºå®ƒåªèƒ½ä¸ªæ¯ä¸€ä¸ªè¯ä¸€ä¸ªç¼–ç å€¼ï¼Œæ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰çš„æƒ…å†µã€‚å°†â€œæˆ‘çˆ±åƒè‹¹æœâ€å’Œâ€œæˆ‘çˆ±è‹¹æœæ‰‹æœºâ€ä¸­çš„è‹¹æœèµ‹äºˆç›¸åŒçš„ç¼–ç æ˜¯ä¸åˆé€‚çš„ï¼Œæ›´åˆç†çš„æ–¹å¼æ˜¯é€šè¿‡ç»“åˆè¯å‡ºç°çš„ä¸Šä¸‹æ–‡åˆ¤æ–­è¯çš„å«ä¹‰ï¼Œæ¯”å¦‚é€šè¿‡â€œåƒâ€å’Œâ€œæ‰‹æœºâ€æ¥åˆ¤æ–­ä¸Šé¢ä¸¤å¥è¯ä¸­çš„â€œè‹¹æœâ€åˆ†åˆ«ä»£è¡¨ä¸€ç§æ°´æœå’Œä¸€ä¸ªå“ç‰Œï¼Œè¿™å°±æ˜¯è¯­å¢ƒè¯ç¼–ç çš„åŸºæœ¬æ€æƒ³ã€‚æ‰€ä»¥ä»ä½¿ç”¨è€…è§’åº¦æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹èƒ½è¿‡é€šè¿‡è¾“å…¥è¯­å¥å¾—åˆ°ï¼ˆè®¡ç®—å‡ºï¼‰è¯¥è¯­å¥çš„å«ä¹‰ï¼Œæˆ–è€…è¯¥è¯­å¥ä¸­æ¯ä¸ªè¯çš„å«ä¹‰ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šéœ€è¦çš„æ˜¯ä¸€ç§èƒ½å¤Ÿæå–è¯­ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™å’ŒCVä¸­çš„è¿ç§»å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚
	- å•å‘è¯­å¢ƒç¼–ç  LSTM
	- åŒå‘è¯­å¢ƒç¼–ç  elmo
		
#### è®­ç»ƒæ–¹å¼  self-supervised learning

ç”±äºè¯­è¨€çš„åŠ¨æ€ç‰¹æ€§ï¼ŒNLPä»»åŠ¡
> NLPçš„æœ€å¤§æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹è¶³å¤Ÿçš„åŸ¹è®­æ•°æ®ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ‰å¤§é‡æ–‡æœ¬æ•°æ®å¯ç”¨ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬è¦åˆ›å»ºç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†ï¼Œåˆ™éœ€è¦å°†è¯¥å †åˆ’åˆ†ä¸ºå¾ˆå¤šä¸åŒçš„å­—æ®µã€‚è€Œå½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆä»…å¾—åˆ°æ•°åƒæˆ–æ•°åä¸‡ä¸ªäººæ ‡è®°çš„åŸ¹è®­ç¤ºä¾‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸ºäº†è¡¨ç°è‰¯å¥½ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„NLPæ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®-åœ¨æ•°ç™¾ä¸‡æˆ–æ•°åäº¿çš„å¸¦æ³¨é‡Šçš„è®­ç»ƒç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œä»–ä»¬çœ‹åˆ°äº†é‡å¤§æ”¹è¿›ã€‚ä¸ºäº†å¸®åŠ©å¼¥åˆæ•°æ®é¸¿æ²Ÿï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å„ç§æŠ€æœ¯ï¼Œå¯åœ¨ç½‘ç»œä¸Šä½¿ç”¨å¤§é‡æœªæ³¨é‡Šçš„æ–‡æœ¬æ¥è®­ç»ƒé€šç”¨è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼ˆè¿™ç§°ä¸ºé¢„è®­ç»ƒï¼‰ã€‚ç„¶åï¼Œå¯ä»¥åœ¨è¾ƒå°çš„ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¤„ç†è¯¸å¦‚é—®é¢˜å›ç­”å’Œæƒ…æ„Ÿåˆ†æä¹‹ç±»çš„é—®é¢˜æ—¶ã€‚ä¸ä»å¤´å¼€å§‹å¯¹è¾ƒå°çš„ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œæ­¤æ–¹æ³•å¯æ˜¾ç€æé«˜å‡†ç¡®æ€§ã€‚

åŒæ ·çš„ä¸ºäº†NLPé¢†åŸŸä¹Ÿç”±ç±»ä¼¼çš„éœ€æ±‚ï¼šä¸ºæ¯ä¸ªè¯å»ºç«‹æ­£ç¡®çš„æ ‡ç­¾æ•°æ®æ¥å¸®åŠ©è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæ ¹æ®è¯­è¨€çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰è¿™ç§è®­ç»ƒä»»åŠ¡æ¥è¿›è¡Œã€‚ã€‚ã€‚LMå±äºè‡ªç›‘ç£ï¼ˆself supervisedï¼‰è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨è¿™ç§è®­ç»ƒæ–¹æ³•ä¸éœ€è¦ä¸ºè¯­å¥è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œè€Œåªä½¿ç”¨è¯­å¥åºåˆ—æœ¬èº«å°±å¯ä»¥è¿›è¡Œè®­ç»ƒã€‚LMæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªåºåˆ—$W$ï¼ˆç”±è¯$w_i, w_2, ... w_m$ç»„æˆçš„ä¸€å¥è¯ï¼‰å‡ºç°çš„æ¦‚ç‡$$P(W)=P(w_1,w_2,w_3,...w_m)$$LMä¹Ÿå¯ä»¥ç”¨äºè®¡ç®—åœ¨ä¸€ä¸ªåºåˆ—ä¸­æŸä¸ªè¯$w_{n+1}$å‡ºç°çš„æ¦‚ç‡$$P(w_{n+1}|w_1,w_2, w_3,...w_n)$$
æ ¹æ®è¿™æ ·ä¸€ä¸ªåŸºæœ¬å‡è®¾ï¼šæ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡æ¯”ä¸æ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡å¤§
The good LM should calculate higher probabilities to â€œrealâ€ and â€œfrequently observedâ€ sentences than the ones that are wrong accordingly to natural language grammar or those that are rarely observed.
-   **Machine translation:**  translating a sentence saying about height it would probably state that  P(tall  man)>P(large  man)P(tall man)>P(large man)  as the â€˜_large_â€™ might also refer to weight or general appearance thus, not as probable as â€˜_tall_â€™
    
-   **Spelling Correction:**  Spell correcting sentence: â€œPut you name into formâ€, so that  P(name  into  form)>P(name  into  from)
ç”±æ­¤æˆ‘ä»¬é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºé¢„æµ‹å€¼$$\argmax P(w_n|w_1,w_2,w_3,...w_{n-1})$$
	- ä½¿ç”¨LMè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æŒ‰ç…§ä»å‰åˆ°åçš„é¡ºåºè¿›è¡Œé¢„æµ‹ï¼Œæ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­åä¸€ä¸ªè¯æ˜¯â€œâ€ï¼Œä¹Ÿå¯ä»¥æŒ‰ç…§ä»åå‘å‰çš„é¡ºåºï¼Œ$$\argmax P(w_i|w_n,w_{n-1},w_{n-2}, ...w_{i+1})$$æ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­å‰ä¸€ä¸ªè¯æ˜¯â€œâ€ã€‚
>  ä»Šå¤© å¤©æ°” ä¸é”™ï¼Œ æˆ‘ä»¬ å» å…¬å›­ ç© å§ã€‚

è¿™å¥è¯ï¼Œå•å‘è¯­è¨€æ¨¡å‹åœ¨å­¦ä¹ çš„æ—¶å€™æ˜¯ä»å·¦å‘å³è¿›è¡Œå­¦ä¹ çš„ï¼Œå…ˆç»™æ¨¡å‹çœ‹åˆ°â€œä»Šå¤© å¤©æ°”â€ä¸¤ä¸ªè¯ï¼Œç„¶åå‘Šè¯‰æ¨¡å‹ä¸‹ä¸€ä¸ªè¦å¡«çš„è¯æ˜¯â€œä¸é”™â€ã€‚ç„¶è€Œå•å‘è¯­è¨€æ¨¡å‹æœ‰ä¸€ä¸ªæ¬ ç¼ºï¼Œå°±æ˜¯æ¨¡å‹å­¦ä¹ çš„æ—¶å€™æ€»æ˜¯æŒ‰ç…§å¥å­çš„ä¸€ä¸ªæ–¹å‘å»å­¦çš„ï¼Œå› æ­¤æ¨¡å‹å­¦ä¹ æ¯ä¸ªè¯çš„æ—¶å€™åªçœ‹åˆ°äº†ä¸Šæ–‡ï¼Œå¹¶æ²¡æœ‰çœ‹åˆ°ä¸‹æ–‡ã€‚æ›´åŠ åˆç†çš„æ–¹å¼åº”è¯¥æ˜¯è®©æ¨¡å‹åŒæ—¶é€šè¿‡ä¸Šä¸‹æ–‡å»å­¦ä¹ ï¼Œè¿™ä¸ªè¿‡ç¨‹æœ‰ç‚¹ç±»ä¼¼äºå®Œå½¢å¡«ç©ºé¢˜ã€‚ä¾‹å¦‚ï¼š

>ä»Šå¤© å¤©æ°” { }ï¼Œ æˆ‘ä»¬ å» å…¬å›­ ç© å§ã€‚

é€šè¿‡è¿™æ ·çš„å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æŠŠæ¡â€œä¸é”™â€è¿™ä¸ªè¯æ‰€å‡ºç°çš„ä¸Šä¸‹æ–‡è¯­å¢ƒã€‚

### å¾®è°ƒ fine tune
ç”±äºä½¿ç”¨æµ·é‡çš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œé¢„è®­ç»ƒæ¨¡å‹é€šå¸¸å…·æœ‰ä¸€èˆ¬çš„å¸¸è¯†ï¼Œç”±æ­¤ä½œä¸ºåŸºç¡€å†è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½æ›´å¥½çš„é€‚åˆç‰¹å®šä»»åŠ¡ã€‚
- æ¨¡å‹è°ƒæ•´
é€šå¸¸åšæ³•æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šå¢åŠ ä»»åŠ¡ç›¸å…³çš„å±‚ï¼Œç”±äºNLPçš„é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸æ˜¯åŒ…å«åºåˆ—ä¸Šä¸‹æ–‡çš„embeddingsï¼Œå¦‚ç”±å…¨è¿æ¥å±‚å’Œsoftmaxè¿ç®—æ„æˆçš„åˆ†ç±»å±‚ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚
![enter image description here](https://miro.medium.com/max/2248/1*GVcm-gUJ5r6niWB6OsOg_w.png)
- supervised learning
ä½¿ç”¨å°‘é‡ä»»åŠ¡ç›¸å…³çš„æ ‡è®°æ•°æ®æ¥è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸çš„åšæ³•æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åé¢ç›´æ¥åŠ ä¸Šä¸Šä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆç”±å…¨è¿æ¥å’Œsoftmaxè¿ç®—æ„æˆï¼‰ä½¿æ¨¡å‹è¾“å‡ºä¸€ä¸ªé¢„æµ‹ç±»å‹ï¼Œè®¡ç®—cross entropyè¯¯å·®ä»è€Œé€šè¿‡åå‘ä¼ é€’æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
	- ~~æ›´æ–°å…¨éƒ¨æ¨¡å‹å‚æ•°~~
	- åªæ›´æ–°ä»»åŠ¡å±‚å‚æ•° - é¢„è®­ç»ƒæ¨¡å‹åªä½œä¸ºç‰¹å¾æå–å™¨

## BERTç®€ä»‹
BERTï¼ˆBidirectional Encoder Representations from Transformerï¼‰ï¼ŒåŒä»–åå­—è¯´çš„ä¸€æ ·ï¼ŒBERTæ˜¯ä¸€ä¸ªåˆ©ç”¨Transformerå®ç°çš„åŒå‘ç¼–ç å™¨ï¼Œ  ç”¨äºæå–è¾“å…¥åºåˆ—ç‰¹å¾ä¿¡æ¯çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚When BERT was published it achieved [state-of-the-art] performance in 11 [natural language understanding] tasks:[[1]] [GLUE]task set (consisting of 8 tasks), [MultiNLI] [SQuAD] v1.1, SQuAD v2.0
2018, googleå‘è¡¨äº†è®ºæ–‡BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingï¼Œ 2019å¹´googleå°†BERTæ¨¡å‹åº”ç”¨åˆ°äº†æœç´¢æœåŠ¡ä¸­ï¼Œç°åœ¨å·²ç»æ”¯æŒäº†è¶…è¿‡70ç§è¯­è¨€

BERTæœ€å¤§çš„åˆ›æ–°æ˜¯å°†Transformeræ¨¡å‹åº”ç”¨åˆ°äº†è¯­è¨€æ¨¡å‹ä¸­ï¼Œå®ç°deep bidirectional contextual embeddingã€‚ã€‚ã€‚ã€‚å½±å“å’Œå†³å®šäº†BERTå¾ˆå¤šç‰¹æ®Šæ€§è´¨ã€‚


**bidirectional <-> LM çš„çŸ›ç›¾å¦‚ä½•è§£å†³ï¼Ÿ MLM+NSP** 

BERTæ¨¡å‹ç”Ÿæˆçš„å…ƒç´ ç¼–ç å±äºåŠ¨æ€çš„åŒå‘è¯­å¢ƒç¼–ç ï¼Œå®ƒèƒ½æ ¹æ®è¾“å…¥åºåˆ—ç”Ÿæˆæ¯ä¸ªåºåˆ—å…ƒç´ ï¼ˆwordï¼‰åœ¨åºåˆ—ä¸Šä¸‹æ–‡ä¸­çš„ç‰¹å¾å‘é‡ï¼Œ ä¸ELMOä¸åŒçš„æ˜¯ï¼Œå®ƒåŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰, åˆ©ç”¨Transformerå¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå®ç°äº†æ·±åº¦åŒå‘è¯­å¢ƒç¼–ç ï¼Œè¿™ä¹Ÿæ˜¯BERTçš„åŒºåˆ«äºä¼ ç»Ÿçš„åŒå‘ç¼–ç æŠ€æœ¯ï¼ˆå¦‚ELMOï¼‰æœ€å¤§åˆ›æ–°ä¹‹å¤„ã€‚é‚£ä¹ˆå¦‚ä½•ç†è§£**æ·±åº¦åŒå‘**å‘¢ï¼Ÿè¿™æ˜¯ç”±BERTä½¿ç”¨çš„Transformerç¼–ç å™¨çš„è‡ªèº«å±æ€§å†³å®šçš„ï¼Œæˆ‘ä»¬çŸ¥é“Transformeræ¨¡å‹æ˜¯ä»¥Attentionæœºåˆ¶ä¸ºåŸºç¡€ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä¸€æ¬¡çœ‹åˆ°æ‰€æœ‰çš„åºåˆ—å…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ çš„ç¼–ç çš„è®¡ç®—éƒ½åŒ…å«äº†è¯¥å…ƒç´ ä¹‹å‰å’Œä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œä»æ–¹å‘æ¥è¯´ï¼ŒåŒæ—¶åŒ…å«äº†ä¹‹å‰å’Œä¹‹åä¸¤ä¸ªæ–¹å‘ï¼Œä»è·ç¦»æ¥è®²ï¼ŒåŒæ—¶è®¡ç®—shu'youå¹¶ä¸”ç”±äºèƒ½å¤ŸåŒæ—¶çœ‹åˆ°å‰å‘å’Œåå‘çš„ä¿¡æ¯ï¼ŒBERTä¸åŒäºä»¥å¾€çš„åŒå‘è¯­è¨€æ¨¡å‹ï¼Œå¦‚ELMOï¼Œç‹¬ç«‹çš„è¿›è¡Œå‰å‘å’Œåå‘çš„ï¼Œ 
- æ–¹å‘
- è·ç¦»shuyou
- 
![enter image description here](https://miro.medium.com/max/1234/1*KbAUVetHPMreJdcbicmJrw.png)
å¹¶éæ‰€æœ‰çš„åŸºäºattentionæœºåˆ¶çš„æ¨¡å‹éƒ½æ˜¯åŒå‘è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚GPTä½¿ç”¨äº†é®ç½©çš„æ–¹å¼ä½¿æ¨¡å‹æ— æ³•çœ‹åˆ°å½“å‰å…ƒç´ ä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤å®ƒå±äºå•å‘è¯­è¨€æ¨¡å‹ã€‚

- 

## BERTé¢„è®­ç»ƒæ¨¡å‹ç»“æ„
### Transformer encoder based
BERTæ¨¡å‹ä¸»è¦åŒ…å«è¿™ä¸ªéƒ¨åˆ†ï¼Œç¼–ç å±‚å’ŒTransformerç¼–ç å™¨
![enter image description here](https://www.lyrn.ai/wp-content/uploads/2018/11/transformer.png)

### ç¼–ç å±‚
ç¼–ç å±‚çš„ä½œç”¨æ˜¯
1. å°†è¾“å…¥è¯­å¥ï¼ˆBERT is powerfulï¼‰è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æµ®ç‚¹æ•°å‘é‡
2. åŠ å…¥ç‰¹æ®Šç¬¦å·[CLS][SEP] -- No! this is done in data preprocessing

    embeddings = inputs_embeds + position_embeddings + token_type_embeddings
    ä¸ºä»€ä¹ˆå¯ä»¥ç›¸åŠ ï¼Ÿ[https://www.zhihu.com/question/374835153/answer/1069173198](https://www.zhihu.com/question/374835153/answer/1069173198)

[https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
![enter image description here](https://i.stack.imgur.com/QCcYF.png)
- è¯ç¼–ç (config.vocab_size, config.hidden_size, padding_idx=0)
[https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding](https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding)
- æ®µç¼–ç (config.type_vocab_size, config.hidden_size)
åœ¨BERTå¤„ç†å¤šæ¡è¯­å¥æ—¶ï¼Œç”¨äºåŒºåˆ†ä¸åŒè¯­å¥
- ä½ç½®ç¼–ç (config.max_position_embeddings, config.hidden_size)
ç”±äºæ³¨æ„åŠ›è®¡ç®—ä¸å…³å¿ƒè¾“å…¥åºåˆ—å…ƒç´ çš„å…ˆåå¾ªåºï¼Œå› æ­¤éœ€è¦äº‹å…ˆåŠ å…¥ä½ç½®ä¿¡æ¯å†è¾“å…¥æ¨¡å‹ã€‚ä¸åŒäºTransformerçš„åŸºäºå‘¨æœŸå‡½æ•°çš„å›ºå®šä½ç½®ç¼–ç æ–¹æ³•ï¼ŒBERTé‡‡ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œbertä¸­çš„æœ€å¤§å¥å­é•¿åº¦æ˜¯512 æ‰€ä»¥Position Embedding layer æ˜¯ä¸€ä¸ªsizeä¸ºï¼ˆ512ï¼Œ768ï¼‰çš„lookup tableï¼Œå…¶ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œéšé¢„è®­ç»ƒè¿™äº›ä½ç½®ç›¸å…³çš„å‚æ•°æ”¶æ•›ï¼Œã€‚ã€‚ã€‚**ç›¸æ¯”Transformerçš„ä½ç½®ç¼–ç ï¼Œä¼¼ä¹æ²¡è€ƒè™‘ç›¸å¯¹ä½ç½®????**
- why 512?
>Theoretically there is nothing restricting a Transformer to have greater sequence length. Practically, there are resource constraints - especially memory complexity when doing self-attention which is quadratic in terms of sequence length. Another reason why BERT is restricted to 512 may be because that was the sequence length it was originally restricted to while training but I am not sure.
>[https://github.com/google-research/bert/issues/27](https://github.com/google-research/bert/issues/27)
>[https://github.com/google-research/bert/issues/66](https://github.com/google-research/bert/issues/66)
>We don't plan to make major changes to this library, so anything like that would be part of a separate project.
Our recommended recipe is exactly what you describe (it's what we do for SQuAD), but you can actually fine-tune on it normally (we just don't do it for SQuAD because only a few percent of SQuAD documents are longer than 384 do so it didnt matter. But we should have).
Let's say you have:
`the man went to the store and bought a gallon of milk`
And had  `max_seq_length = 6, stride = 3`, then you could split it up like this:
```
the man went to the store
to the store and bought a
and bought a gallon of milk
```
>So from  `BertModel`'s perspective this is a 3x6 minibatch, but crucially you can reshape it after you get it back from  `BertModel.get_sequence_output()`  and softmax over all the tokens when you compute the loss (with some masking to make sure you don't double count the boundary words like  `to the store`  and  `and bought a`). So you will be fine-tuning over the whole document end-to-end. The exact implementation is task-specific of course.

### Transformerç¼–ç å™¨
Transformeræ¨¡å‹æ˜¯ç”±google aiäº2017å¹´å‘å¸ƒçš„ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼Œæœ€åˆåº”ç”¨äºæœºå™¨ç¿»è¯‘ã€‚Transformerçš„æœ€å¤§ç‰¹ç‚¹æ˜¯ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰ï¼Œè§£å†³äº†ä½¿ç”¨RNNæ¨¡å‹é€ æˆçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ— æ³•å¹¶è¡Œçš„é—®é¢˜ï¼Œå¹¶ä¸”å®è·µè¯æ˜transformerä¸­æå‡ºçš„å¤šå¤´æ³¨æ„åŠ›å…·æœ‰å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œæ€§èƒ½è¶…è¶Šäº†RNN,CNNç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚
> Transformeræ‰€ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å»è®¡ç®—ä¸€å¥è¯ä¸­çš„æ¯ä¸ªè¯å¯¹äºè¿™å¥è¯ä¸­æ‰€æœ‰è¯çš„ç›¸äº’å…³ç³»ï¼Œç„¶åè®¤ä¸ºè¿™äº›è¯ä¸è¯ä¹‹é—´çš„ç›¸äº’å…³ç³»åœ¨ä¸€å®šç¨‹åº¦ä¸Šååº”äº†è¿™å¥è¯ä¸­ä¸åŒè¯ä¹‹é—´çš„å…³è”æ€§ä»¥åŠé‡è¦ç¨‹åº¦ã€‚å› æ­¤å†åˆ©ç”¨è¿™äº›ç›¸äº’å…³ç³»æ¥è°ƒæ•´æ¯ä¸ªè¯çš„é‡è¦æ€§ï¼ˆæƒé‡ï¼‰å°±å¯ä»¥è·å¾—æ¯ä¸ªè¯æ–°çš„è¡¨è¾¾ã€‚è¿™ä¸ªæ–°çš„è¡¨å¾ä¸ä½†è•´å«äº†è¯¥è¯æœ¬èº«ï¼Œè¿˜è•´å«äº†å…¶ä»–è¯ä¸è¿™ä¸ªè¯çš„å…³ç³»ï¼Œå› æ­¤å’Œå•çº¯çš„è¯å‘é‡ç›¸æ¯”æ˜¯ä¸€ä¸ªæ›´åŠ å…¨å±€çš„è¡¨è¾¾ã€‚
> Transformeré€šè¿‡å¯¹è¾“å…¥çš„æ–‡æœ¬ä¸æ–­è¿›è¡Œè¿™æ ·çš„æ³¨æ„åŠ›æœºåˆ¶å±‚å’Œæ™®é€šçš„éçº¿æ€§å±‚äº¤å æ¥å¾—åˆ°æœ€ç»ˆçš„æ–‡æœ¬è¡¨è¾¾ã€‚

Transformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆwordï¼‰è½¬æ¢ä¸ºåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œå†ç”±è§£ç å™¨æ ¹æ®ç¼–ç åçš„ç‰¹å¾å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚BERTæ¨¡å‹ä¸­åªä½¿ç”¨äº†transformerçš„ç¼–ç å™¨ï¼Œå®ƒä¸»è¦ç”±è‹¥å¹²ä¸ªç»“æ„ç›¸åŒçš„ç¼–ç å±‚è¿æ¥è€Œæˆã€‚æ¯ä¸€ä¸ªç¼–ç å±‚ä¸»è¦æœ‰ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒï¼ˆMulti-Head Attentionï¼‰å’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œ(Feed Forward)ç»„æˆï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒè´Ÿè´£ä¸ºæ¯ä¸ªè¾“å…¥å…ƒç´ ç”Ÿæˆç‰¹å¾å‘é‡ï¼Œå‰é¦ˆç½‘ç»œèƒ½å¤Ÿé€šè¿‡ç»„åˆå…ƒç´ ç‰¹å¾å‘é‡ç”Ÿæˆæ›´å¤æ‚çš„ç‰¹å¾å‘é‡ã€‚

![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSqp25HORnsDrfUfkTFUgKeTC7IITVZrTMXBuf6eSp4_HmCsGRoGwAxEoN87fuhT98Xsc4IulE_U4vM/pub?w=960&h=720)

## BERTçš„é¢„è®­ç»ƒ
è®­ç»ƒæ•°æ®: 
- BooksCorpus (800M words)
- EnglishWikipedia (2.5B words)

### ä»»åŠ¡è®¾è®¡
BERTçš„é¢„è®­ç»ƒè¢«è®¾è®¡ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰ï¼ŒåŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼šä¸€ä¸ªæ˜¯ Masked Language Modelï¼Œå¦ä¸€ä¸ªæ˜¯ Next Sentence Predictionã€‚è¿™ç§è®¾è®¡çš„åŸå› æ˜¯ç”±äºBERTä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶æœ‰å…¨å±€çš„è§†é‡ï¼Œèƒ½å¤Ÿä¸€æ¬¡åŒæ—¶è®¿é—®åºåˆ—çš„æ‰€æœ‰å…ƒç´ ï¼Œå› æ­¤æ— æ³•ä½¿ç”¨ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹é‚£ç§ä¸€æ­¥ä¸€çœ‹çš„è®­ç»ƒæ–¹å¼ã€‚**å‰è€…ç”¨äºå»ºæ¨¡æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡ mask æ¥å¼ºåˆ¶æ¨¡å‹ç»™æ¯ä¸ªè¯è®°ä½æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›åè€…ç”¨æ¥å»ºæ¨¡å¤šä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼Œ**
![enter image description here](https://www.researchgate.net/profile/Jan_Christian_Blaise_Cruz/publication/334160936/figure/fig1/AS:776030256111617@1562031439583/Overall-BERT-pretraining-and-finetuning-framework-Note-that-the-same-architecture-in.ppm)
#### Masked Language Model  - MLM
æ³¨æ„åŠ›æœºåˆ¶çš„ä½¿ç”¨ä½¿å¾—BERTæ¨¡å‹èƒ½å¤ŸåŒæ—¶â€œçœ‹åˆ°â€æ‰€æœ‰çš„åºåˆ—å…ƒç´ ï¼Œå› æ­¤æ— æ³•ä½¿ç”¨ä¼ ç»Ÿè¯­è¨€æ¨¡å‹é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªå…ƒç´ çš„æ–¹å¼æ¥è¿›è¡Œè®­ç»ƒã€‚å› æ­¤BERTä½¿ç”¨äº†é¢„æµ‹éšæœºé®ç½©å…ƒç´ çš„æ–¹å¼ï¼Œå³masked language modelã€‚è¿™ç§MLMè®­ç»ƒçš„æ€è·¯ç±»ä¼¼äºå¡«è¯æ¸¸æˆï¼Œé€šè¿‡ä¸Šä¸‹æ–‡çš„ä¿¡æ¯æ¥åˆ¤æ–­æ¨¡å‹è¢«éšè—çš„è¯ï¼Œï¼ˆå¦‚æœmaskå¤ªå¤šï¼Œä¼šä¸¢å¤±contextï¼Œå¦‚æœmaskå¤ªå°‘ï¼Œè®­ç»ƒå¤ªæ…¢ï¼‰
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
BERTçš„å…·ä½“åšæ³•æ˜¯ç»™å®šä¸€ä¸ªå¥å­ï¼ŒéšæœºMask 15%çš„è¯ï¼ˆå³ç”¨[Mask]æ¥æ›¿æ¢åŸæ¥çš„è¯ï¼‰ï¼Œç„¶åè¾“å…¥BERTæ¨¡å‹å¹¶è®©BERTæ¥é¢„æµ‹è¿™äº›Maskçš„è¯ï¼Œ~~å¦‚åŒä¸Šè¿°10.1æ‰€è¿°ï¼Œåœ¨è¾“å…¥ä¾§å¼•å…¥[Mask]æ ‡è®°ï¼Œä¼šå¯¼è‡´é¢„è®­ç»ƒé˜¶æ®µå’ŒFine-tuningé˜¶æ®µä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå› æ­¤åœ¨è®ºæ–‡ä¸­ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé‡‡å–äº†å¦‚ä¸‹æªæ–½ï¼š~~

å¯¹äºæ¯ä¸ªåœ¨è¢«é€‰ä¸­çš„15%çš„Tokené‡Œï¼Œåˆ™æŒ‰ç…§ä¸‹é¢çš„æ–¹å¼éšæœºçš„æ‰§è¡Œï¼š

-   80%çš„æ¦‚ç‡æ›¿æ¢æˆ[MASK]ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is [MASK]
-   10%çš„æ¦‚ç‡æ›¿æ¢æˆéšæœºçš„ä¸€ä¸ªè¯ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is apple
-   10%çš„æ¦‚ç‡æ›¿æ¢æˆå®ƒæœ¬èº«ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is hairy

BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context.
BERT is [MASK1] to help **milk** understand the meaning of ambiguous language in text by using **surrounding** text to [MASK2] context
ä»»åŠ¡ç›®æ ‡ï¼š é¢„æµ‹æ‰€æœ‰[MASK] ä»¥åŠmilkå’Œsurroundingä½ç½®ä¸Šçš„è¯
æµ‹è¯•æ•°æ®ï¼š[MASK1]=designed, milk=computers, surrounding=surrounding, [MASK2]=establish

 - å¦‚æœåªåš[MASK]æ›¿æ¢ï¼Œé¢„è®­ç»ƒæ¨¡å‹ä¼šè¢«è®­ç»ƒä¸ºå¯¹[MASK]è¿›è¡Œé¢„æµ‹ï¼Œæ‰€ä»¥åªä¼šåŠ å¼º[MASK]é™„è¿‘ä¸Šä¸‹æ–‡çš„åˆ†æè€Œä¸æ˜¯å…¨éƒ¨åºåˆ—çš„åˆ†æã€‚ è€Œå¾®è°ƒé˜¶æ®µçš„ç›®æ ‡æ˜¯åˆ†ææ•´ä¸ªåºåˆ—ï¼Œå®ƒçš„è¾“å…¥ä¸åŒ…å«[MASK]ï¼Œä¸é¢„è®­ç»ƒæ¨¡å‹çš„ç›®æ ‡ä¸ä¸€è‡´ï¼Œå› æ­¤ä¼šå¯¼è‡´é¢„è®­ç»ƒæ¨¡å‹åœ¨å¾®è°ƒé˜¶æ®µæ€§èƒ½ä¸‹é™ã€‚
 - ä¸ºäº†æ›´åŠ ç¬¦åˆå¾®è°ƒé˜¶æ®µçš„ç›®æ ‡ï¼Œä½œè€…åŠ å…¥äº†ä¸€ç§æ–°çš„é¢„å¤„ç†æ–¹å¼ï¼Œå³ä»¥10%çš„å‡ ç‡éšæœºå°†åŸè¯computeræ›¿æ¢ä¸ºå…¶ä»–è¯milkè€Œä¸æ˜¯[MASK]ï¼Œä¸ºäº†å¾—å‡ºæ­£ç¡®ç»“æœï¼ˆcomputerï¼‰æ¨¡å‹éœ€è¦åˆ†æmilkçš„ä¸Šä¸‹æ–‡ã€‚ç”±äºæ‰€æœ‰çš„è¯éƒ½å¯èƒ½è¢«æ›¿æ¢ï¼Œè¿™å°±è¦æ±‚æ¨¡å‹è¦å¯¹æ‰€æœ‰è¾“å…¥å…ƒç´ çš„ä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æï¼Œä»è€Œæ»¡è¶³å¾®è°ƒçš„éœ€è¦ã€‚
 - è€ƒè™‘åˆ°å¦‚æœåªç”¨[Mask]å’Œä»»æ„è¯è¿›è¡Œæ›¿æ¢ï¼Œæ¨¡å‹ä¼šè®¤ä¸ºçœ‹åˆ°å½“å‰çš„è¯éƒ½æ˜¯ä¸çœŸå®çš„ï¼ˆæ›¿æ¢è¿‡çš„ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´ç”Ÿæˆembeddingçš„è¿‡ç¨‹å®Œå…¨ä¸å‚è€ƒå½“å‰è¯ã€‚ä¸ºæ­¤é¢„è®­ç»ƒæ—¶ä¹Ÿä¼šä¹Ÿ10%çš„æ¦‚ç‡ä½¿ç”¨åŸè¯æ›¿æ¢ï¼ˆå¦‚surroundingï¼‰ï¼Œè¿™æ ·æ¨¡å‹ä¹Ÿä¼šå‚è€ƒå½“å‰è¯æ¥ç”Ÿæˆembeddingã€‚
 - å¯¹äºä¸ºä½•ä¹Ÿ80%ï¼Œ10%å’Œ10%çš„æ¯”ä¾‹åˆ†åˆ«è¿›è¡ŒMaskï¼Œéšæœºè¯å’ŒåŸè¯æ›¿æ¢ï¼Œä½œè€…çš„è§£é‡Šæ˜¯åŸºäºç»éªŒè®¾è®¡çš„æ¯”ä¾‹ï¼Œå¯èƒ½å­˜åœ¨æ•ˆæœæ›´å¥½çš„æ¯”ä¾‹åˆ†å¸ƒï¼Œä½†æ˜¯æœ€ç»ˆç»“æœåº”è¯¥ç›¸å·®ä¸å¤§ã€‚
 > We didn't try a lot of ablation on this. Those numbers are just what made sense to me and the only thing that I tried. It's possible that other values will work better (or more likely, the system isn't very sensitive to the exact hyperparameters).   [https://github.com/google-research/bert/issues/85](https://github.com/google-research/bert/issues/85)
- æœ€åï¼Œç”±äºMLMåªé¢„æµ‹15%çš„åºåˆ—å…ƒç´ ï¼Œå› æ­¤æ¯”æ ‡å‡†LMè®­ç»ƒé€Ÿåº¦è¦æ…¢ã€‚

>_Why did they not use a â€˜<MASK>â€™ replacement token all around?_
If the model had been trained on only predicting â€˜<MASK>â€™ tokens and then never saw this token during fine-tuning, it would have thought that there was no need to predict anything and this would have hampered performance. Furthermore, the model would have only learned a contextual representation of the â€˜<MASK>â€™ token and this would have made it learn slowly (since only 15% of the input tokens are masked). By sometimes asking it to predict a word in a position that did not have a â€˜<MASK>â€™ token, the model needed to learn a contextual representation of  _all_  the words in the input sentence, just in case it was asked to predict them afterwards.
_Are not random tokens enough? Why did they leave some sentences intact?_
Well, ideally we want the modelâ€™s representation of the masked token to be better than random. By sometimes keeping the sentence intact (while still asking the model to predict the chosen token) the authors biased the model to learn a meaningful representation of the masked tokens.
_Will random tokens confuse the model?_
The model will indeed try to use the embedding of the random token to help in its prediction and it will learn that it was actually not useful once it sees the target (correct token). However, the random replacement happened in 1.5% of the tokens (10%*15%) and the authors claim that it did not affect the modelâ€™s performance.
_The model will only predict 15% of the tokens but language models predict 100% of tokens, does this mean that the model needs more iterations to achieve the same loss?_
Yes, the model does converge more slowly but the increased steps in converging are justified by an considerable improvement in downstream performance.
##### MLM loss

#### NSP

>Next Sentence Predictionï¼ˆNSPï¼‰çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å¥å­Bæ˜¯å¦æ˜¯å¥å­Açš„ä¸‹æ–‡ã€‚å¦‚æœæ˜¯çš„è¯è¾“å‡ºâ€™IsNextâ€˜ï¼Œå¦åˆ™è¾“å‡ºâ€™NotNextâ€˜ã€‚è®­ç»ƒæ•°æ®çš„ç”Ÿæˆæ–¹å¼æ˜¯ä»å¹³è¡Œè¯­æ–™ä¸­éšæœºæŠ½å–çš„è¿ç»­ä¸¤å¥è¯ï¼Œå…¶ä¸­50%ä¿ç•™æŠ½å–çš„ä¸¤å¥è¯ï¼Œå®ƒä»¬ç¬¦åˆIsNextå…³ç³»ï¼Œå¦å¤–50%çš„ç¬¬äºŒå¥è¯æ˜¯éšæœºä»é¢„æ–™ä¸­æå–çš„ï¼Œå®ƒä»¬çš„å…³ç³»æ˜¯NotNextçš„ã€‚è¿™ä¸ªå…³ç³»ä¿å­˜åœ¨å›¾4ä¸­çš„`[CLS]`ç¬¦å·ä¸­ã€‚

>_Why is a second task necessary at all?_
The authors pre-trained their model in  _Next Sentence Prediction_  because they thought important that the model knew how to relate two different sentences to perform downstream tasks like question answering or natural language inference and the â€œmasked language modelâ€ did not capture this knowledge. They prove that pre-training with this second task notably increases performance in both question answering and natural language inference.
_What percentage of sentences where actually next sentences?_
50% of the sentences were paired with actual adjacent sentences in the corpus and 50% of them were paired with sentences picked randomly from the corpus.
##### NSP loss
### é¢„è®­ç»ƒ=BERT + MSMï¼ŒNSP head
![enter image description here](https://miro.medium.com/max/1270/1*i8zICfESnaGt4EVRcWBLKw.png)
### æŸå¤±å‡½æ•°
total_loss = masked_lm_loss + next_sentence_loss
ä¸ä»»åŠ¡ç›¸å¯¹åº”ï¼ŒBERTçš„æŸå¤±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯æ¥è‡ª Mask-LM çš„**å•è¯çº§åˆ«åˆ†ç±»ä»»åŠ¡**ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯**å¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡**ã€‚é€šè¿‡è¿™ä¸¤ä¸ªä»»åŠ¡çš„è”åˆå­¦ä¹ ï¼Œå¯ä»¥ä½¿å¾— BERT å­¦ä¹ åˆ°çš„è¡¨å¾æ—¢æœ‰ token çº§åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¹ŸåŒ…å«äº†å¥å­çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ã€‚å…·ä½“æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%2C+%5Ctheta_%7B2%7D%5Cright%29%3DL_%7B1%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%2BL_%7B2%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29)

å…¶ä¸­  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta)  â€‹ æ˜¯ BERT ä¸­ Encoder éƒ¨åˆ†çš„å‚æ•°ï¼Œâ€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta_1)  æ˜¯ Mask-LM ä»»åŠ¡ä¸­åœ¨ Encoder ä¸Šæ‰€æ¥çš„è¾“å‡ºå±‚ä¸­çš„å‚æ•°ï¼Œâ€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta_2)  åˆ™æ˜¯å¥å­é¢„æµ‹ä»»åŠ¡ä¸­åœ¨ Encoder æ¥ä¸Šçš„åˆ†ç±»å™¨å‚æ•°ã€‚å› æ­¤ï¼Œåœ¨ç¬¬ä¸€éƒ¨åˆ†çš„æŸå¤±å‡½æ•°ä¸­ï¼Œå¦‚æœè¢« mask çš„è¯é›†åˆä¸º Mï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªè¯å…¸å¤§å° |V| ä¸Šçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆå…·ä½“è¯´æ¥æœ‰ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L_%7B1%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Clog+p%5Cleft%28m%3Dm_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%2C+m_%7Bi%7D+%5Cin%5B1%2C2%2C+%5Cldots%2C%7CV%7C%5D)

åœ¨å¥å­é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L_%7B2%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29%3D-%5Csum_%7Bj%3D1%7D%5E%7BN%7D+%5Clog+p%5Cleft%28n%3Dn_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29%2C+n_%7Bi%7D+%5Cin%5B%5Ctext+%7BIsNext%7D%2C+%5Ctext+%7BNotNext%7D%5D)

å› æ­¤ï¼Œä¸¤ä¸ªä»»åŠ¡è”åˆå­¦ä¹ çš„æŸå¤±å‡½æ•°æ˜¯ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%2C+%5Ctheta_%7B2%7D%5Cright%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Clog+p%5Cleft%28m%3Dm_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29-%5Csum_%7Bj%3D1%7D%5E%7BN%7D+%5Clog+p%5Cleft%28n%3Dn_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29)

### é¢„è®­ç»ƒæŠ€å·§
å…·ä½“çš„é¢„è®­ç»ƒå·¥ç¨‹å®ç°ç»†èŠ‚æ–¹é¢ï¼ŒBERT è¿˜åˆ©ç”¨äº†ä¸€ç³»åˆ—ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹æ›´æ˜“äºè®­ç»ƒï¼Œé™¤äº†å¸¸ç”¨çš„layer normalizationï¼Œdropoutä¹‹å¤–ï¼Œè¿˜æœ‰å¯¹äºå­¦ä¹ ç‡çš„ warm-up ç­–ç•¥ï¼Œä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ä¸å†æ˜¯æ™®é€šçš„ ReLuï¼Œè€Œæ˜¯ GeLuã€‚
- Transformer related :  dropout, layer_norm, residual
- 
### é¢„è®­ç»ƒæµç¨‹
é¢„è®­ç»ƒçš„ç›®çš„æ˜¯ç”Ÿæˆèƒ½å¤Ÿç»™ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨çš„é€šç”¨æ¨¡å‹ï¼Œå› æ­¤BERTåœ¨é¢„è®­ç»ƒä¸­åŠ å…¥ä¸¤ä¸ªç‰¹æ®Štokenï¼ŒCLSå’ŒSEPã€‚
CLSåŠ åœ¨è¾“å…¥åºåˆ—çš„å¼€å¤´ï¼Œå®ƒä¹Ÿå‚ä¸Transformerè®¡ç®—ã€‚æˆ‘ä»¬çŸ¥é“æ³¨æ„åŠ›è®¡ç®—æ˜¯å¯¹æ‰€æœ‰å…ƒç´ ä»¥ä¸€å®šçš„æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œç”±äºCLSæœ¬èº«ä¸åŒ…å«ä»»ä½•æ„ä¹‰ï¼Œå› æ­¤ä¸åºåˆ—ä¸­çš„å…¶ä»–å…ƒç´ éƒ½ä¸ç›¸å…³ï¼Œå› æ­¤CLS tokené€šè¿‡æ³¨æ„åŠ›è¿ç®—çš„ç»“æœæ˜¯å°†æ‰€æœ‰å…ƒç´ çš„æ„æ€ä»¥ç›¸ä¼¼çš„æƒé‡è¿›è¡ŒåŠ æƒå¹³å±€ï¼Œè¿™ä¹Ÿå°±æ˜¯æ•´ä¸ªåºåˆ—çš„unbiasæ„ä¹‰ã€‚ç”±äºCLS embeddingåŒ…å«äº†è¿™ä¸ªåºåˆ—çš„å«ä¹‰ï¼Œå› æ­¤åœ¨å¯¹åºåˆ—è¿›è¡Œåˆ†ç±»ç­‰å¾®è°ƒä»»åŠ¡ä¸­ä¼šç›´æ¥å¯¹CLS embeddingè¿›è¡Œåˆ†ç±»è®­ç»ƒã€‚
å¦ä¸€ä¸ªç‰¹æ®Štokenæ˜¯SEPï¼Œå½“è¾“å…¥åºåˆ—ä¸­åŒ…å«å¤šä¸ªå¥å­æ—¶ï¼Œä½¿ç”¨è¿™ä¸ªtokenåˆ†éš”ä¸åŒçš„å¥å­ã€‚å’ŒCLSä¸åŒçš„æ˜¯ï¼ŒSEP embeddingæœ¬èº«ä¸ä¼šç”¨äºå¾®è°ƒä»»åŠ¡ï¼Œå®ƒä¸»è¦ç”¨äºé¢„è®­ç»ƒä¸­çš„NSPå­ä»»åŠ¡ã€‚
>The pre-training corpus was built from BookCorpus (800M words) and English Wikipedia (2,500M words). Tokens were tokenized using 37,000 WordPiece tokens.
To generate the pre-training sequences, the authors got random samples in batches of two (50% of the time adjacent to each other) such that the combined length of the two chosen sentences was â‰¤512 tokens. Once each sequence was built, 15% of its tokens were masked.
An example of a pre-training sequence presented in the paper is:
> > Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
In this case the sentences are adjacent, so the label in [CLS] would be â€˜<IsNext>â€™ as in:
> > Input = <IsNext> the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
The loss was calculated as the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.
![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vRFdq5CGCgn5WdAHdz88Z5ePsIU58vHz0HVYx56PQ3TP7Xi2WAbSkAbWx1Q4VA8ZkJ3mpSvlpmV1v-0/pub?w=1746&h=911)
[http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)  Recapping a sentenceâ€™s journey
Each training data contains Two sentences, $W_1[w_{11}, w_{12}, w_{13}, w_{14}, w_{15}], W2[w_{21}, w_{22},w_{23},w_{24},w_{25}]$
1. é¢„å¤„ç†: 
    1.0  tokenization
	1.1 . åŠ å…¥ç‰¹æ®Šç¬¦å·CLSå’ŒSEPï¼š [CLS] BERT is awesome. [SEP] I love BERT. [SEP]"
	1.2.  Add masks to some words: [CLS] BERT [MASK] awesome. [SEP] I love BERT. [SEP
	1.3.  Generate pretrain data
		1.3.1 for NSP:  50% isNext, 50% isNotNext
		1.3.2 for MLM: {tokens:["CLS", "BERT", "MASK", "awesome", "SEP"], masked_token:{index:2, value:"is"}}
		
2. Embedding
	2.1 word embedding(WE):  wordpiece tokenization (shape=(vocab_size * hidden_size))
	2.2 positional embedding(PE): (shape=(max_position * hidden_size))
	2.3 segment embedding(SE): (shape=(segment_size * hidden_size))
	E = WE + PE + SE
3. Transformerç¼–ç : 
    Bert embeddings = Transformer(E)
4. é¢„æµ‹
	
6. è®¡ç®—loss(mlm loss + nsp loss)ï¼Œæ›´æ–°weights
	

## BERTçš„å¾®è°ƒfine tune
å¦‚ä¸Šæ‰€è¿°ï¼ŒBERTè¿™ç§é€šç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ©ç”¨æ·±åº¦å¾ˆå¤§çš„å¤æ‚æ¨¡å‹æ¥æå–å¤æ‚ç‰¹å¾ï¼Œè¿™ä½¿å¾—å®ƒçš„é¢„è®­ç»ƒéœ€è¦å¼ºå¤§çš„è®¡ç®—èµ„æºï¼Œæ™®é€šäººåŸºæœ¬æ— æ³•å‚ä¸ã€‚é¢„è®­ç»ƒ-å¾®è°ƒè¿™ç§ä¸¤é˜¶æ®µè®­ç»ƒ
å› æ­¤ç›¸æ¯”é¢„è®­ç»ƒï¼Œå¾®è°ƒå¯¹äºæ²¡æœ‰å¤§é‡è®¡ç®—èµ„æºçš„æ™®é€šçˆ±å¥½è€…æ›´å…·ç°å®æ„ä¹‰ã€‚
é€šè¿‡ä¸åŒç±»å‹çš„å¾®è°ƒä»»åŠ¡ï¼ŒBERTå¯ä»¥å®Œæˆå¤šç§ç±»å‹çš„å­¦ä¹ ä»»åŠ¡ã€‚
**æ–¹æ³•ï¼šå›ºå®šé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œè®­ç»ƒå¾®è°ƒå±‚çš„å‚æ•°**
![enter image description here](https://www.researchgate.net/profile/Jan_Christian_Blaise_Cruz/publication/334160936/figure/fig1/AS:776030256111617@1562031439583/Overall-BERT-pretraining-and-finetuning-framework-Note-that-the-same-architecture-in.ppm)
### å¾®è°ƒä»»åŠ¡ç±»å‹
![enter image description here](https://lilianweng.github.io/lil-log/assets/images/BERT-downstream-tasks.png)
### è¯­ä¹‰åˆ†æ
è¿™ç§ç±»å‹çš„ä»»åŠ¡å¯¹è¾“å…¥ï¼ˆä¸€å¥è¯ï¼‰è¿›è¡Œè¯­ä¹‰åˆ†æã€‚è¾“å…¥ä¸€å¥è¯ï¼Œé¢„æµ‹è¿™å¥è¯çš„åˆ†ç±»ï¼Œå¦‚åˆ†æä¸€æ¡è´­ä¹°è¯„ä»·çš„è¯­ä¹‰æ˜¯è‚¯å®šçš„è¿˜æ˜¯å¦å®šçš„ã€‚
- è®­ç»ƒæ•°æ®
	- $x={x_1, x_2, x_3, ... , x_n}, y=label$
- Make use of the CLS token
- å¾®è°ƒå±‚ç»“æ„ï¼šåˆ†ç±»å™¨ï¼ˆå…¨è¿æ¥+softmaxï¼‰[https://github.com/huggingface/transformers/blob/c67d1a0259cbb3aef31952b4f37d4fee0e36f134/src/transformers/modeling_bert.py#L1234-L1241](https://github.com/huggingface/transformers/blob/c67d1a0259cbb3aef31952b4f37d4fee0e36f134/src/transformers/modeling_bert.py#L1234-L1241)

    class BertForSequenceClassification(BertPreTrainedModel):
        def __init__(self, config):
            super().__init__(config)
            self.num_labels = config.num_labels
            self.bert = BertModel(config)
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
            self.classifier = nn.Linear(config.hidden_size, config.num_labels)

[https://github.com/huggingface/transformers/blob/c67d1a0259cbb3aef31952b4f37d4fee0e36f134/src/transformers/modeling_bert.py#L1291-L1299](https://github.com/huggingface/transformers/blob/c67d1a0259cbb3aef31952b4f37d4fee0e36f134/src/transformers/modeling_bert.py#L1291-L1299)
è®­ç»ƒï¼ˆå›¾ï¼Ÿï¼‰
- æŸå¤±å‡½æ•°ï¼šcross-entropy
- 
é¢„æµ‹

    from transformers import BertTokenizer, BertForSequenceClassification
    import torch
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
    inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
    outputs = model(**inputs, labels=labels)
    loss, logits = outputs[:2]

~~### è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ
è¾“å…¥ä¸¤å¥è¯ï¼Œåˆ†æä»–ä»¬çš„è¯­ä¹‰æ˜¯ç›¸ä¼¼çš„è¿˜æ˜¯ä¸åŒçš„ã€‚
é¢„å¤„ç†
![](https://pic1.zhimg.com/80/v2-971f887ed616ea0f65941c8dc15ee128_720w.jpg)
  å®é™…æ“ä½œæ—¶ï¼Œä¸Šè¿°æœ€åä¸€å¥è¯ä¹‹åè¿˜ä¼šåŠ ä¸€ä¸ª[SEP] tokenï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡å°†ä¸¤ä¸ªå¥å­æŒ‰ç…§ä¸Šè¿°æ–¹å¼è¾“å…¥å³å¯ï¼Œä¹‹åä¸è®ºæ–‡ä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå°†[CLS] tokenä½ç½®å¯¹åº”çš„è¾“å‡ºï¼Œæ¥ä¸Šsoftmaxåšåˆ†ç±»å³å¯(å®é™…ä¸ŠGLUEä»»åŠ¡ä¸­å°±æœ‰å¾ˆå¤šè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ•°æ®é›†)ã€‚
  å¾®è°ƒå±‚ï¼š~~

~~### å¤šæ ‡ç­¾åˆ†ç±» NER
å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå³MultiLabelï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªæ ·æœ¬å¯èƒ½åŒæ—¶å±äºå¤šä¸ªç±»ï¼Œå³æœ‰å¤šä¸ªæ ‡ç­¾ã€‚ä»¥å•†å“ä¸ºä¾‹ï¼Œä¸€ä»¶Lå°ºå¯¸çš„æ£‰æœï¼Œåˆ™è¯¥æ ·æœ¬å°±æœ‰è‡³å°‘ä¸¤ä¸ªæ ‡ç­¾â€”â€”å‹å·ï¼šLï¼Œç±»å‹ï¼šå†¬è£…ã€‚
å¯¹äºå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ˜¾è€Œæ˜“è§çš„æœ´ç´ åšæ³•å°±æ˜¯ä¸ç®¡æ ·æœ¬å±äºå‡ ä¸ªç±»ï¼Œå°±ç»™å®ƒè®­ç»ƒå‡ ä¸ªåˆ†ç±»æ¨¡å‹å³å¯ï¼Œç„¶åå†ä¸€ä¸€åˆ¤æ–­åœ¨è¯¥ç±»åˆ«ä¸­ï¼Œå…¶å±äºé‚£ä¸ªå­ç±»åˆ«ï¼Œä½†æ˜¯è¿™æ ·åšæœªå…å¤ªæš´åŠ›äº†ï¼Œè€Œå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå…¶å®æ˜¯å¯ä»¥**åªç”¨ä¸€ä¸ªæ¨¡å‹**æ¥è§£å†³çš„ã€‚
åˆ©ç”¨BERTæ¨¡å‹è§£å†³å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜æ—¶ï¼Œå…¶è¾“å…¥ä¸æ™®é€šå•æ ‡ç­¾åˆ†ç±»é—®é¢˜ä¸€è‡´ï¼Œå¾—åˆ°å…¶embeddingè¡¨ç¤ºä¹‹å(ä¹Ÿå°±æ˜¯BERTè¾“å‡ºå±‚çš„embedding)ï¼Œæœ‰å‡ ä¸ªlabelå°±è¿æ¥åˆ°å‡ ä¸ªå…¨è¿æ¥å±‚(ä¹Ÿå¯ä»¥ç§°ä¸ºprojection layer)ï¼Œç„¶åå†åˆ†åˆ«æ¥ä¸Šsoftmaxåˆ†ç±»å±‚ï¼Œè¿™æ ·çš„è¯ä¼šå¾—åˆ°â€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=loss_1%2C%5C+loss_2%2C%5C+%5Ccdots%2C%5C+loss_n)  ï¼Œæœ€åå†å°†æ‰€æœ‰çš„lossç›¸åŠ èµ·æ¥å³å¯ã€‚è¿™ç§åšæ³•å°±ç›¸å½“äºå°†nä¸ªåˆ†ç±»æ¨¡å‹çš„ç‰¹å¾æå–å±‚å‚æ•°å…±äº«ï¼Œå¾—åˆ°ä¸€ä¸ªå…±äº«çš„è¡¨ç¤º(å…¶ç»´åº¦å¯ä»¥è§†ä»»åŠ¡è€Œå®šï¼Œç”±äºæ˜¯å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤å…¶ç»´åº¦å¯ä»¥é€‚å½“å¢å¤§ä¸€äº›)ï¼Œæœ€åå†åšå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ã€‚~~

### é™å®šä¸Šä¸‹æ–‡é—®ç­” SQuAD
åœ¨è¾“å…¥
- é¢„æµ‹answer span(start pos, end pos)
- è®­ç»ƒæ•°æ®
	- question
	- reference
	- answer_start_pos
	- answer_end_pos
- è®­ç»ƒæµç¨‹
	- question + [SEP] + reference
	- 
Use classification head for each token
can deal with looooong senquenceï¼Ÿï¼ˆ>512ï¼‰: 
[https://github.com/google-research/bert/issues/66](https://github.com/google-research/bert/issues/66)
how to get the context vector?

- **æ–‡æœ¬ç”Ÿæˆï¼ŸNO!**
remember BERT does not include decoder?
- Bert use transformer as encoder, there is no decoder in BERT
- 

## æ€»ç»“

BERTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨Transformeræ¥è¿›è¡Œæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡çš„è¯­ä¹‰åˆ†æï¼Œä½†æ˜¯Transformeræ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼Œå®ƒä¸€æ–¹é¢æä¾›äº†å¼ºå¤§æ·±åº¦åŒå‘å¤„ç†èƒ½åŠ›ï¼Œè€Œä¸€æ–¹é¢ä¹Ÿä½¿ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹LMè®­ç»ƒæ–¹æ³•æ”¶åˆ°äº†å½±å“ã€‚  ç”±äºæ·±åº¦åŒå‘ä¼šå¯¼è‡´ã€‚ã€‚è€Œæ— æ³•ä½¿ç”¨LMè¿›è¡Œè®­ç»ƒï¼Œä½œè€…åˆ©ç”¨äº†MLMå¹¶è®¾è®¡äº†ç›¸åº”çš„é¢„å¤„ç†æ¥è§£å†³é¢„è®­ç»ƒå’Œå¾®è°ƒè®­ç»ƒçš„å†²çªã€‚ã€‚ã€‚
BERTç»™æˆ‘ä»¬çš„å¯ç¤ºæ˜¯






### å¾®è°ƒæŠ€å·§
1. è°ƒæ•´å‚æ•°ï¼ˆå†…å­˜ï¼‰ï¼Œæ¨¡å‹é€‰æ‹©
2.  **é•¿æ–‡æœ¬å¤„ç†**
[https://zhuanlan.zhihu.com/p/109143667](https://zhuanlan.zhihu.com/p/109143667)
	å¯¹äºé•¿æ–‡æœ¬æ–‡ä¸­åšäº†ä¸¤ç§å¤„ç†æ–¹å¼ï¼Œæˆªæ–­å’Œåˆ‡åˆ†ã€‚
	-   æˆªæ–­ï¼šä¸€èˆ¬æ¥è¯´æ–‡æœ¬ä¸­æœ€é‡è¦çš„ä¿¡æ¯æ˜¯å¼€å§‹å’Œç»“å°¾ï¼Œå› æ­¤æ–‡ä¸­å¯¹äºé•¿æ–‡æœ¬åšäº†æˆªæ–­å¤„ç†ã€‚
	> head-onlyï¼šä¿ç•™å‰510ä¸ªå­—ç¬¦  
	> tail-onlyï¼šä¿ç•™å510ä¸ªå­—ç¬¦  
	> head+tailï¼šä¿ç•™å‰128ä¸ªå’Œå382ä¸ªå­—ç¬¦
	
	- åˆ‡åˆ†: å°†æ–‡æœ¬åˆ†æˆkæ®µï¼Œæ¯æ®µçš„è¾“å…¥å’ŒBertå¸¸è§„è¾“å…¥ç›¸åŒï¼Œç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯[CLS]è¡¨ç¤ºè¿™æ®µçš„åŠ æƒä¿¡æ¯ã€‚æ–‡ä¸­ä½¿ç”¨äº†Max-pooling, Average poolingå’Œself-attentionç»“åˆè¿™äº›ç‰‡æ®µçš„è¡¨ç¤ºã€‚
	- 
ä¸‹é¢æ˜¯å®éªŒçš„ç»“æœï¼Œhead+tailçš„è¡¨ç¤ºåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ•ˆæœéƒ½æ¯”è¾ƒå¥½ã€‚åº”è¯¥æ˜¯é•¿æ–‡æœ¬ç»“åˆäº†å¥é¦–å’Œå¥å°¾çš„ä¿¡æ¯ï¼Œè·å–çš„ä¿¡æ¯æ¯”è¾ƒå‡è¡¡ã€‚ä¸è¿‡å¥‡æ€ªçš„æ˜¯æ‹¼æ¥çš„æ–¹å¼æ•´ä½“å±…ç„¶ä¸å¦‚æˆªæ–­ï¼Œä¸ªäººçŒœæµ‹å¯èƒ½æ˜¯å°†å¥å­åˆ‡æˆå‡ æ®µä¹‹åå¢åŠ äº†æ¨¡å‹çš„ä¸ç¨³å®šæ€§ï¼Œè€Œé”™è¯¯å åŠ èµ·æ¥å¯èƒ½å°±ä¼šè¢«æ”¾å¤§ã€‚è€Œmax-poolingå’Œself-attentionä¹Ÿæ›´åŠ å¼ºè°ƒäº†æ–‡æœ¬ä¸­æ¯”è¾ƒæœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æ•´ä½“æ•ˆæœä¼˜äºaverage.
![enter image description here](https://pic3.zhimg.com/80/v2-f932b2ed7aa4af745b512e2e0f43093e_720w.jpg)

## BERTçš„æ”¹è¿›
[å…³äºBERTçš„è‹¥å¹²é—®é¢˜æ•´ç†è®°å½•](https://zhuanlan.zhihu.com/p/95594311)
### task design
- spanBERT [https://zhuanlan.zhihu.com/p/75893972](https://zhuanlan.zhihu.com/p/75893972)
### distillation
~~### LAMPï¼Ÿnot a BERT improvement~~
## BERTåº”ç”¨
### imageBert
### codeBert
## BERT in action
[https://github.com/ProHiryu/bert-chinese-ner](https://github.com/ProHiryu/bert-chinese-ner)
[https://github.com/chiahsuan156/ODSQA](https://github.com/chiahsuan156/ODSQA)
### environment-colab
 - User BERT base model
 - Tweak: batch size, max length
 - Mixed precision training
 - Gradient checkpoint
### huggingface transformer
### BERT as a service
### DistilBERT
### SQUAD













	
## Transfer learning
- what?
- why?
	- Deep model which has lots of parameters need lots of training data
	- labeled training data is very expensive
	- To save training efforts (save money and time)
	- Transfer learning has been successful in CV tasks
- how?
	- pretraining - generate embeddings (word embeddings)
	- supervised finetuning - train for downstream tasks
	
### sequential transfer learning
- pretraining
- Adaptation
- finetuning
	- supervised
	- unsupervised

## pretraining
- self-supervised learningè‡ªç›‘ç£å­¦ä¹  based on  Language Model
	- Many successful pretraining approaches are based on language modeling
	- Informally, a LM learns PÏ´(text) or PÏ´(text | some other text)
	- Doesnâ€™t require human annotation
	- Many languages have enough text to learn high capacity model
	- Versatileâ€”can learn both sentence and word representations with a variety of
objective functions
- from static embedding to dynamic embedding
	- static encoding (contextless embedding)- word2vec, glove
	- dynamic encoding (contextual embedding) - elmo, bert
- How LM help NLP transfer learning
	- feature based
	**Feature-based**æŒ‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¸­é—´ç»“æœä¹Ÿå°±æ˜¯LM embedding, å°†å…¶ä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼Œå¼•å…¥åˆ°åŸä»»åŠ¡çš„æ¨¡å‹ä¸­ã€‚é€šå¸¸feature-basedæ–¹æ³•åŒ…æ‹¬ä¸¤æ­¥ï¼š

		1.  é¦–å…ˆåœ¨å¤§çš„è¯­æ–™Aä¸Šæ— ç›‘ç£åœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒå®Œæ¯•å¾—åˆ°è¯­è¨€æ¨¡å‹
		2.  ç„¶åæ„é€ task-specific modelä¾‹å¦‚åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒtask-sepcific modelï¼Œå°†è¯­è¨€æ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œè¯­æ–™Bçš„è®­ç»ƒæ•°æ®ç»è¿‡è¯­è¨€æ¨¡å‹å¾—åˆ°LM embeddingï¼Œä½œä¸ºtask-specific modelçš„é¢å¤–ç‰¹å¾ã€‚ELMoæ˜¯è¿™æ–¹é¢çš„å…¸å‹å·¥ä½œï¼Œè¯·å‚è€ƒ[2]
	- fine-tuning
	Fine-tuningæ–¹å¼æ˜¯æŒ‡åœ¨å·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å°‘é‡çš„task-specific parameters, ä¾‹å¦‚å¯¹äºåˆ†ç±»é—®é¢˜åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸ŠåŠ ä¸€å±‚softmaxç½‘ç»œï¼Œç„¶ååœ¨æ–°çš„è¯­æ–™ä¸Šé‡æ–°è®­ç»ƒæ¥è¿›è¡Œfine-tuneã€‚ä¾‹å¦‚OpenAI GPT [3] ä¸­é‡‡ç”¨äº†è¿™æ ·çš„æ–¹æ³•ï¼Œæ¨¡å‹å¦‚ä¸‹æ‰€ç¤º

![](https://pic1.zhimg.com/80/v2-8f857288cf73acba9ddb6b3742265144_hd.jpg)

å›¾2 Transformer LM + fine-tuningæ¨¡å‹ç¤ºæ„å›¾

  
é¦–å…ˆè¯­è¨€æ¨¡å‹é‡‡ç”¨äº†Transformer Decoderçš„æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨æ–‡æœ¬é¢„æµ‹ä½œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œè®­ç»ƒå®Œæ¯•ä¹‹åï¼ŒåŠ ä¸€å±‚Linear Projectæ¥å®Œæˆåˆ†ç±»/ç›¸ä¼¼åº¦è®¡ç®—ç­‰NLPä»»åŠ¡ã€‚å› æ­¤æ€»ç»“æ¥è¯´ï¼ŒLM + Fine-Tuningçš„æ–¹æ³•å·¥ä½œåŒ…æ‹¬ä¸¤æ­¥ï¼š

1.  æ„é€ è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤§çš„è¯­æ–™Aæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹
2.  åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ å°‘é‡ç¥ç»ç½‘ç»œå±‚æ¥å®Œæˆspecific taskä¾‹å¦‚åºåˆ—æ ‡æ³¨ã€åˆ†ç±»ç­‰ï¼Œç„¶åé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­è¯­è¨€æ¨¡å‹çš„å‚æ•°å¹¶ä¸å›ºå®šï¼Œä¾ç„¶æ˜¯trainable variables.
- Encoding
	- character level
	- BPE
	- word level
- task design (training objective) for self-supervised learning
	- Language model
	- bidirectional LM
	- MLM, NSP
	- GAN (ELATRA)
- The pretrained model is too complex to use
	- distillation
	- 
## Adaptation
GPT-2è®ºè¯äº†ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Ÿå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸åŒé¢†åŸŸçš„æ–‡æœ¬ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„taskï¼Œè€Œå¦‚æœæŠŠè¿™äº›taskç»„åˆèµ·æ¥å­¦ä¹ ï¼Œé‚£ä¹ˆå°±æ˜¯multi-taskå­¦ä¹ ã€‚æ‰€ç‰¹æ®Šçš„æ˜¯è¿™äº›taskéƒ½æ˜¯åŒè´¨çš„ï¼Œå³å®ƒä»¬çš„ç›®æ ‡å‡½æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç»Ÿä¸€å­¦ä¹ ã€‚é‚£ä¹ˆå½“å¢å¤§æ•°æ®é›†åï¼Œç›¸å½“äºæ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸä¸Šè¿›è¡Œäº†å­¦ä¹ ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰äº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚
### GPT-2 ç›´æ¥åšä¸‹æ¸¸ä»»åŠ¡

é™¤äº†è¯­è¨€æ¨¡å‹ä¸Šçš„è¿›å±•ä¹‹å¤–ï¼ŒGPT-2è¿˜é¦–æ¬¡å°è¯•äº†ç›´æ¥ç”¨è¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸ç”¨åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„æŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿ

æ¯”å¦‚ï¼Œå¦‚æœæ˜¯summarizationä»»åŠ¡ï¼Œé‚£ä¹ˆå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆ‘åŠ ä¸€ä¸ªæ–°è¯TL;DR:, æ”¹è¯å‰é¢æ˜¯contextï¼Œåé¢æ˜¯æ‘˜è¦ã€‚é‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°è¿™ä¸ªè¯åï¼Œå°±èƒ½æ¨æ–­å‡ºæ¥ï¼Œæ¥ä¸‹æ¥è¦åšæŠ½æ‘˜è¦çš„å·¥ä½œäº†ã€‚

åŒç†ï¼Œå¯¹äºtranslateä»»åŠ¡ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åšæˆ french sentence = english sentenceï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°=çš„æ—¶å€™ï¼Œåº”è¯¥èƒ½æ¨æ–­å‡ºæ¥ä¸‹æ¥æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚

è™½ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒGPT-2éƒ½æ²¡æœ‰è¾¾åˆ°SOTAçš„æ•ˆæœï¼Œä½†æ˜¯æ•ˆæœä¹Ÿæ˜¯ç›¸å½“å¯è§‚çš„ã€‚è¡¨æ˜äº†é«˜å®¹é‡æ¨¡å‹åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å¯èƒ½æ€§ã€‚

## Downstream fine-tuning
- finetuning tips - ULMFit
- tools: TF-hub

### Example

[Generalized language model](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[How to build openai's GPT2](https://blog.floydhub.com/gpt2/)
[BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[NLPé¢„è®­ç»ƒæ¼”è¿› - from Word2Vec to XLNet](https://zhuanlan.zhihu.com/p/93343298)
[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼š](https://zhuanlan.zhihu.com/p/56382372)
[å²ä¸Šæœ€å…¨è¯å‘é‡è®²è§£](https://zhuanlan.zhihu.com/p/75391062)
[ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡](https://zhuanlan.zhihu.com/p/89763176)
[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
[å¦‚ä½•è¯„ä»·BERT-å›ç­”](https://www.zhihu.com/question/298203515/answer/516170825)
[NLPè§„åˆ™æ”¹å†™](https://zhuanlan.zhihu.com/p/47488095)
[BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)
[BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
[Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[ELMO](https://petrlorenc.github.io/ELMO/)
[Character embedding CNN](https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)
[The Illustrated BERT EMLO and co.](http://jalammar.github.io/illustrated-bert/)
[Transfer learning in NLP](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_177_4)
[VisualGuideToUsingBERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
[An In-Depth Tutorial to AllenNLP](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
[Transfer learning using elmo embedding](https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c)
[State of transfer learing in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)
[Generalized language model: ULMfit&openai GPT](https://www.topbots.com/generalized-language-models-ulmfit-openai-gpt/)
[Bertæ¨¡å‹åŠfine-tuning](https://zhuanlan.zhihu.com/p/46833276)
[Openai GPT2 è¯¦è§£](https://zhuanlan.zhihu.com/p/57251615)
[How to make custom AI-generated text with GPT2](https://minimaxir.com/2019/09/howto-gpt2/)
[GPT2: Understand language generation through visualization](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)
[GPTä¸ºä»€ä¹ˆä¸èƒ½åŒå‘ï¼Ÿ](https://www.zhihu.com/question/322034410/answer/794201004)
[ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
[ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Unsupervised NER with BERT](https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT)
[BERT explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[Zero shot GPT2](https://rakeshchada.github.io/Zero-Shot-GPT-2.html)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Understanding BERT Part 2: BERT Specifics](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
[Google BERT â€” Pre Training and Fine Tuning for NLP Tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[why BERT has 3 embedding layers?](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
[from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
[google BERT - pretraining and finetuing for NLP tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[NLP: Explaining Neural language model](https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/#.XniDIWgzZPY)
[Bertå¾®è°ƒæŠ€å·§å®éªŒå¤§å…¨](https://zhuanlan.zhihu.com/p/109143667)
[BERT finetuneçš„è‰ºæœ¯](https://zhuanlan.zhihu.com/p/62642374)
[Bertåœ¨NLPå„é¢†åŸŸçš„åº”ç”¨è¿›å±•](https://zhuanlan.zhihu.com/p/68446772)
[GPT2 finetune @familiarcycle.net/](https://familiarcycle.net/)
[paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained](https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
[Understanding BERT part2](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
[BERTæºç åˆ†æ](https://blog.csdn.net/weixin_37947156/article/details/94885499)
[BERT author explain BERT](https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/)
[Examining BERT's raw embeddings](https://towardsdatascience.com/examining-berts-raw-embeddings-fd905cb22df7)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTUxODkxMjEwNywtMjAzMzc1OTgyMCwtMT
I3MTgxNjY4Myw3OTM1NDI1MzcsODUwMTEwMTk0LC0xNTA3MTI4
MjMyLC0zNzc0Njg3NjAsMTM5ODEzNzA2MSwyMDE4MjY4NDA3LD
ExNzQwMDQ5OTMsMTk1NTE1MjM1NCwtMTMwMzAzNjUzLDk1NTIx
MzM3LC0xMDg3MTIxMzUxLC0zODgwMzEyMTEsLTc0NjgwNjMsLT
IwNjg3MTM3NDQsLTU3MTMyODMwNiwtMjE1OTE4OTkwLC0xOTc0
MjY1NTI1XX0=
-->