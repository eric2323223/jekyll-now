# NLP transfer learning 

# NLPçš„è¿ç§»å­¦ä¹ -BERTç¯‡

# Training objective of self-supervised learning - From Word2Vec to Elmo to Bert to XLNet

self-supervised learning is important area because it can greatly reduce the effort of training deep model, 

ä½œä¸ºNLPè¿ç§»å­¦ä¹ çš„æˆåŠŸåº”ç”¨ï¼ŒBERTè¯æ˜äº†ã€‚ã€‚ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTæ¨¡å‹çš„ç»“æ„å’Œè®¾è®¡åŸç†ï¼Œä»¥åŠBERTçš„åº”ç”¨ã€‚
## è¿ç§»å­¦ä¹ -é¢„è®­ç»ƒæ¨¡å‹çš„è¯ç”Ÿ
![enter image description here](https://miro.medium.com/max/3283/1*Z11P-CjNYWBofEbmGQrptA.png)
è¿ç§»å­¦ä¹ æ—¨åœ¨é€šè¿‡é‡ç”¨ ã€‚ã€‚ã€‚æ¥åŠ é€Ÿå­¦ä¹ å’Œå¢å¼ºé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¯¹äºå½“ä»Šè¶Šæ¥è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œéœ€è¦å·¨å¤§çš„äººåŠ›ç‰©åŠ›å’Œæ—¶é—´æˆæœ¬ã€‚ã€‚ã€‚ä½¿ç”¨è¿ç§»å­¦ä¹ æ˜¯éå¸¸æœ‰æ„ä¹‰çš„ã€‚é€šè¿‡å†imagenetè®­ç»ƒè§†è§‰ç‰¹å¾æå–ç½‘ç»œï¼Œæ•°æ®æ¯”è¾ƒä»å¤´è®­ç»ƒå’Œä½¿ç”¨è¿ç§»è®­ç»ƒã€‚ã€‚ã€‚
ç°å®çš„é—®é¢˜æ˜¯è·å–è¶³å¤Ÿçš„æ ‡è®°æ•°æ®éå¸¸å›°éš¾ï¼Œå› æ­¤
### NLPçš„è¿ç§»å­¦ä¹ 
æˆ‘ä»¬çŸ¥é“åœ¨CVä¸­çš„è¿ç§»å­¦ä¹ è¿‡ç¨‹æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„çš„å›¾åƒç‰¹å¾æå–æ¨¡å‹ï¼ˆå¦‚VGG19ï¼Œ ResNet50ç­‰ï¼‰ï¼Œå†ç»“åˆä¸‹æ¸¸ä»»åŠ¡éœ€è¦é€šè¿‡æ‰©å±•ç¬¬ä¸€é˜¶æ®µçš„æ¨¡å‹æ¥è¿›è¡Œfine tuningã€‚è¿›è¡Œä¸CVä»»åŠ¡ç±»ä¼¼ï¼Œåº”ç”¨è¿ç§»å­¦ä¹ è§£å†³NLPé—®é¢˜ä¹Ÿå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å‡ºå¯é‡ç”¨çš„ç‰¹å¾æå–æ¨¡å‹ï¼Œä¹Ÿå«é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºNLPä¸»è¦å…³æ³¨è¯­è¨€ï¼ˆå­—ç¬¦åºåˆ—ï¼‰çš„ç†è§£å’Œå¤„ç†ï¼Œä½œä¸ºè¯­è¨€åŸºæœ¬ç»„æˆå•ä½çš„è¯ï¼ˆwordï¼‰ä¹Ÿå°±è‡ªç„¶æˆä¸ºäº†é¢„è®­ç»ƒçš„å…³æ³¨ç‚¹ã€‚é¢„è®­ç»ƒçš„ç›®æ ‡ç»å†é€æ­¥çš„å‘å±•å˜åŒ–
#### é¢„è®­ç»ƒ
- output: embeddings
	- é™æ€è¯ç¼–ç ï¼ˆstatic word embeddingï¼‰ï¼Œæ¯”Word2Vecï¼ŒGloveç­‰ï¼Œé¡¾åæ€ä¹‰è¿™ç±»ç¼–ç èµ‹äºˆæ¯ä¸ªè¯å›ºå®šçš„ç¼–ç å€¼ï¼Œå¹¶ä¸”ç¼–ç å€¼ä½“ç°äº†è¯çš„ä»£è¡¨çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ç¼–ç å€¼çš„è¿ç®—å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœï¼Œæ¯”å¦‚è‘—åçš„ä¾‹å­
***king â€” man + woman = queen***
![enter image description here](https://miro.medium.com/max/634/1*dm9dudL37B6JG8saeR3zIw.png)

	- è¯­å¢ƒè¯ç¼–ç ï¼ˆcontextualized word embeddingï¼‰ï¼Œé™æ€è¯ç¼–ç çš„æœ€å¤§çš„é—®é¢˜åœ¨äºå®ƒåªèƒ½ä¸ªæ¯ä¸€ä¸ªè¯ä¸€ä¸ªç¼–ç å€¼ï¼Œæ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰çš„æƒ…å†µã€‚å°†â€œæˆ‘çˆ±åƒè‹¹æœâ€å’Œâ€œæˆ‘çˆ±è‹¹æœæ‰‹æœºâ€ä¸­çš„è‹¹æœèµ‹äºˆç›¸åŒçš„ç¼–ç æ˜¯ä¸åˆé€‚çš„ï¼Œæ›´åˆç†çš„æ–¹å¼æ˜¯é€šè¿‡ç»“åˆè¯å‡ºç°çš„ä¸Šä¸‹æ–‡åˆ¤æ–­è¯çš„å«ä¹‰ï¼Œæ¯”å¦‚é€šè¿‡â€œåƒâ€å’Œâ€œæ‰‹æœºâ€æ¥åˆ¤æ–­ä¸Šé¢ä¸¤å¥è¯ä¸­çš„â€œè‹¹æœâ€åˆ†åˆ«ä»£è¡¨ä¸€ç§æ°´æœå’Œä¸€ä¸ªå“ç‰Œï¼Œè¿™å°±æ˜¯è¯­å¢ƒè¯ç¼–ç çš„åŸºæœ¬æ€æƒ³ã€‚æ‰€ä»¥ä»ä½¿ç”¨è€…è§’åº¦æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹èƒ½è¿‡é€šè¿‡è¾“å…¥è¯­å¥å¾—åˆ°ï¼ˆè®¡ç®—å‡ºï¼‰è¯¥è¯­å¥çš„å«ä¹‰ï¼Œæˆ–è€…è¯¥è¯­å¥ä¸­æ¯ä¸ªè¯çš„å«ä¹‰ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šéœ€è¦çš„æ˜¯ä¸€ç§èƒ½å¤Ÿæå–è¯­ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™å’ŒCVä¸­çš„è¿ç§»å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚
		
- self-supervised learning
	- Language model based 
	- å•å‘ - åŒå‘
	
#### fine tune
- supervised
- unsupervised
- 
## BERTç®€ä»‹
BERTæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒå¯ä»¥æå–è¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œ


- bidirectional
- context dependent embedding
- 

## BERTæ¨¡å‹ç»“æ„
### Transformer encoder based
![enter image description here](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS6Tpws-svHjCwryVCZcEAxIWZ9LjTrg46pSmBG-mi2DMVwDamd)
è¾“å‡ºæ˜¯

### embedding layer
[https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
![enter image description here](https://i.stack.imgur.com/QCcYF.png)


## BERTçš„é¢„è®­ç»ƒ
### ä»»åŠ¡è®¾è®¡
- MLM
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows â€” 80% are replaced with a â€œ[MASK]â€ token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight):

-   If we used [MASK] 100% of the time the model wouldnâ€™t necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.
-   If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is  _never_  correct.
-   If we used [MASK] 90% of the time and kept the same word 10% of the time, then the model could just trivially copy the non-contextual embedding.

No ablation was done on the ratios of this approach, and it may have worked better with different ratios. In addition, the model performance wasnâ€™t tested with simply masking 100% of the selected tokens.
- NSP
### add special tokens to input
- [CLS] ç”¨äºåˆ†ç±»ä»»åŠ¡
- [SEP] ç”¨äºåˆ†å‰²
- 

### optimizer
[https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866](https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866)
- size matters
- 

## BERTçš„fine tune	
- how about [CLS] and [SEP]?
- 
- downstream tasks
	- sentence classification(sentiment classification)
	- token classification NER
	- SQUAD & unsupervised SQUAD

## BERTåº”ç”¨
### environment-colab
### DistilBERT


## æ€»ç»“













	
## Transfer learning
- what?
- why?
	- Deep model which has lots of parameters need lots of training data
	- labeled training data is very expensive
	- To save training efforts (save money and time)
	- Transfer learning has been successful in CV tasks
- how?
	- pretraining - generate embeddings (word embeddings)
	- supervised finetuning - train for downstream tasks
	
### sequential transfer learning
- pretraining
- Adaptation
- finetuning
	- supervised
	- unsupervised

## pretraining
- self-supervised learningè‡ªç›‘ç£å­¦ä¹  based on  Language Model
	- Many successful pretraining approaches are based on language modeling
	- Informally, a LM learns PÏ´(text) or PÏ´(text | some other text)
	- Doesnâ€™t require human annotation
	- Many languages have enough text to learn high capacity model
	- Versatileâ€”can learn both sentence and word representations with a variety of
objective functions
- from static embedding to dynamic embedding
	- static encoding (contextless embedding)- word2vec, glove
	- dynamic encoding (contextual embedding) - elmo, bert
- How LM help NLP transfer learning
	- feature based
	**Feature-based**æŒ‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¸­é—´ç»“æœä¹Ÿå°±æ˜¯LM embedding, å°†å…¶ä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼Œå¼•å…¥åˆ°åŸä»»åŠ¡çš„æ¨¡å‹ä¸­ã€‚é€šå¸¸feature-basedæ–¹æ³•åŒ…æ‹¬ä¸¤æ­¥ï¼š

		1.  é¦–å…ˆåœ¨å¤§çš„è¯­æ–™Aä¸Šæ— ç›‘ç£åœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒå®Œæ¯•å¾—åˆ°è¯­è¨€æ¨¡å‹
		2.  ç„¶åæ„é€ task-specific modelä¾‹å¦‚åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒtask-sepcific modelï¼Œå°†è¯­è¨€æ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œè¯­æ–™Bçš„è®­ç»ƒæ•°æ®ç»è¿‡è¯­è¨€æ¨¡å‹å¾—åˆ°LM embeddingï¼Œä½œä¸ºtask-specific modelçš„é¢å¤–ç‰¹å¾ã€‚ELMoæ˜¯è¿™æ–¹é¢çš„å…¸å‹å·¥ä½œï¼Œè¯·å‚è€ƒ[2]
	- fine-tuning
	Fine-tuningæ–¹å¼æ˜¯æŒ‡åœ¨å·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å°‘é‡çš„task-specific parameters, ä¾‹å¦‚å¯¹äºåˆ†ç±»é—®é¢˜åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸ŠåŠ ä¸€å±‚softmaxç½‘ç»œï¼Œç„¶ååœ¨æ–°çš„è¯­æ–™ä¸Šé‡æ–°è®­ç»ƒæ¥è¿›è¡Œfine-tuneã€‚ä¾‹å¦‚OpenAI GPT [3] ä¸­é‡‡ç”¨äº†è¿™æ ·çš„æ–¹æ³•ï¼Œæ¨¡å‹å¦‚ä¸‹æ‰€ç¤º

![](https://pic1.zhimg.com/80/v2-8f857288cf73acba9ddb6b3742265144_hd.jpg)

å›¾2 Transformer LM + fine-tuningæ¨¡å‹ç¤ºæ„å›¾

  
é¦–å…ˆè¯­è¨€æ¨¡å‹é‡‡ç”¨äº†Transformer Decoderçš„æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨æ–‡æœ¬é¢„æµ‹ä½œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œè®­ç»ƒå®Œæ¯•ä¹‹åï¼ŒåŠ ä¸€å±‚Linear Projectæ¥å®Œæˆåˆ†ç±»/ç›¸ä¼¼åº¦è®¡ç®—ç­‰NLPä»»åŠ¡ã€‚å› æ­¤æ€»ç»“æ¥è¯´ï¼ŒLM + Fine-Tuningçš„æ–¹æ³•å·¥ä½œåŒ…æ‹¬ä¸¤æ­¥ï¼š

1.  æ„é€ è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤§çš„è¯­æ–™Aæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹
2.  åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ å°‘é‡ç¥ç»ç½‘ç»œå±‚æ¥å®Œæˆspecific taskä¾‹å¦‚åºåˆ—æ ‡æ³¨ã€åˆ†ç±»ç­‰ï¼Œç„¶åé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­è¯­è¨€æ¨¡å‹çš„å‚æ•°å¹¶ä¸å›ºå®šï¼Œä¾ç„¶æ˜¯trainable variables.
- Encoding
	- character level
	- BPE
	- word level
- task design (training objective) for self-supervised learning
	- Language model
	- bidirectional LM
	- MLM, NSP
	- GAN (ELATRA)
- The pretrained model is too complex to use
	- distillation
	- 
## Adaptation
GPT-2è®ºè¯äº†ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Ÿå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸åŒé¢†åŸŸçš„æ–‡æœ¬ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„taskï¼Œè€Œå¦‚æœæŠŠè¿™äº›taskç»„åˆèµ·æ¥å­¦ä¹ ï¼Œé‚£ä¹ˆå°±æ˜¯multi-taskå­¦ä¹ ã€‚æ‰€ç‰¹æ®Šçš„æ˜¯è¿™äº›taskéƒ½æ˜¯åŒè´¨çš„ï¼Œå³å®ƒä»¬çš„ç›®æ ‡å‡½æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç»Ÿä¸€å­¦ä¹ ã€‚é‚£ä¹ˆå½“å¢å¤§æ•°æ®é›†åï¼Œç›¸å½“äºæ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸä¸Šè¿›è¡Œäº†å­¦ä¹ ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰äº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚
### GPT-2 ç›´æ¥åšä¸‹æ¸¸ä»»åŠ¡

é™¤äº†è¯­è¨€æ¨¡å‹ä¸Šçš„è¿›å±•ä¹‹å¤–ï¼ŒGPT-2è¿˜é¦–æ¬¡å°è¯•äº†ç›´æ¥ç”¨è¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸ç”¨åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„æŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿ

æ¯”å¦‚ï¼Œå¦‚æœæ˜¯summarizationä»»åŠ¡ï¼Œé‚£ä¹ˆå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆ‘åŠ ä¸€ä¸ªæ–°è¯TL;DR:, æ”¹è¯å‰é¢æ˜¯contextï¼Œåé¢æ˜¯æ‘˜è¦ã€‚é‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°è¿™ä¸ªè¯åï¼Œå°±èƒ½æ¨æ–­å‡ºæ¥ï¼Œæ¥ä¸‹æ¥è¦åšæŠ½æ‘˜è¦çš„å·¥ä½œäº†ã€‚

åŒç†ï¼Œå¯¹äºtranslateä»»åŠ¡ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åšæˆ french sentence = english sentenceï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°=çš„æ—¶å€™ï¼Œåº”è¯¥èƒ½æ¨æ–­å‡ºæ¥ä¸‹æ¥æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚

è™½ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒGPT-2éƒ½æ²¡æœ‰è¾¾åˆ°SOTAçš„æ•ˆæœï¼Œä½†æ˜¯æ•ˆæœä¹Ÿæ˜¯ç›¸å½“å¯è§‚çš„ã€‚è¡¨æ˜äº†é«˜å®¹é‡æ¨¡å‹åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å¯èƒ½æ€§ã€‚

## Downstream fine-tuning
- finetuning tips - ULMFit
- tools: TF-hub

### Example

[Generalized language model](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[How to build openai's GPT2](https://blog.floydhub.com/gpt2/)
[BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[NLPé¢„è®­ç»ƒæ¼”è¿› - from Word2Vec to XLNet](https://zhuanlan.zhihu.com/p/93343298)
[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼š](https://zhuanlan.zhihu.com/p/56382372)
[å²ä¸Šæœ€å…¨è¯å‘é‡è®²è§£](https://zhuanlan.zhihu.com/p/75391062)
[ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡](https://zhuanlan.zhihu.com/p/89763176)
[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
[å¦‚ä½•è¯„ä»·BERT-å›ç­”](https://www.zhihu.com/question/298203515/answer/516170825)
[NLPè§„åˆ™æ”¹å†™](https://zhuanlan.zhihu.com/p/47488095)
[BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)
[BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
[Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[ELMO](https://petrlorenc.github.io/ELMO/)
[Character embedding CNN](https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)
[The Illustrated BERT EMLO and co.](http://jalammar.github.io/illustrated-bert/)
[Transfer learning in NLP](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_177_4)
[VisualGuideToUsingBERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
[An In-Depth Tutorial to AllenNLP](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
[Transfer learning using elmo embedding](https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c)
[State of transfer learing in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)
[Generalized language model: ULMfit&openai GPT](https://www.topbots.com/generalized-language-models-ulmfit-openai-gpt/)
[Bertæ¨¡å‹åŠfine-tuning](https://zhuanlan.zhihu.com/p/46833276)
[Openai GPT2 è¯¦è§£](https://zhuanlan.zhihu.com/p/57251615)
[How to make custom AI-generated text with GPT2](https://minimaxir.com/2019/09/howto-gpt2/)
[GPT2: Understand language generation through visualization](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)
[GPTä¸ºä»€ä¹ˆä¸èƒ½åŒå‘ï¼Ÿ](https://www.zhihu.com/question/322034410/answer/794201004)
[ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
[ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Unsupervised NER with BERT](https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT)
[BERT explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[Zero shot GPT2](https://rakeshchada.github.io/Zero-Shot-GPT-2.html)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Understanding BERT Part 2: BERT Specifics](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
[Google BERT â€” Pre Training and Fine Tuning for NLP Tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[why BERT has 3 embedding layers?](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
[from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
[google BERT - pretraining and finetuing for NLP tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTcyMzE0MzY3NSwxNDY0ODE3OTIsNDQ1Mz
AzODU5LDY1NTk4NjU3MCwtMjAxOTQ4ODIyNywxMTY4MTU3ODc3
LC00OTQyODEwOTgsMzUxMjg0MzIsLTYxNDE5NzcyMSwtMTkyMj
Q2MTIxLDE5NTU4NjMwNzksLTQ3Njg3MjI0NSwxMDg0NjY3ODA1
LC02Mzg0NDQ4NjIsLTc1MzU1OTI3Miw2MDMyMzY2NDIsLTgzOT
czMjU2MywxNDU4MjAxMjEyLDExMzM2MTEyMjksNzQ3NDQ3ODMy
XX0=
-->