# NLPçš„è¿ç§»å­¦ä¹ -BERTç¯‡
è¿ç§»å­¦ä¹ è®©æ™®é€šäººåº”ç”¨å¤æ‚å¼ºå¤§æ¨¡å‹è§£å†³å®é™…é—®é¢˜çš„æ·å¾„ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§èƒ½åŠ›ï¼ŒBERTåœ¨NLPé¢†åŸŸçš„ä¸€ç³»åˆ—ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°é«˜ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTçš„ç»“æ„ï¼Œç‰¹æ€§ï¼Œé¢„è®­ç»ƒæ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œå¹¶è¯•å›¾è§£é‡ŠBERTæ¨¡å‹è®¾è®¡èƒŒåçš„åŸå› ã€‚æœ€åå›å½’åº”ç”¨ï¼Œä»‹ç»äº†å¦‚ä½•åˆ©ç”¨BERTé¢„è®­ç»ƒæ¨¡å‹åœ¨colabå¹³å°å¿«é€Ÿå®ç°æ™ºèƒ½é—®ç­”ã€‚
1. è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
    1.1 NLPçš„è¿ç§»å­¦ä¹ 
    1.2 è¯­è¨€æ¨¡å‹
2. BERTç®€ä»‹
3. BERTæ¨¡å‹ç»“æ„
    3.1 ç¼–ç å±‚
    3.2 Transformerç¼–ç å™¨
4. BERTçš„é¢„è®­ç»ƒ
    4.1 ä»»åŠ¡è®¾è®¡
    4.2 é¢„è®­ç»ƒæµç¨‹
    4.3 ä¼˜åŒ–
5. BERTçš„å¾®è°ƒ
    5.1 æƒ…ç»ªåˆ†æä»»åŠ¡
    5.2 åç§°å®ä½“è¯†åˆ«NERä»»åŠ¡
    5.3 é€šç”¨è¯­è¨€ç†è§£GLUEä»»åŠ¡
    5.4 é—®ç­”SQuADä»»åŠ¡
6. BERTçš„åº”ç”¨-å®ç°ä¸€ä¸ªæ™ºèƒ½é—®ç­”æœºå™¨äºº
    6.1 ç¯å¢ƒæ­å»º
    6.2 å®éªŒæµç¨‹
7. æ€»ç»“


self-supervised learning is important area because it can greatly reduce the effort of training deep model, 

ä½œä¸ºNLPè¿ç§»å­¦ä¹ çš„æˆåŠŸåº”ç”¨ï¼ŒBERTè¯æ˜äº†ã€‚ã€‚ã€‚æœ¬æ–‡æ—¨åœ¨ä»‹ç»BERTæ¨¡å‹çš„ç»“æ„å’Œè®¾è®¡åŸç†ï¼Œä»¥åŠBERTçš„åº”ç”¨ã€‚
## è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
![enter image description here](https://miro.medium.com/max/3283/1*Z11P-CjNYWBofEbmGQrptA.png)
è¿ç§»å­¦ä¹ æ—¨åœ¨é€šè¿‡é‡ç”¨ ã€‚ã€‚ã€‚æ¥åŠ é€Ÿå­¦ä¹ å’Œå¢å¼ºé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¯¹äºå½“ä»Šè¶Šæ¥è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œéœ€è¦å·¨å¤§çš„äººåŠ›ç‰©åŠ›å’Œæ—¶é—´æˆæœ¬ã€‚ã€‚ã€‚ä½¿ç”¨è¿ç§»å­¦ä¹ æ˜¯éå¸¸æœ‰æ„ä¹‰çš„ã€‚é€šè¿‡å†imagenetè®­ç»ƒè§†è§‰ç‰¹å¾æå–ç½‘ç»œï¼Œæ•°æ®æ¯”è¾ƒä»å¤´è®­ç»ƒå’Œä½¿ç”¨è¿ç§»è®­ç»ƒã€‚ã€‚ã€‚
ç°å®çš„é—®é¢˜æ˜¯è·å–è¶³å¤Ÿçš„æ ‡è®°æ•°æ®éå¸¸å›°éš¾ï¼Œå› æ­¤
### NLPçš„è¿ç§»å­¦ä¹ 
æˆ‘ä»¬çŸ¥é“åœ¨CVä¸­çš„è¿ç§»å­¦ä¹ è¿‡ç¨‹æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé€šç”¨çš„çš„å›¾åƒç‰¹å¾æå–æ¨¡å‹ï¼ˆå¦‚VGG19ï¼Œ ResNet50ç­‰ï¼‰ï¼Œå†ç»“åˆä¸‹æ¸¸ä»»åŠ¡éœ€è¦é€šè¿‡æ‰©å±•ç¬¬ä¸€é˜¶æ®µçš„æ¨¡å‹æ¥è¿›è¡Œfine tuningã€‚è¿›è¡Œä¸CVä»»åŠ¡ç±»ä¼¼ï¼Œåº”ç”¨è¿ç§»å­¦ä¹ è§£å†³NLPé—®é¢˜ä¹Ÿå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å‡ºå¯é‡ç”¨çš„ç‰¹å¾æå–æ¨¡å‹ï¼Œä¹Ÿå«é¢„è®­ç»ƒæ¨¡å‹ã€‚
> NLPçš„æœ€å¤§æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹è¶³å¤Ÿçš„åŸ¹è®­æ•°æ®ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ‰å¤§é‡æ–‡æœ¬æ•°æ®å¯ç”¨ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬è¦åˆ›å»ºç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†ï¼Œåˆ™éœ€è¦å°†è¯¥å †åˆ’åˆ†ä¸ºå¾ˆå¤šä¸åŒçš„å­—æ®µã€‚è€Œå½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆä»…å¾—åˆ°æ•°åƒæˆ–æ•°åä¸‡ä¸ªäººæ ‡è®°çš„åŸ¹è®­ç¤ºä¾‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¸ºäº†è¡¨ç°è‰¯å¥½ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„NLPæ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®-åœ¨æ•°ç™¾ä¸‡æˆ–æ•°åäº¿çš„å¸¦æ³¨é‡Šçš„è®­ç»ƒç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œä»–ä»¬çœ‹åˆ°äº†é‡å¤§æ”¹è¿›ã€‚ä¸ºäº†å¸®åŠ©å¼¥åˆæ•°æ®é¸¿æ²Ÿï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å„ç§æŠ€æœ¯ï¼Œå¯åœ¨ç½‘ç»œä¸Šä½¿ç”¨å¤§é‡æœªæ³¨é‡Šçš„æ–‡æœ¬æ¥è®­ç»ƒé€šç”¨è¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼ˆè¿™ç§°ä¸ºé¢„è®­ç»ƒï¼‰ã€‚ç„¶åï¼Œå¯ä»¥åœ¨è¾ƒå°çš„ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¤„ç†è¯¸å¦‚é—®é¢˜å›ç­”å’Œæƒ…æ„Ÿåˆ†æä¹‹ç±»çš„é—®é¢˜æ—¶ã€‚ä¸ä»å¤´å¼€å§‹å¯¹è¾ƒå°çš„ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œæ­¤æ–¹æ³•å¯æ˜¾ç€æé«˜å‡†ç¡®æ€§ã€‚

![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vStoAwye3EraSC6HH5m_S8VOsVEp3hsTtQuAVF-dEmPlFvEZqAxBHDQryl3FnVf_BZ6Csb969AGbChe/pub?w=791&h=385)

ç”±äºNLPä¸»è¦å…³æ³¨è¯­è¨€ï¼ˆå­—ç¬¦åºåˆ—ï¼‰çš„ç†è§£å’Œå¤„ç†ï¼Œä½œä¸ºè¯­è¨€åŸºæœ¬ç»„æˆå•ä½çš„è¯ï¼ˆwordï¼‰ä¹Ÿå°±è‡ªç„¶æˆä¸ºäº†é¢„è®­ç»ƒçš„å…³æ³¨ç‚¹ã€‚é¢„è®­ç»ƒçš„ç›®æ ‡ç»å†é€æ­¥çš„å‘å±•å˜åŒ–
#### é¢„è®­ç»ƒ pre training

- output: embeddings
	- é™æ€è¯ç¼–ç ï¼ˆstatic word embeddingï¼‰ï¼Œæ¯”Word2Vecï¼ŒGloveç­‰ï¼Œé¡¾åæ€ä¹‰è¿™ç±»ç¼–ç èµ‹äºˆæ¯ä¸ªè¯å›ºå®šçš„ç¼–ç å€¼ï¼Œå¹¶ä¸”ç¼–ç å€¼ä½“ç°äº†è¯çš„ä»£è¡¨çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹ç¼–ç å€¼çš„è¿ç®—å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœï¼Œæ¯”å¦‚è‘—åçš„ä¾‹å­
***king â€” man + woman = queen***
![enter image description here](https://miro.medium.com/max/634/1*dm9dudL37B6JG8saeR3zIw.png)

	- è¯­å¢ƒè¯ç¼–ç ï¼ˆcontextualized word embeddingï¼‰ï¼Œé™æ€è¯ç¼–ç çš„æœ€å¤§çš„é—®é¢˜åœ¨äºå®ƒåªèƒ½ä¸ªæ¯ä¸€ä¸ªè¯ä¸€ä¸ªç¼–ç å€¼ï¼Œæ— æ³•å¤„ç†ä¸€è¯å¤šä¹‰çš„æƒ…å†µã€‚å°†â€œæˆ‘çˆ±åƒè‹¹æœâ€å’Œâ€œæˆ‘çˆ±è‹¹æœæ‰‹æœºâ€ä¸­çš„è‹¹æœèµ‹äºˆç›¸åŒçš„ç¼–ç æ˜¯ä¸åˆé€‚çš„ï¼Œæ›´åˆç†çš„æ–¹å¼æ˜¯é€šè¿‡ç»“åˆè¯å‡ºç°çš„ä¸Šä¸‹æ–‡åˆ¤æ–­è¯çš„å«ä¹‰ï¼Œæ¯”å¦‚é€šè¿‡â€œåƒâ€å’Œâ€œæ‰‹æœºâ€æ¥åˆ¤æ–­ä¸Šé¢ä¸¤å¥è¯ä¸­çš„â€œè‹¹æœâ€åˆ†åˆ«ä»£è¡¨ä¸€ç§æ°´æœå’Œä¸€ä¸ªå“ç‰Œï¼Œè¿™å°±æ˜¯è¯­å¢ƒè¯ç¼–ç çš„åŸºæœ¬æ€æƒ³ã€‚æ‰€ä»¥ä»ä½¿ç”¨è€…è§’åº¦æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹èƒ½è¿‡é€šè¿‡è¾“å…¥è¯­å¥å¾—åˆ°ï¼ˆè®¡ç®—å‡ºï¼‰è¯¥è¯­å¥çš„å«ä¹‰ï¼Œæˆ–è€…è¯¥è¯­å¥ä¸­æ¯ä¸ªè¯çš„å«ä¹‰ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šéœ€è¦çš„æ˜¯ä¸€ç§èƒ½å¤Ÿæå–è¯­ä¹‰ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™å’ŒCVä¸­çš„è¿ç§»å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚
		
- self-supervised learning
Imagenetå°†è¶…è¿‡ä¸€åƒå››ç™¾ä¸‡å›¾ç‰‡é€šè¿‡ä¼—åŒ…çš„æ–¹å¼è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œå°†ä»–ä»¬åˆ†æˆ2ä¸‡å¤šä¸ªä¸åŒåˆ†ç±»ï¼Œè¿™é¡¹ä»2007å¹´å¼€å§‹çš„æµ©å¤§å·¥ç¨‹ä¸ºè®¡ç®—æœºè§†è§‰å›¾å½¢ç›¸å…³çš„ç›‘ç£å¼æœºå™¨å­¦ä¹ æä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œä¸ºCVè¿ç§»å­¦ä¹ æ‰“ä¸‹äº†åŸºç¡€ã€‚åŒæ ·çš„ä¸ºäº†NLPé¢†åŸŸä¹Ÿç”±ç±»ä¼¼çš„éœ€æ±‚ï¼šä¸ºæ¯ä¸ªè¯å»ºç«‹æ­£ç¡®çš„æ ‡ç­¾æ•°æ®æ¥å¸®åŠ©è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæ ¹æ®è¯­è¨€çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰è¿™ç§è®­ç»ƒä»»åŠ¡æ¥è¿›è¡Œã€‚ã€‚ã€‚LMå±äºè‡ªç›‘ç£ï¼ˆself supervisedï¼‰è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨è¿™ç§è®­ç»ƒæ–¹æ³•ä¸éœ€è¦ä¸ºè¯­å¥è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œè€Œåªä½¿ç”¨è¯­å¥åºåˆ—æœ¬èº«å°±å¯ä»¥è¿›è¡Œè®­ç»ƒã€‚LMæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªåºåˆ—$W$ï¼ˆç”±è¯$w_i, w_2, ... w_m$ç»„æˆçš„ä¸€å¥è¯ï¼‰å‡ºç°çš„æ¦‚ç‡$$P(W)=P(w_1,w_2,w_3,...w_m)$$LMä¹Ÿå¯ä»¥ç”¨äºè®¡ç®—åœ¨ä¸€ä¸ªåºåˆ—ä¸­æŸä¸ªè¯$w_{n+1}$å‡ºç°çš„æ¦‚ç‡$$P(w_{n+1}|w_1,w_2, w_3,...w_n)$$
æ ¹æ®è¿™æ ·ä¸€ä¸ªåŸºæœ¬å‡è®¾ï¼šæ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡æ¯”ä¸æ­£ç¡®çš„è¯­å¥å‡ºç°çš„æ¦‚ç‡å¤§
The good LM should calculate higher probabilities to â€œrealâ€ and â€œfrequently observedâ€ sentences than the ones that are wrong accordingly to natural language grammar or those that are rarely observed.
-   **Machine translation:**  translating a sentence saying about height it would probably state that  P(tall  man)>P(large  man)P(tall man)>P(large man)  as the â€˜_large_â€™ might also refer to weight or general appearance thus, not as probable as â€˜_tall_â€™
    
-   **Spelling Correction:**  Spell correcting sentence: â€œPut you name into formâ€, so that  P(name  into  form)>P(name  into  from)
ç”±æ­¤æˆ‘ä»¬é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ä½œä¸ºé¢„æµ‹å€¼$$\argmax P(w_n|w_1,w_2,w_3,...w_{n-1})$$
	- ä½¿ç”¨LMè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æŒ‰ç…§ä»å‰åˆ°åçš„é¡ºåºè¿›è¡Œé¢„æµ‹ï¼Œæ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­åä¸€ä¸ªè¯æ˜¯â€œâ€ï¼Œä¹Ÿå¯ä»¥æŒ‰ç…§ä»åå‘å‰çš„é¡ºåºï¼Œ$$\argmax P(w_i|w_n,w_{n-1},w_{n-2}, ...w_{i+1})$$æ¯”å¦‚é€šè¿‡â€œâ€åˆ¤æ–­å‰ä¸€ä¸ªè¯æ˜¯â€œâ€ã€‚
>  ä»Šå¤© å¤©æ°” ä¸é”™ï¼Œ æˆ‘ä»¬ å» å…¬å›­ ç© å§ã€‚

è¿™å¥è¯ï¼Œå•å‘è¯­è¨€æ¨¡å‹åœ¨å­¦ä¹ çš„æ—¶å€™æ˜¯ä»å·¦å‘å³è¿›è¡Œå­¦ä¹ çš„ï¼Œå…ˆç»™æ¨¡å‹çœ‹åˆ°â€œä»Šå¤© å¤©æ°”â€ä¸¤ä¸ªè¯ï¼Œç„¶åå‘Šè¯‰æ¨¡å‹ä¸‹ä¸€ä¸ªè¦å¡«çš„è¯æ˜¯â€œä¸é”™â€ã€‚ç„¶è€Œå•å‘è¯­è¨€æ¨¡å‹æœ‰ä¸€ä¸ªæ¬ ç¼ºï¼Œå°±æ˜¯æ¨¡å‹å­¦ä¹ çš„æ—¶å€™æ€»æ˜¯æŒ‰ç…§å¥å­çš„ä¸€ä¸ªæ–¹å‘å»å­¦çš„ï¼Œå› æ­¤æ¨¡å‹å­¦ä¹ æ¯ä¸ªè¯çš„æ—¶å€™åªçœ‹åˆ°äº†ä¸Šæ–‡ï¼Œå¹¶æ²¡æœ‰çœ‹åˆ°ä¸‹æ–‡ã€‚æ›´åŠ åˆç†çš„æ–¹å¼åº”è¯¥æ˜¯è®©æ¨¡å‹åŒæ—¶é€šè¿‡ä¸Šä¸‹æ–‡å»å­¦ä¹ ï¼Œè¿™ä¸ªè¿‡ç¨‹æœ‰ç‚¹ç±»ä¼¼äºå®Œå½¢å¡«ç©ºé¢˜ã€‚ä¾‹å¦‚ï¼š

>ä»Šå¤© å¤©æ°” { }ï¼Œ æˆ‘ä»¬ å» å…¬å›­ ç© å§ã€‚

é€šè¿‡è¿™æ ·çš„å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æŠŠæ¡â€œä¸é”™â€è¿™ä¸ªè¯æ‰€å‡ºç°çš„ä¸Šä¸‹æ–‡è¯­å¢ƒã€‚

#### å¾®è°ƒ fine tune
ç”±äºä½¿ç”¨æµ·é‡çš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œé¢„è®­ç»ƒæ¨¡å‹é€šå¸¸å…·æœ‰ä¸€èˆ¬çš„å¸¸è¯†ï¼Œç”±æ­¤ä½œä¸ºåŸºç¡€å†è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹èƒ½æ›´å¥½çš„é€‚åˆç‰¹å®šä»»åŠ¡ã€‚
- æ¨¡å‹è°ƒæ•´
é€šå¸¸åšæ³•æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šå¢åŠ ä»»åŠ¡ç›¸å…³çš„å±‚ï¼Œå¦‚ç”±å…¨è¿æ¥å±‚å’Œsoftmaxè¿ç®—æ„æˆçš„åˆ†ç±»å±‚ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚
- supervised learning
ä½¿ç”¨å°‘é‡ä»»åŠ¡ç›¸å…³çš„æ ‡è®°æ•°æ®æ¥è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸çš„åšæ³•æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åé¢ç›´æ¥åŠ ä¸Šä¸Šä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆç”±å…¨è¿æ¥å’Œsoftmaxè¿ç®—æ„æˆï¼‰ä½¿æ¨¡å‹è¾“å‡ºä¸€ä¸ªé¢„æµ‹ç±»å‹ï¼Œè®¡ç®—cross entropyè¯¯å·®ä»è€Œé€šè¿‡åå‘ä¼ é€’æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
	- æ›´æ–°å…¨éƒ¨æ¨¡å‹å‚æ•°
	- åªæ›´æ–°ä»»åŠ¡å±‚å‚æ•° - é¢„è®­ç»ƒæ¨¡å‹åªä½œä¸ºç‰¹å¾æå–å™¨


## ~~- unsupervised fine tuning? - clustering and measure class separation - classify result by compute distances to different classes -~~


- zero shot learning
~~æ— å¾®è°ƒé€‚ç”¨äºå®¹é‡æ›´å¤§é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™ç±»æ¨¡å‹ä¸€èˆ¬åŒ…å«äº†æ›´å¤šçš„å¸¸è¯†ï¼Œæ¯”å¦‚GPT2ä½¿ç”¨äº†xxçš„é«˜è´¨é‡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œæ— éœ€å¾®è°ƒä¹Ÿå¯èƒ½åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡é‡ç”Ÿæˆå¯æ¥å—çš„é¢„æµ‹ã€‚å¯¹äºè¿™ç±»æ¨¡å‹ï¼Œåªéœ€è¦ç»™å‡ºå°‘é‡çš„æ ·ä¾‹è®©æ¨¡å‹ç†è§£é¢„æµ‹æ„å›¾ã€‚ã€‚ã€‚~~

## BERTç®€ä»‹
BERTï¼ˆBidirectional Encoder Representations from Transformerï¼‰æ˜¯ä¸€ä¸ªç”¨äºæå–è¾“å…¥åºåˆ—ç‰¹å¾ä¿¡æ¯çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚When BERT was published it achieved [state-of-the-art] performance in 11 [natural language understanding] tasks:[[1]] [GLUE]task set (consisting of 8 tasks), [MultiNLI] [SQuAD] v1.1, SQuAD v2.0
2018, googleå‘è¡¨äº†è®ºæ–‡BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingï¼Œ 2019å¹´googleå°†BERTæ¨¡å‹åº”ç”¨åˆ°äº†æœç´¢æœåŠ¡ä¸­ï¼Œç°åœ¨å·²ç»æ”¯æŒäº†è¶…è¿‡70ç§è¯­è¨€

BERTæœ€å¤§çš„åˆ›æ–°æ˜¯å°†Transformeræ¨¡å‹åº”ç”¨åˆ°äº†è¯­è¨€æ¨¡å‹ä¸­ï¼Œã€‚ã€‚ã€‚ã€‚å½±å“å’Œå†³å®šäº†BERTå¾ˆå¤šç‰¹æ®Šæ€§è´¨ã€‚åœ¨BERTä¹‹å‰ï¼Œ

- context dependent embedding
BERTæ¨¡å‹ç”Ÿæˆçš„å…ƒç´ ç¼–ç å±äºåŠ¨æ€ç¼–ç ï¼Œå®ƒèƒ½æ ¹æ®è¾“å…¥åºåˆ—ç”Ÿæˆæ¯ä¸ªåºåˆ—å…ƒç´ ï¼ˆwordï¼‰åœ¨åºåˆ—ä¸Šä¸‹æ–‡ä¸­çš„ç‰¹å¾å‘é‡
- bidirectional Language Model
è¿™æ˜¯ç”±äºå®ƒæ˜¯ä»¥Attentionæœºåˆ¶ä¸ºåŸºç¡€ã€‚æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä¸€æ¬¡çœ‹åˆ°æ‰€æœ‰çš„åºåˆ—å…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ çš„ç¼–ç çš„è®¡ç®—éƒ½åŒ…å«äº†è¯¥å…ƒç´ ä¹‹å‰å’Œä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤BERTå±äºåŒå‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”ç”±äºèƒ½å¤ŸåŒæ—¶çœ‹åˆ°å‰å‘å’Œåå‘çš„ä¿¡æ¯ï¼ŒBERTä¸åŒäºä»¥å¾€çš„åŒå‘è¯­è¨€æ¨¡å‹ï¼Œå¦‚ELMOï¼Œã€‚ã€‚ã€‚ã€‚ã€‚ã€‚deep bidirectional 
å¹¶éæ‰€æœ‰çš„åŸºäºattentionæœºåˆ¶çš„æ¨¡å‹éƒ½æ˜¯åŒå‘è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚GPTä½¿ç”¨äº†é®ç½©çš„æ–¹å¼ä½¿æ¨¡å‹æ— æ³•çœ‹åˆ°å½“å‰å…ƒç´ ä¹‹åçš„åºåˆ—ä¿¡æ¯ï¼Œå› æ­¤å®ƒå±äºå•å‘è¯­è¨€æ¨¡å‹ã€‚

- 

## BERTæ¨¡å‹ç»“æ„
### Transformer encoder based
BERTæ¨¡å‹ä¸»è¦åŒ…å«è¿™ä¸ªéƒ¨åˆ†ï¼Œç¼–ç å±‚å’ŒTransformerç¼–ç å™¨
![enter image description here](https://www.lyrn.ai/wp-content/uploads/2018/11/transformer.png)

### ç¼–ç å±‚
ç¼–ç å±‚çš„ä½œç”¨æ˜¯
1. å°†è¾“å…¥è¯­å¥ï¼ˆBERT is powerfulï¼‰è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æµ®ç‚¹æ•°å‘é‡
2. åŠ å…¥ç‰¹æ®Šç¬¦å·[CLS][SEP] -- No! this is done in data preprocessing

    embeddings = inputs_embeds + position_embeddings + token_type_embeddings
    ä¸ºä»€ä¹ˆå¯ä»¥ç›¸åŠ ï¼Ÿ[https://www.zhihu.com/question/374835153/answer/1069173198](https://www.zhihu.com/question/374835153/answer/1069173198)

[https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
![enter image description here](https://i.stack.imgur.com/QCcYF.png)
- è¯ç¼–ç (config.vocab_size, config.hidden_size, padding_idx=0)
[https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding](https://www.topbots.com/generalized-language-models-bert-openai-gpt2/#input-embedding)
- æ®µç¼–ç (config.type_vocab_size, config.hidden_size)
åœ¨BERTå¤„ç†å¤šæ¡è¯­å¥æ—¶ï¼Œç”¨äºåŒºåˆ†ä¸åŒè¯­å¥
- ä½ç½®ç¼–ç (config.max_position_embeddings, config.hidden_size)
ç”±äºæ³¨æ„åŠ›è®¡ç®—ä¸å…³å¿ƒè¾“å…¥åºåˆ—å…ƒç´ çš„å…ˆåå¾ªåºï¼Œå› æ­¤éœ€è¦äº‹å…ˆåŠ å…¥ä½ç½®ä¿¡æ¯å†è¾“å…¥æ¨¡å‹ã€‚ä¸åŒäºTransformerçš„åŸºäºå‘¨æœŸå‡½æ•°çš„å›ºå®šä½ç½®ç¼–ç æ–¹æ³•ï¼ŒBERTé‡‡ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œbertä¸­çš„æœ€å¤§å¥å­é•¿åº¦æ˜¯512 æ‰€ä»¥Position Embedding layer æ˜¯ä¸€ä¸ªsizeä¸ºï¼ˆ512ï¼Œ768ï¼‰çš„lookup tableï¼Œå…¶ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œéšé¢„è®­ç»ƒè¿™äº›ä½ç½®ç›¸å…³çš„å‚æ•°æ”¶æ•›ï¼Œã€‚ã€‚ã€‚**ç›¸æ¯”Transformerçš„ä½ç½®ç¼–ç ï¼Œä¼¼ä¹æ²¡è€ƒè™‘ç›¸å¯¹ä½ç½®????**
### Transformerç¼–ç å™¨
Transformeræ¨¡å‹æ˜¯ç”±google aiäº2017å¹´å‘å¸ƒçš„ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼Œæœ€åˆåº”ç”¨äºæœºå™¨ç¿»è¯‘ã€‚Transformerçš„æœ€å¤§ç‰¹ç‚¹æ˜¯ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰ï¼Œè§£å†³äº†ä½¿ç”¨RNNæ¨¡å‹é€ æˆçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ— æ³•å¹¶è¡Œçš„é—®é¢˜ï¼Œå¹¶ä¸”å®è·µè¯æ˜transformerä¸­æå‡ºçš„å¤šå¤´æ³¨æ„åŠ›å…·æœ‰å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œæ€§èƒ½è¶…è¶Šäº†RNN,CNNç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚
> Transformeræ‰€ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å»è®¡ç®—ä¸€å¥è¯ä¸­çš„æ¯ä¸ªè¯å¯¹äºè¿™å¥è¯ä¸­æ‰€æœ‰è¯çš„ç›¸äº’å…³ç³»ï¼Œç„¶åè®¤ä¸ºè¿™äº›è¯ä¸è¯ä¹‹é—´çš„ç›¸äº’å…³ç³»åœ¨ä¸€å®šç¨‹åº¦ä¸Šååº”äº†è¿™å¥è¯ä¸­ä¸åŒè¯ä¹‹é—´çš„å…³è”æ€§ä»¥åŠé‡è¦ç¨‹åº¦ã€‚å› æ­¤å†åˆ©ç”¨è¿™äº›ç›¸äº’å…³ç³»æ¥è°ƒæ•´æ¯ä¸ªè¯çš„é‡è¦æ€§ï¼ˆæƒé‡ï¼‰å°±å¯ä»¥è·å¾—æ¯ä¸ªè¯æ–°çš„è¡¨è¾¾ã€‚è¿™ä¸ªæ–°çš„è¡¨å¾ä¸ä½†è•´å«äº†è¯¥è¯æœ¬èº«ï¼Œè¿˜è•´å«äº†å…¶ä»–è¯ä¸è¿™ä¸ªè¯çš„å…³ç³»ï¼Œå› æ­¤å’Œå•çº¯çš„è¯å‘é‡ç›¸æ¯”æ˜¯ä¸€ä¸ªæ›´åŠ å…¨å±€çš„è¡¨è¾¾ã€‚
> Transformeré€šè¿‡å¯¹è¾“å…¥çš„æ–‡æœ¬ä¸æ–­è¿›è¡Œè¿™æ ·çš„æ³¨æ„åŠ›æœºåˆ¶å±‚å’Œæ™®é€šçš„éçº¿æ€§å±‚äº¤å æ¥å¾—åˆ°æœ€ç»ˆçš„æ–‡æœ¬è¡¨è¾¾ã€‚

Transformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆwordï¼‰è½¬æ¢ä¸ºåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œå†ç”±è§£ç å™¨æ ¹æ®ç¼–ç åçš„ç‰¹å¾å‘é‡ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚BERTæ¨¡å‹ä¸­åªä½¿ç”¨äº†transformerçš„ç¼–ç å™¨ï¼Œå®ƒä¸»è¦ç”±è‹¥å¹²ä¸ªç»“æ„ç›¸åŒçš„ç¼–ç å±‚è¿æ¥è€Œæˆã€‚æ¯ä¸€ä¸ªç¼–ç å±‚ä¸»è¦æœ‰ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒï¼ˆMulti-Head Attentionï¼‰å’ŒæŒ‰ä½å‰é¦ˆç½‘ç»œ(Feed Forward)ç»„æˆï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒè´Ÿè´£ä¸ºæ¯ä¸ªè¾“å…¥å…ƒç´ ç”Ÿæˆç‰¹å¾å‘é‡ï¼Œå‰é¦ˆç½‘ç»œèƒ½å¤Ÿé€šè¿‡ç»„åˆå…ƒç´ ç‰¹å¾å‘é‡ç”Ÿæˆæ›´å¤æ‚çš„ç‰¹å¾å‘é‡ã€‚

![enter image description here](https://docs.google.com/drawings/d/e/2PACX-1vSqp25HORnsDrfUfkTFUgKeTC7IITVZrTMXBuf6eSp4_HmCsGRoGwAxEoN87fuhT98Xsc4IulE_U4vM/pub?w=960&h=720)
## BERTçš„é¢„è®­ç»ƒ
### ä»»åŠ¡è®¾è®¡
BERTçš„é¢„è®­ç»ƒè¢«è®¾è®¡ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰ï¼ŒåŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼š
- MLM
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
ç»™å®šä¸€ä¸ªå¥å­ï¼Œä¼šéšæœºMask 15%çš„è¯ï¼Œç„¶åè®©BERTæ¥é¢„æµ‹è¿™äº›Maskçš„è¯ï¼Œå¦‚åŒä¸Šè¿°10.1æ‰€è¿°ï¼Œåœ¨è¾“å…¥ä¾§å¼•å…¥[Mask]æ ‡è®°ï¼Œä¼šå¯¼è‡´é¢„è®­ç»ƒé˜¶æ®µå’ŒFine-tuningé˜¶æ®µä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå› æ­¤åœ¨è®ºæ–‡ä¸­ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé‡‡å–äº†å¦‚ä¸‹æªæ–½ï¼š

å¦‚æœæŸä¸ªTokenåœ¨è¢«é€‰ä¸­çš„15%ä¸ªTokené‡Œï¼Œåˆ™æŒ‰ç…§ä¸‹é¢çš„æ–¹å¼éšæœºçš„æ‰§è¡Œï¼š

-   80%çš„æ¦‚ç‡æ›¿æ¢æˆ[MASK]ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is [MASK]
-   10%çš„æ¦‚ç‡æ›¿æ¢æˆéšæœºçš„ä¸€ä¸ªè¯ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is apple
-   10%çš„æ¦‚ç‡æ›¿æ¢æˆå®ƒæœ¬èº«ï¼Œæ¯”å¦‚my dog is hairy â†’ my dog is hairy

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼ŒBERTå¹¶ä¸çŸ¥é“[MASK]æ›¿æ¢çš„æ˜¯è¿™15%ä¸ªTokenä¸­çš„å“ªä¸€ä¸ªè¯(**æ³¨æ„ï¼šè¿™é‡Œæ„æ€æ˜¯è¾“å…¥çš„æ—¶å€™ä¸çŸ¥é“[MASK]æ›¿æ¢çš„æ˜¯å“ªä¸€ä¸ªè¯ï¼Œä½†æ˜¯è¾“å‡ºè¿˜æ˜¯çŸ¥é“è¦é¢„æµ‹å“ªä¸ªè¯çš„**)ï¼Œè€Œä¸”ä»»ä½•ä¸€ä¸ªè¯éƒ½æœ‰å¯èƒ½æ˜¯è¢«æ›¿æ¢æ‰çš„ï¼Œæ¯”å¦‚å®ƒçœ‹åˆ°çš„appleå¯èƒ½æ˜¯è¢«æ›¿æ¢çš„è¯ã€‚è¿™æ ·å¼ºè¿«æ¨¡å‹åœ¨ç¼–ç å½“å‰æ—¶åˆ»çš„æ—¶å€™ä¸èƒ½å¤ªä¾èµ–äºå½“å‰çš„è¯ï¼Œè€Œè¦è€ƒè™‘å®ƒçš„ä¸Šä¸‹æ–‡ï¼Œç”šè‡³å¯¹å…¶ä¸Šä¸‹æ–‡è¿›è¡Œâ€çº é”™â€ã€‚æ¯”å¦‚ä¸Šé¢çš„ä¾‹å­æ¨¡å‹åœ¨ç¼–ç appleæ˜¯æ ¹æ®ä¸Šä¸‹æ–‡my dog isåº”è¯¥æŠŠapple(éƒ¨åˆ†)ç¼–ç æˆhairyçš„è¯­ä¹‰è€Œä¸æ˜¯appleçš„è¯­ä¹‰ã€‚
ç»†èŠ‚ä¸‰ï¼šå¯¹äºä»»åŠ¡ä¸€ï¼Œå¯¹äºåœ¨æ•°æ®ä¸­éšæœºé€‰æ‹© 15% çš„æ ‡è®°ï¼Œå…¶ä¸­80%è¢«æ¢ä½[mask]ï¼Œ10%ä¸å˜ã€10%éšæœºæ›¿æ¢å…¶ä»–å•è¯ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

**ä¸¤ä¸ªç¼ºç‚¹ï¼š**

1ã€å› ä¸ºBertç”¨äºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œ [MASK] æ ‡è®°ä¸ä¼šå‡ºç°ï¼Œå®ƒåªå‡ºç°åœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸­ã€‚è¿™å°±é€ æˆäº†é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„ä¸åŒ¹é…ï¼Œå¾®è°ƒä¸å‡ºç°[MASK]è¿™ä¸ªæ ‡è®°ï¼Œæ¨¡å‹å¥½åƒå°±æ²¡æœ‰äº†ç€åŠ›ç‚¹ã€ä¸çŸ¥ä»å“ªå…¥æ‰‹ã€‚æ‰€ä»¥åªå°†80%çš„æ›¿æ¢ä¸º[mask]ï¼Œä½†è¿™ä¹Ÿ**åªæ˜¯ç¼“è§£ã€ä¸èƒ½è§£å†³**ã€‚

2ã€ç›¸è¾ƒäºä¼ ç»Ÿè¯­è¨€æ¨¡å‹ï¼ŒBertçš„æ¯æ‰¹æ¬¡è®­ç»ƒæ•°æ®ä¸­åªæœ‰ 15% çš„æ ‡è®°è¢«é¢„æµ‹ï¼Œè¿™å¯¼è‡´æ¨¡å‹éœ€è¦æ›´å¤šçš„è®­ç»ƒæ­¥éª¤æ¥æ”¶æ•›ã€‚
- NSP
### æŸå¤±å‡½æ•°
total_loss = masked_lm_loss + next_sentence_loss
BERTçš„æŸå¤±å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯æ¥è‡ª Mask-LM çš„**å•è¯çº§åˆ«åˆ†ç±»ä»»åŠ¡**ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯**å¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡**ã€‚é€šè¿‡è¿™ä¸¤ä¸ªä»»åŠ¡çš„è”åˆå­¦ä¹ ï¼Œå¯ä»¥ä½¿å¾— BERT å­¦ä¹ åˆ°çš„è¡¨å¾æ—¢æœ‰ token çº§åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¹ŸåŒ…å«äº†å¥å­çº§åˆ«çš„è¯­ä¹‰ä¿¡æ¯ã€‚å…·ä½“æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%2C+%5Ctheta_%7B2%7D%5Cright%29%3DL_%7B1%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%2BL_%7B2%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29)

å…¶ä¸­  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta)  â€‹ æ˜¯ BERT ä¸­ Encoder éƒ¨åˆ†çš„å‚æ•°ï¼Œâ€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta_1)  æ˜¯ Mask-LM ä»»åŠ¡ä¸­åœ¨ Encoder ä¸Šæ‰€æ¥çš„è¾“å‡ºå±‚ä¸­çš„å‚æ•°ï¼Œâ€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta_2)  åˆ™æ˜¯å¥å­é¢„æµ‹ä»»åŠ¡ä¸­åœ¨ Encoder æ¥ä¸Šçš„åˆ†ç±»å™¨å‚æ•°ã€‚å› æ­¤ï¼Œåœ¨ç¬¬ä¸€éƒ¨åˆ†çš„æŸå¤±å‡½æ•°ä¸­ï¼Œå¦‚æœè¢« mask çš„è¯é›†åˆä¸º Mï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªè¯å…¸å¤§å° |V| ä¸Šçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆå…·ä½“è¯´æ¥æœ‰ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L_%7B1%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Clog+p%5Cleft%28m%3Dm_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29%2C+m_%7Bi%7D+%5Cin%5B1%2C2%2C+%5Cldots%2C%7CV%7C%5D)

åœ¨å¥å­é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L_%7B2%7D%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29%3D-%5Csum_%7Bj%3D1%7D%5E%7BN%7D+%5Clog+p%5Cleft%28n%3Dn_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29%2C+n_%7Bi%7D+%5Cin%5B%5Ctext+%7BIsNext%7D%2C+%5Ctext+%7BNotNext%7D%5D)

å› æ­¤ï¼Œä¸¤ä¸ªä»»åŠ¡è”åˆå­¦ä¹ çš„æŸå¤±å‡½æ•°æ˜¯ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=L%5Cleft%28%5Ctheta%2C+%5Ctheta_%7B1%7D%2C+%5Ctheta_%7B2%7D%5Cright%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Clog+p%5Cleft%28m%3Dm_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B1%7D%5Cright%29-%5Csum_%7Bj%3D1%7D%5E%7BN%7D+%5Clog+p%5Cleft%28n%3Dn_%7Bi%7D+%7C+%5Ctheta%2C+%5Ctheta_%7B2%7D%5Cright%29)

å…·ä½“çš„é¢„è®­ç»ƒå·¥ç¨‹å®ç°ç»†èŠ‚æ–¹é¢ï¼ŒBERT è¿˜åˆ©ç”¨äº†ä¸€ç³»åˆ—ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹æ›´æ˜“äºè®­ç»ƒï¼Œæ¯”å¦‚å¯¹äºå­¦ä¹ ç‡çš„ warm-up ç­–ç•¥ï¼Œä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ä¸å†æ˜¯æ™®é€šçš„ ReLuï¼Œè€Œæ˜¯ GeLuï¼Œä¹Ÿä½¿ç”¨äº† dropout ç­‰å¸¸è§çš„è®­ç»ƒæŠ€å·§ã€‚
### é¢„è®­ç»ƒæµç¨‹
[http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)  Recapping a sentenceâ€™s journey
1. Preprocessing: Add special token to raw input: "BERT is awesome. BERT is wonderful" becomes "[CLS] BERT is awesome [SEP] BERT is wonderful [SEP]"
2. Embedding
	2.1 word embedding: tokenization
	2.2 positional embedding
	2.3 segment embedding
3. Transformer encoder: 

like this



## BERTçš„å¾®è°ƒfine tune

### å¾®è°ƒä»»åŠ¡ç±»å‹
![enter image description here](https://lilianweng.github.io/lil-log/assets/images/BERT-downstream-tasks.png)
### **7.1 é’ˆå¯¹å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦çš„ä»»åŠ¡**
  
![](https://pic1.zhimg.com/80/v2-971f887ed616ea0f65941c8dc15ee128_720w.jpg)

  å®é™…æ“ä½œæ—¶ï¼Œä¸Šè¿°æœ€åä¸€å¥è¯ä¹‹åè¿˜ä¼šåŠ ä¸€ä¸ª[SEP] tokenï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡å°†ä¸¤ä¸ªå¥å­æŒ‰ç…§ä¸Šè¿°æ–¹å¼è¾“å…¥å³å¯ï¼Œä¹‹åä¸è®ºæ–‡ä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå°†[CLS] tokenä½ç½®å¯¹åº”çš„è¾“å‡ºï¼Œæ¥ä¸Šsoftmaxåšåˆ†ç±»å³å¯(å®é™…ä¸ŠGLUEä»»åŠ¡ä¸­å°±æœ‰å¾ˆå¤šè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ•°æ®é›†)ã€‚

### **7.2 é’ˆå¯¹å¤šæ ‡ç­¾åˆ†ç±»çš„ä»»åŠ¡**

å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå³MultiLabelï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªæ ·æœ¬å¯èƒ½åŒæ—¶å±äºå¤šä¸ªç±»ï¼Œå³æœ‰å¤šä¸ªæ ‡ç­¾ã€‚ä»¥å•†å“ä¸ºä¾‹ï¼Œä¸€ä»¶Lå°ºå¯¸çš„æ£‰æœï¼Œåˆ™è¯¥æ ·æœ¬å°±æœ‰è‡³å°‘ä¸¤ä¸ªæ ‡ç­¾â€”â€”å‹å·ï¼šLï¼Œç±»å‹ï¼šå†¬è£…ã€‚

å¯¹äºå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ˜¾è€Œæ˜“è§çš„æœ´ç´ åšæ³•å°±æ˜¯ä¸ç®¡æ ·æœ¬å±äºå‡ ä¸ªç±»ï¼Œå°±ç»™å®ƒè®­ç»ƒå‡ ä¸ªåˆ†ç±»æ¨¡å‹å³å¯ï¼Œç„¶åå†ä¸€ä¸€åˆ¤æ–­åœ¨è¯¥ç±»åˆ«ä¸­ï¼Œå…¶å±äºé‚£ä¸ªå­ç±»åˆ«ï¼Œä½†æ˜¯è¿™æ ·åšæœªå…å¤ªæš´åŠ›äº†ï¼Œè€Œå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå…¶å®æ˜¯å¯ä»¥**åªç”¨ä¸€ä¸ªæ¨¡å‹**æ¥è§£å†³çš„ã€‚

åˆ©ç”¨BERTæ¨¡å‹è§£å†³å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜æ—¶ï¼Œå…¶è¾“å…¥ä¸æ™®é€šå•æ ‡ç­¾åˆ†ç±»é—®é¢˜ä¸€è‡´ï¼Œå¾—åˆ°å…¶embeddingè¡¨ç¤ºä¹‹å(ä¹Ÿå°±æ˜¯BERTè¾“å‡ºå±‚çš„embedding)ï¼Œæœ‰å‡ ä¸ªlabelå°±è¿æ¥åˆ°å‡ ä¸ªå…¨è¿æ¥å±‚(ä¹Ÿå¯ä»¥ç§°ä¸ºprojection layer)ï¼Œç„¶åå†åˆ†åˆ«æ¥ä¸Šsoftmaxåˆ†ç±»å±‚ï¼Œè¿™æ ·çš„è¯ä¼šå¾—åˆ°â€‹  ![[å…¬å¼]](https://www.zhihu.com/equation?tex=loss_1%2C%5C+loss_2%2C%5C+%5Ccdots%2C%5C+loss_n)  ï¼Œæœ€åå†å°†æ‰€æœ‰çš„lossç›¸åŠ èµ·æ¥å³å¯ã€‚è¿™ç§åšæ³•å°±ç›¸å½“äºå°†nä¸ªåˆ†ç±»æ¨¡å‹çš„ç‰¹å¾æå–å±‚å‚æ•°å…±äº«ï¼Œå¾—åˆ°ä¸€ä¸ªå…±äº«çš„è¡¨ç¤º(å…¶ç»´åº¦å¯ä»¥è§†ä»»åŠ¡è€Œå®šï¼Œç”±äºæ˜¯å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤å…¶ç»´åº¦å¯ä»¥é€‚å½“å¢å¤§ä¸€äº›)ï¼Œæœ€åå†åšå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ã€‚

### **7.4 æ–‡æœ¬ç”Ÿæˆï¼ŸNO!**

### å¾®è°ƒæŠ€å·§
1. è°ƒæ•´å‚æ•°ï¼ˆå†…å­˜ï¼‰ï¼Œæ¨¡å‹é€‰æ‹©
2.  **é•¿æ–‡æœ¬å¤„ç†**
[https://zhuanlan.zhihu.com/p/109143667](https://zhuanlan.zhihu.com/p/109143667)
	å¯¹äºé•¿æ–‡æœ¬æ–‡ä¸­åšäº†ä¸¤ç§å¤„ç†æ–¹å¼ï¼Œæˆªæ–­å’Œåˆ‡åˆ†ã€‚

	-   æˆªæ–­ï¼šä¸€èˆ¬æ¥è¯´æ–‡æœ¬ä¸­æœ€é‡è¦çš„ä¿¡æ¯æ˜¯å¼€å§‹å’Œç»“å°¾ï¼Œå› æ­¤æ–‡ä¸­å¯¹äºé•¿æ–‡æœ¬åšäº†æˆªæ–­å¤„ç†ã€‚

	> head-onlyï¼šä¿ç•™å‰510ä¸ªå­—ç¬¦  
	> tail-onlyï¼šä¿ç•™å510ä¸ªå­—ç¬¦  
	> head+tailï¼šä¿ç•™å‰128ä¸ªå’Œå382ä¸ªå­—ç¬¦
	
	- åˆ‡åˆ†: å°†æ–‡æœ¬åˆ†æˆkæ®µï¼Œæ¯æ®µçš„è¾“å…¥å’ŒBertå¸¸è§„è¾“å…¥ç›¸åŒï¼Œç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯[CLS]è¡¨ç¤ºè¿™æ®µçš„åŠ æƒä¿¡æ¯ã€‚æ–‡ä¸­ä½¿ç”¨äº†Max-pooling, Average poolingå’Œself-attentionç»“åˆè¿™äº›ç‰‡æ®µçš„è¡¨ç¤ºã€‚
	- 
ä¸‹é¢æ˜¯å®éªŒçš„ç»“æœï¼Œhead+tailçš„è¡¨ç¤ºåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ•ˆæœéƒ½æ¯”è¾ƒå¥½ã€‚åº”è¯¥æ˜¯é•¿æ–‡æœ¬ç»“åˆäº†å¥é¦–å’Œå¥å°¾çš„ä¿¡æ¯ï¼Œè·å–çš„ä¿¡æ¯æ¯”è¾ƒå‡è¡¡ã€‚ä¸è¿‡å¥‡æ€ªçš„æ˜¯æ‹¼æ¥çš„æ–¹å¼æ•´ä½“å±…ç„¶ä¸å¦‚æˆªæ–­ï¼Œä¸ªäººçŒœæµ‹å¯èƒ½æ˜¯å°†å¥å­åˆ‡æˆå‡ æ®µä¹‹åå¢åŠ äº†æ¨¡å‹çš„ä¸ç¨³å®šæ€§ï¼Œè€Œé”™è¯¯å åŠ èµ·æ¥å¯èƒ½å°±ä¼šè¢«æ”¾å¤§ã€‚è€Œmax-poolingå’Œself-attentionä¹Ÿæ›´åŠ å¼ºè°ƒäº†æ–‡æœ¬ä¸­æ¯”è¾ƒæœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æ•´ä½“æ•ˆæœä¼˜äºaverage.
![enter image description here](https://pic3.zhimg.com/80/v2-f932b2ed7aa4af745b512e2e0f43093e_720w.jpg)

## BERTçš„æ”¹è¿›
[å…³äºBERTçš„è‹¥å¹²é—®é¢˜æ•´ç†è®°å½•](https://zhuanlan.zhihu.com/p/95594311)
### task design
- spanBERT [https://zhuanlan.zhihu.com/p/75893972](https://zhuanlan.zhihu.com/p/75893972)
### distillation
### LAMPï¼Ÿnot a BERT improvement
## BERTåº”ç”¨
[https://github.com/ProHiryu/bert-chinese-ner](https://github.com/ProHiryu/bert-chinese-ner)
[https://github.com/chiahsuan156/ODSQA](https://github.com/chiahsuan156/ODSQA)
### environment-colab
 - User BERT base model
 - Tweak: batch size, max length
 - Mixed precision training
 - Gradient checkpoint
### huggingface transformer
### BERT as a service
### DistilBERT
### SQUAD

## æ€»ç»“













	
## Transfer learning
- what?
- why?
	- Deep model which has lots of parameters need lots of training data
	- labeled training data is very expensive
	- To save training efforts (save money and time)
	- Transfer learning has been successful in CV tasks
- how?
	- pretraining - generate embeddings (word embeddings)
	- supervised finetuning - train for downstream tasks
	
### sequential transfer learning
- pretraining
- Adaptation
- finetuning
	- supervised
	- unsupervised

## pretraining
- self-supervised learningè‡ªç›‘ç£å­¦ä¹  based on  Language Model
	- Many successful pretraining approaches are based on language modeling
	- Informally, a LM learns PÏ´(text) or PÏ´(text | some other text)
	- Doesnâ€™t require human annotation
	- Many languages have enough text to learn high capacity model
	- Versatileâ€”can learn both sentence and word representations with a variety of
objective functions
- from static embedding to dynamic embedding
	- static encoding (contextless embedding)- word2vec, glove
	- dynamic encoding (contextual embedding) - elmo, bert
- How LM help NLP transfer learning
	- feature based
	**Feature-based**æŒ‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¸­é—´ç»“æœä¹Ÿå°±æ˜¯LM embedding, å°†å…¶ä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼Œå¼•å…¥åˆ°åŸä»»åŠ¡çš„æ¨¡å‹ä¸­ã€‚é€šå¸¸feature-basedæ–¹æ³•åŒ…æ‹¬ä¸¤æ­¥ï¼š

		1.  é¦–å…ˆåœ¨å¤§çš„è¯­æ–™Aä¸Šæ— ç›‘ç£åœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒå®Œæ¯•å¾—åˆ°è¯­è¨€æ¨¡å‹
		2.  ç„¶åæ„é€ task-specific modelä¾‹å¦‚åºåˆ—æ ‡æ³¨æ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒtask-sepcific modelï¼Œå°†è¯­è¨€æ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œè¯­æ–™Bçš„è®­ç»ƒæ•°æ®ç»è¿‡è¯­è¨€æ¨¡å‹å¾—åˆ°LM embeddingï¼Œä½œä¸ºtask-specific modelçš„é¢å¤–ç‰¹å¾ã€‚ELMoæ˜¯è¿™æ–¹é¢çš„å…¸å‹å·¥ä½œï¼Œè¯·å‚è€ƒ[2]
	- fine-tuning
	Fine-tuningæ–¹å¼æ˜¯æŒ‡åœ¨å·²ç»è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å°‘é‡çš„task-specific parameters, ä¾‹å¦‚å¯¹äºåˆ†ç±»é—®é¢˜åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸ŠåŠ ä¸€å±‚softmaxç½‘ç»œï¼Œç„¶ååœ¨æ–°çš„è¯­æ–™ä¸Šé‡æ–°è®­ç»ƒæ¥è¿›è¡Œfine-tuneã€‚ä¾‹å¦‚OpenAI GPT [3] ä¸­é‡‡ç”¨äº†è¿™æ ·çš„æ–¹æ³•ï¼Œæ¨¡å‹å¦‚ä¸‹æ‰€ç¤º

![](https://pic1.zhimg.com/80/v2-8f857288cf73acba9ddb6b3742265144_hd.jpg)

å›¾2 Transformer LM + fine-tuningæ¨¡å‹ç¤ºæ„å›¾

  
é¦–å…ˆè¯­è¨€æ¨¡å‹é‡‡ç”¨äº†Transformer Decoderçš„æ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨æ–‡æœ¬é¢„æµ‹ä½œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ï¼Œè®­ç»ƒå®Œæ¯•ä¹‹åï¼ŒåŠ ä¸€å±‚Linear Projectæ¥å®Œæˆåˆ†ç±»/ç›¸ä¼¼åº¦è®¡ç®—ç­‰NLPä»»åŠ¡ã€‚å› æ­¤æ€»ç»“æ¥è¯´ï¼ŒLM + Fine-Tuningçš„æ–¹æ³•å·¥ä½œåŒ…æ‹¬ä¸¤æ­¥ï¼š

1.  æ„é€ è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¤§çš„è¯­æ–™Aæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹
2.  åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šå¢åŠ å°‘é‡ç¥ç»ç½‘ç»œå±‚æ¥å®Œæˆspecific taskä¾‹å¦‚åºåˆ—æ ‡æ³¨ã€åˆ†ç±»ç­‰ï¼Œç„¶åé‡‡ç”¨æœ‰æ ‡è®°çš„è¯­æ–™Bæ¥æœ‰ç›‘ç£åœ°è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­è¯­è¨€æ¨¡å‹çš„å‚æ•°å¹¶ä¸å›ºå®šï¼Œä¾ç„¶æ˜¯trainable variables.
- Encoding
	- character level
	- BPE
	- word level
- task design (training objective) for self-supervised learning
	- Language model
	- bidirectional LM
	- MLM, NSP
	- GAN (ELATRA)
- The pretrained model is too complex to use
	- distillation
	- 
## Adaptation
GPT-2è®ºè¯äº†ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Ÿå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œä¸åŒé¢†åŸŸçš„æ–‡æœ¬ç›¸å½“äºä¸€ä¸ªç‹¬ç«‹çš„taskï¼Œè€Œå¦‚æœæŠŠè¿™äº›taskç»„åˆèµ·æ¥å­¦ä¹ ï¼Œé‚£ä¹ˆå°±æ˜¯multi-taskå­¦ä¹ ã€‚æ‰€ç‰¹æ®Šçš„æ˜¯è¿™äº›taskéƒ½æ˜¯åŒè´¨çš„ï¼Œå³å®ƒä»¬çš„ç›®æ ‡å‡½æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç»Ÿä¸€å­¦ä¹ ã€‚é‚£ä¹ˆå½“å¢å¤§æ•°æ®é›†åï¼Œç›¸å½“äºæ¨¡å‹åœ¨æ›´å¤šé¢†åŸŸä¸Šè¿›è¡Œäº†å­¦ä¹ ï¼Œå³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰äº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚
### GPT-2 ç›´æ¥åšä¸‹æ¸¸ä»»åŠ¡

é™¤äº†è¯­è¨€æ¨¡å‹ä¸Šçš„è¿›å±•ä¹‹å¤–ï¼ŒGPT-2è¿˜é¦–æ¬¡å°è¯•äº†ç›´æ¥ç”¨è¯­è¨€æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸ç”¨åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„æŸå¤±å‡½æ•°ã€‚è¿™æ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿ

æ¯”å¦‚ï¼Œå¦‚æœæ˜¯summarizationä»»åŠ¡ï¼Œé‚£ä¹ˆå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆ‘åŠ ä¸€ä¸ªæ–°è¯TL;DR:, æ”¹è¯å‰é¢æ˜¯contextï¼Œåé¢æ˜¯æ‘˜è¦ã€‚é‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°è¿™ä¸ªè¯åï¼Œå°±èƒ½æ¨æ–­å‡ºæ¥ï¼Œæ¥ä¸‹æ¥è¦åšæŠ½æ‘˜è¦çš„å·¥ä½œäº†ã€‚

åŒç†ï¼Œå¯¹äºtranslateä»»åŠ¡ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åšæˆ french sentence = english sentenceï¼Œé‚£ä¹ˆè¯­è¨€æ¨¡å‹é‡åˆ°=çš„æ—¶å€™ï¼Œåº”è¯¥èƒ½æ¨æ–­å‡ºæ¥ä¸‹æ¥æ˜¯ç¿»è¯‘ä»»åŠ¡ã€‚

è™½ç„¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼ŒGPT-2éƒ½æ²¡æœ‰è¾¾åˆ°SOTAçš„æ•ˆæœï¼Œä½†æ˜¯æ•ˆæœä¹Ÿæ˜¯ç›¸å½“å¯è§‚çš„ã€‚è¡¨æ˜äº†é«˜å®¹é‡æ¨¡å‹åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å¯èƒ½æ€§ã€‚

## Downstream fine-tuning
- finetuning tips - ULMFit
- tools: TF-hub

### Example

[Generalized language model](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[How to build openai's GPT2](https://blog.floydhub.com/gpt2/)
[BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[NLPé¢„è®­ç»ƒæ¼”è¿› - from Word2Vec to XLNet](https://zhuanlan.zhihu.com/p/93343298)
[nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼š](https://zhuanlan.zhihu.com/p/56382372)
[å²ä¸Šæœ€å…¨è¯å‘é‡è®²è§£](https://zhuanlan.zhihu.com/p/75391062)
[ELECTRA: è¶…è¶ŠBERT, 19å¹´æœ€ä½³NLPé¢„è®­ç»ƒæ¨¡](https://zhuanlan.zhihu.com/p/89763176)
[ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)
[å¦‚ä½•è¯„ä»·BERT-å›ç­”](https://www.zhihu.com/question/298203515/answer/516170825)
[NLPè§„åˆ™æ”¹å†™](https://zhuanlan.zhihu.com/p/47488095)
[BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)
[BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
[Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
[ELMO](https://petrlorenc.github.io/ELMO/)
[Character embedding CNN](https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb)
[The Illustrated BERT EMLO and co.](http://jalammar.github.io/illustrated-bert/)
[Transfer learning in NLP](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_177_4)
[VisualGuideToUsingBERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
[An In-Depth Tutorial to AllenNLP](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
[Transfer learning using elmo embedding](https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c)
[State of transfer learing in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)
[Generalized language model: ULMfit&openai GPT](https://www.topbots.com/generalized-language-models-ulmfit-openai-gpt/)
[Bertæ¨¡å‹åŠfine-tuning](https://zhuanlan.zhihu.com/p/46833276)
[Openai GPT2 è¯¦è§£](https://zhuanlan.zhihu.com/p/57251615)
[How to make custom AI-generated text with GPT2](https://minimaxir.com/2019/09/howto-gpt2/)
[GPT2: Understand language generation through visualization](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8)
[GPTä¸ºä»€ä¹ˆä¸èƒ½åŒå‘ï¼Ÿ](https://www.zhihu.com/question/322034410/answer/794201004)
[ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
[ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Unsupervised NER with BERT](https://www.quora.com/q/idpysofgzpanjxuh/Unsupervised-NER-using-BERT)
[BERT explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
[Zero shot GPT2](https://rakeshchada.github.io/Zero-Shot-GPT-2.html)
[Practical Applications of Open AIâ€™s GPT-2 Deep Learning Model](https://medium.com/the-research-nest/practical-applications-of-open-ais-gpt-2-deep-learning-model-14701f18a432)
[Understanding BERT Part 2: BERT Specifics](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
[Google BERT â€” Pre Training and Fine Tuning for NLP Tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[why BERT has 3 embedding layers?](https://mc.ai/why-bert-has-3-embedding-layers-and-their-implementation-details/)
[from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
[google BERT - pretraining and finetuing for NLP tasks](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78)
[NLP: Explaining Neural language model](https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/#.XniDIWgzZPY)
[Bertå¾®è°ƒæŠ€å·§å®éªŒå¤§å…¨](https://zhuanlan.zhihu.com/p/109143667)
[BERT finetuneçš„è‰ºæœ¯](https://zhuanlan.zhihu.com/p/62642374)
[Bertåœ¨NLPå„é¢†åŸŸçš„åº”ç”¨è¿›å±•](https://zhuanlan.zhihu.com/p/68446772)
[GPT2 finetune @familiarcycle.net/](https://familiarcycle.net/)
[paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained](https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTQyMDMyODA1LDgwMDczMjU3NCwtMTgyMz
Y5MTI3OCwtNjAwNDkxMjQzLC02MTA1Mzk3MTUsMzEzNjM3ODcx
LC05MDc5NDI3OTIsLTIwMDYzNzE4ODQsODc0MjQ3MTgzLC02OD
M5OTMxNjYsLTM3MDI5MjIzOSwxNzIzMTQzNjc1LDE0NjQ4MTc5
Miw0NDUzMDM4NTksNjU1OTg2NTcwLC0yMDE5NDg4MjI3LDExNj
gxNTc4NzcsLTQ5NDI4MTA5OCwzNTEyODQzMiwtNjE0MTk3NzIx
XX0=
-->